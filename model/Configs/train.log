2020-02-26 16:03:21,431 Helloooooooo! This is RegJoey-NMT.
2020-02-26 16:03:39,484 Total params: 15427584
2020-02-26 16:03:39,485 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-02-26 16:03:58,961 cfg.name                           : C_Base
2020-02-26 16:03:58,962 cfg.data.src                       : gloss
2020-02-26 16:03:58,962 cfg.data.trg                       : skels
2020-02-26 16:03:58,962 cfg.data.files                     : files
2020-02-26 16:03:58,962 cfg.data.train                     : .Data/train
2020-02-26 16:03:58,962 cfg.data.dev                       : .Data/dev
2020-02-26 16:03:58,962 cfg.data.test                      : .Data/test
2020-02-26 16:03:58,962 cfg.data.level                     : word
2020-02-26 16:03:58,962 cfg.data.lowercase                 : False
2020-02-26 16:03:58,962 cfg.data.max_sent_length           : 300
2020-02-26 16:03:58,963 cfg.data.percent_tok               : True
2020-02-26 16:03:58,963 cfg.data.output_train              : True
2020-02-26 16:03:58,963 cfg.data.train_inf                 : True
2020-02-26 16:03:58,963 cfg.data.scale_data                : 1
2020-02-26 16:03:58,963 cfg.data.output_videos             : True
2020-02-26 16:03:58,963 cfg.data.skip_frames               : 2
2020-02-26 16:03:58,963 cfg.data.save_bt                   : True
2020-02-26 16:03:58,963 cfg.testing.beam_size              : 0
2020-02-26 16:03:58,963 cfg.testing.alpha                  : 1.0
2020-02-26 16:03:58,963 cfg.testing.test_drive_count       : True
2020-02-26 16:03:58,963 cfg.training.random_seed           : 27
2020-02-26 16:03:58,963 cfg.training.optimizer             : adam
2020-02-26 16:03:58,963 cfg.training.learning_rate         : 0.001
2020-02-26 16:03:58,963 cfg.training.learning_rate_min     : 0.0002
2020-02-26 16:03:58,963 cfg.training.weight_decay          : 0.0
2020-02-26 16:03:58,963 cfg.training.clip_grad_norm        : 5.0
2020-02-26 16:03:58,963 cfg.training.batch_size            : 64
2020-02-26 16:03:58,963 cfg.training.batch_type            : sentence
2020-02-26 16:03:58,963 cfg.training.scheduling            : plateau
2020-02-26 16:03:58,963 cfg.training.patience              : 7
2020-02-26 16:03:58,963 cfg.training.decrease_factor       : 0.7
2020-02-26 16:03:58,963 cfg.training.early_stopping_metric : dtw
2020-02-26 16:03:58,963 cfg.training.epochs                : 20000
2020-02-26 16:03:58,963 cfg.training.validation_freq       : 10000
2020-02-26 16:03:58,964 cfg.training.logging_freq          : 250
2020-02-26 16:03:58,964 cfg.training.eval_metric           : bleu
2020-02-26 16:03:58,964 cfg.training.model_dir             : .Model/2_4_512_GN5
2020-02-26 16:03:58,964 cfg.training.overwrite             : False
2020-02-26 16:03:58,964 cfg.training.continue              : True
2020-02-26 16:03:58,964 cfg.training.shuffle               : True
2020-02-26 16:03:58,964 cfg.training.use_cuda              : True
2020-02-26 16:03:58,964 cfg.training.max_output_length     : 300
2020-02-26 16:03:58,964 cfg.training.print_valid_sents     : [0, 3, 6]
2020-02-26 16:03:58,964 cfg.training.keep_last_ckpts       : 1
2020-02-26 16:03:58,964 cfg.training.loss                  : MSE
2020-02-26 16:03:58,964 cfg.model.initializer              : xavier
2020-02-26 16:03:58,964 cfg.model.init_gain                : 1.0
2020-02-26 16:03:58,964 cfg.model.bias_initializer         : zeros
2020-02-26 16:03:58,964 cfg.model.embed_initializer        : xavier
2020-02-26 16:03:58,964 cfg.model.embed_init_gain          : 1.0
2020-02-26 16:03:58,964 cfg.model.tied_embeddings          : False
2020-02-26 16:03:58,964 cfg.model.default                  : False
2020-02-26 16:03:58,964 cfg.model.tied_softmax             : False
2020-02-26 16:03:58,964 cfg.model.trg_size                 : 150
2020-02-26 16:03:58,964 cfg.model.count_in                 : True
2020-02-26 16:03:58,964 cfg.model.EOS_input                : True
2020-02-26 16:03:58,965 cfg.model.zero_bos_frame           : False
2020-02-26 16:03:58,965 cfg.model.just_count_in            : False
2020-02-26 16:03:58,965 cfg.model.gaussian_noise           : True
2020-02-26 16:03:58,965 cfg.model.noise_rate               : 5
2020-02-26 16:03:58,965 cfg.model.noise_from               : Within_Error
2020-02-26 16:03:58,965 cfg.model.sparse_noise             : False
2020-02-26 16:03:58,965 cfg.model.sparse_probability       : 0.2
2020-02-26 16:03:58,965 cfg.model.specific_noise           : False
2020-02-26 16:03:58,965 cfg.model.shift_frames             : 0
2020-02-26 16:03:58,965 cfg.model.future_prediction        : 0
2020-02-26 16:03:58,965 cfg.model.future_prediction_from   : 0
2020-02-26 16:03:58,965 cfg.model.encoder.type             : transformer
2020-02-26 16:03:58,965 cfg.model.encoder.num_layers       : 2
2020-02-26 16:03:58,965 cfg.model.encoder.num_heads        : 4
2020-02-26 16:03:58,965 cfg.model.encoder.embeddings.embedding_dim : 512
2020-02-26 16:03:58,965 cfg.model.encoder.embeddings.dropout : 0.0
2020-02-26 16:03:58,965 cfg.model.encoder.hidden_size      : 512
2020-02-26 16:03:58,965 cfg.model.encoder.ff_size          : 2048
2020-02-26 16:03:58,965 cfg.model.encoder.dropout          : 0.0
2020-02-26 16:03:58,965 cfg.model.decoder.type             : transformer
2020-02-26 16:03:58,965 cfg.model.decoder.num_layers       : 2
2020-02-26 16:03:58,965 cfg.model.decoder.num_heads        : 4
2020-02-26 16:03:58,965 cfg.model.decoder.embeddings.embedding_dim : 512
2020-02-26 16:03:58,966 cfg.model.decoder.embeddings.dropout : 0.0
2020-02-26 16:03:58,966 cfg.model.decoder.hidden_size      : 512
2020-02-26 16:03:58,966 cfg.model.decoder.ff_size          : 2048
2020-02-26 16:03:58,966 cfg.model.decoder.dropout          : 0.0
2020-02-26 16:03:58,967 EPOCH 1
2020-02-26 16:04:12,643 Epoch   1: total training loss 0.09460
2020-02-26 16:04:12,644 EPOCH 2
2020-02-26 16:04:23,086 Epoch   2: total training loss 0.01319
2020-02-26 16:04:23,087 EPOCH 3
2020-02-26 16:04:25,710 Epoch   3 Step:      250 Batch Loss:     0.000074 Tokens per Sec:  9749525, Lr: 0.001000
2020-02-26 16:04:33,700 Epoch   3: total training loss 0.00992
2020-02-26 16:04:33,701 EPOCH 4
2020-02-26 16:04:44,199 Epoch   4: total training loss 0.00864
2020-02-26 16:04:44,199 EPOCH 5
2020-02-26 16:04:49,618 Epoch   5 Step:      500 Batch Loss:     0.000066 Tokens per Sec:  9518918, Lr: 0.001000
2020-02-26 16:04:54,938 Epoch   5: total training loss 0.00765
2020-02-26 16:04:54,939 EPOCH 6
2020-02-26 16:05:05,051 Epoch   6: total training loss 0.00694
2020-02-26 16:05:05,051 EPOCH 7
2020-02-26 16:05:12,518 Epoch   7 Step:      750 Batch Loss:     0.000032 Tokens per Sec: 10386142, Lr: 0.001000
2020-02-26 16:05:15,060 Epoch   7: total training loss 0.00635
2020-02-26 16:05:15,060 EPOCH 8
2020-02-26 16:05:25,077 Epoch   8: total training loss 0.00609
2020-02-26 16:05:25,078 EPOCH 9
2020-02-26 16:05:35,245 Epoch   9: total training loss 0.00578
2020-02-26 16:05:35,246 EPOCH 10
2020-02-26 16:05:35,362 Epoch  10 Step:     1000 Batch Loss:     0.000054 Tokens per Sec:  7637395, Lr: 0.001000
2020-02-26 16:05:45,252 Epoch  10: total training loss 0.00561
2020-02-26 16:05:45,252 EPOCH 11
2020-02-26 16:05:55,238 Epoch  11: total training loss 0.00548
2020-02-26 16:05:55,239 EPOCH 12
2020-02-26 16:05:58,151 Epoch  12 Step:     1250 Batch Loss:     0.000051 Tokens per Sec:  9995263, Lr: 0.001000
2020-02-26 16:06:05,559 Epoch  12: total training loss 0.00539
2020-02-26 16:06:05,559 EPOCH 13
2020-02-26 16:06:15,761 Epoch  13: total training loss 0.00530
2020-02-26 16:06:15,761 EPOCH 14
2020-02-26 16:06:21,049 Epoch  14 Step:     1500 Batch Loss:     0.000051 Tokens per Sec: 10036001, Lr: 0.001000
2020-02-26 16:06:26,056 Epoch  14: total training loss 0.00518
2020-02-26 16:06:26,056 EPOCH 15
2020-02-26 16:06:36,530 Epoch  15: total training loss 0.00549
2020-02-26 16:06:36,531 EPOCH 16
2020-02-26 16:06:44,427 Epoch  16 Step:     1750 Batch Loss:     0.000047 Tokens per Sec: 10176362, Lr: 0.001000
2020-02-26 16:06:46,607 Epoch  16: total training loss 0.00586
2020-02-26 16:06:46,607 EPOCH 17
2020-02-26 16:06:56,795 Epoch  17: total training loss 0.00501
2020-02-26 16:06:56,796 EPOCH 18
2020-02-26 16:07:06,894 Epoch  18: total training loss 0.00498
2020-02-26 16:07:06,895 EPOCH 19
2020-02-26 16:07:07,072 Epoch  19 Step:     2000 Batch Loss:     0.000051 Tokens per Sec:  8737135, Lr: 0.001000
2020-02-26 16:07:16,876 Epoch  19: total training loss 0.00486
2020-02-26 16:07:16,877 EPOCH 20
2020-02-26 16:07:26,837 Epoch  20: total training loss 0.00494
2020-02-26 16:07:26,837 EPOCH 21
2020-02-26 16:07:29,608 Epoch  21 Step:     2250 Batch Loss:     0.000049 Tokens per Sec: 10627815, Lr: 0.001000
2020-02-26 16:07:36,685 Epoch  21: total training loss 0.00487
2020-02-26 16:07:36,686 EPOCH 22
2020-02-26 16:07:46,659 Epoch  22: total training loss 0.00488
2020-02-26 16:07:46,659 EPOCH 23
2020-02-26 16:07:51,815 Epoch  23 Step:     2500 Batch Loss:     0.000047 Tokens per Sec: 10387155, Lr: 0.001000
2020-02-26 16:07:56,604 Epoch  23: total training loss 0.00485
2020-02-26 16:07:56,605 EPOCH 24
2020-02-26 16:08:06,865 Epoch  24: total training loss 0.00482
2020-02-26 16:08:06,865 EPOCH 25
2020-02-26 16:08:14,799 Epoch  25 Step:     2750 Batch Loss:     0.000057 Tokens per Sec: 10045343, Lr: 0.001000
2020-02-26 16:08:17,183 Epoch  25: total training loss 0.00479
2020-02-26 16:08:17,183 EPOCH 26
2020-02-26 16:08:27,271 Epoch  26: total training loss 0.00502
2020-02-26 16:08:27,272 EPOCH 27
2020-02-26 16:08:37,378 Epoch  27: total training loss 0.00483
2020-02-26 16:08:37,378 EPOCH 28
2020-02-26 16:08:37,622 Epoch  28 Step:     3000 Batch Loss:     0.000042 Tokens per Sec:  9324818, Lr: 0.001000
2020-02-26 16:08:47,201 Epoch  28: total training loss 0.00584
2020-02-26 16:08:47,201 EPOCH 29
2020-02-26 16:08:57,138 Epoch  29: total training loss 0.00459
2020-02-26 16:08:57,139 EPOCH 30
2020-02-26 16:08:59,917 Epoch  30 Step:     3250 Batch Loss:     0.000026 Tokens per Sec: 10470794, Lr: 0.001000
2020-02-26 16:09:07,157 Epoch  30: total training loss 0.00468
2020-02-26 16:09:07,157 EPOCH 31
2020-02-26 16:09:17,115 Epoch  31: total training loss 0.00449
2020-02-26 16:09:17,116 EPOCH 32
2020-02-26 16:09:22,218 Epoch  32 Step:     3500 Batch Loss:     0.000045 Tokens per Sec: 10328407, Lr: 0.001000
2020-02-26 16:09:27,251 Epoch  32: total training loss 0.00453
2020-02-26 16:09:27,251 EPOCH 33
2020-02-26 16:09:37,717 Epoch  33: total training loss 0.00460
2020-02-26 16:09:37,717 EPOCH 34
2020-02-26 16:09:46,019 Epoch  34 Step:     3750 Batch Loss:     0.000025 Tokens per Sec:  9846026, Lr: 0.001000
2020-02-26 16:09:48,278 Epoch  34: total training loss 0.00456
2020-02-26 16:09:48,278 EPOCH 35
2020-02-26 16:09:58,644 Epoch  35: total training loss 0.00448
2020-02-26 16:09:58,644 EPOCH 36
2020-02-26 16:10:09,330 Epoch  36: total training loss 0.00495
2020-02-26 16:10:09,331 EPOCH 37
2020-02-26 16:10:09,736 Epoch  37 Step:     4000 Batch Loss:     0.000049 Tokens per Sec:  9923784, Lr: 0.001000
2020-02-26 16:10:19,465 Epoch  37: total training loss 0.00450
2020-02-26 16:10:19,465 EPOCH 38
2020-02-26 16:10:29,399 Epoch  38: total training loss 0.00475
2020-02-26 16:10:29,400 EPOCH 39
2020-02-26 16:10:32,170 Epoch  39 Step:     4250 Batch Loss:     0.000052 Tokens per Sec: 10205180, Lr: 0.001000
2020-02-26 16:10:39,344 Epoch  39: total training loss 0.00462
2020-02-26 16:10:39,345 EPOCH 40
2020-02-26 16:10:49,404 Epoch  40: total training loss 0.00461
2020-02-26 16:10:49,404 EPOCH 41
2020-02-26 16:10:54,671 Epoch  41 Step:     4500 Batch Loss:     0.000026 Tokens per Sec: 10437227, Lr: 0.001000
2020-02-26 16:10:59,221 Epoch  41: total training loss 0.00452
2020-02-26 16:10:59,222 EPOCH 42
2020-02-26 16:11:09,085 Epoch  42: total training loss 0.00481
2020-02-26 16:11:09,086 EPOCH 43
2020-02-26 16:11:16,926 Epoch  43 Step:     4750 Batch Loss:     0.000028 Tokens per Sec: 10234180, Lr: 0.001000
2020-02-26 16:11:19,153 Epoch  43: total training loss 0.00484
2020-02-26 16:11:19,153 EPOCH 44
2020-02-26 16:11:29,316 Epoch  44: total training loss 0.00450
2020-02-26 16:11:29,316 EPOCH 45
2020-02-26 16:11:39,482 Epoch  45: total training loss 0.00449
2020-02-26 16:11:39,483 EPOCH 46
2020-02-26 16:11:40,002 Epoch  46 Step:     5000 Batch Loss:     0.000045 Tokens per Sec: 10418762, Lr: 0.001000
2020-02-26 16:11:49,684 Epoch  46: total training loss 0.00458
2020-02-26 16:11:49,685 EPOCH 47
2020-02-26 16:11:59,948 Epoch  47: total training loss 0.00442
2020-02-26 16:11:59,949 EPOCH 48
2020-02-26 16:12:02,938 Epoch  48 Step:     5250 Batch Loss:     0.000050 Tokens per Sec: 10362779, Lr: 0.001000
2020-02-26 16:12:10,098 Epoch  48: total training loss 0.00443
2020-02-26 16:12:10,099 EPOCH 49
2020-02-26 16:12:20,053 Epoch  49: total training loss 0.00447
2020-02-26 16:12:20,053 EPOCH 50
2020-02-26 16:12:25,353 Epoch  50 Step:     5500 Batch Loss:     0.000033 Tokens per Sec: 10308657, Lr: 0.001000
2020-02-26 16:12:30,001 Epoch  50: total training loss 0.00449
2020-02-26 16:12:30,002 EPOCH 51
2020-02-26 16:12:39,842 Epoch  51: total training loss 0.00448
2020-02-26 16:12:39,842 EPOCH 52
2020-02-26 16:12:47,722 Epoch  52 Step:     5750 Batch Loss:     0.000049 Tokens per Sec: 10328972, Lr: 0.001000
2020-02-26 16:12:49,749 Epoch  52: total training loss 0.00445
2020-02-26 16:12:49,750 EPOCH 53
2020-02-26 16:12:59,593 Epoch  53: total training loss 0.00441
2020-02-26 16:12:59,593 EPOCH 54
2020-02-26 16:13:09,406 Epoch  54: total training loss 0.00453
2020-02-26 16:13:09,407 EPOCH 55
2020-02-26 16:13:09,956 Epoch  55 Step:     6000 Batch Loss:     0.000041 Tokens per Sec: 10134007, Lr: 0.001000
2020-02-26 16:13:19,452 Epoch  55: total training loss 0.00452
2020-02-26 16:13:19,452 EPOCH 56
2020-02-26 16:13:29,526 Epoch  56: total training loss 0.00444
2020-02-26 16:13:29,527 EPOCH 57
2020-02-26 16:13:32,571 Epoch  57 Step:     6250 Batch Loss:     0.000047 Tokens per Sec: 10348843, Lr: 0.001000
2020-02-26 16:13:39,365 Epoch  57: total training loss 0.00443
2020-02-26 16:13:39,366 EPOCH 58
2020-02-26 16:13:49,326 Epoch  58: total training loss 0.00456
2020-02-26 16:13:49,327 EPOCH 59
2020-02-26 16:13:54,891 Epoch  59 Step:     6500 Batch Loss:     0.000045 Tokens per Sec: 10462287, Lr: 0.001000
2020-02-26 16:13:59,111 Epoch  59: total training loss 0.00439
2020-02-26 16:13:59,112 EPOCH 60
2020-02-26 16:14:08,966 Epoch  60: total training loss 0.00455
2020-02-26 16:14:09,677 EPOCH 61
2020-02-26 16:14:17,895 Epoch  61 Step:     6750 Batch Loss:     0.000030 Tokens per Sec: 10252800, Lr: 0.001000
2020-02-26 16:14:19,667 Epoch  61: total training loss 0.00453
2020-02-26 16:14:19,667 EPOCH 62
2020-02-26 16:14:29,574 Epoch  62: total training loss 0.00453
2020-02-26 16:14:29,576 EPOCH 63
2020-02-26 16:14:39,503 Epoch  63: total training loss 0.00460
2020-02-26 16:14:39,515 EPOCH 64
2020-02-26 16:14:40,269 Epoch  64 Step:     7000 Batch Loss:     0.000040 Tokens per Sec: 10571092, Lr: 0.001000
2020-02-26 16:14:49,407 Epoch  64: total training loss 0.00457
2020-02-26 16:14:49,407 EPOCH 65
2020-02-26 16:14:59,150 Epoch  65: total training loss 0.00457
2020-02-26 16:14:59,151 EPOCH 66
2020-02-26 16:15:02,179 Epoch  66 Step:     7250 Batch Loss:     0.000047 Tokens per Sec: 10658638, Lr: 0.001000
2020-02-26 16:15:08,808 Epoch  66: total training loss 0.00466
2020-02-26 16:15:08,809 EPOCH 67
2020-02-26 16:15:18,625 Epoch  67: total training loss 0.00449
2020-02-26 16:15:18,626 EPOCH 68
2020-02-26 16:15:24,090 Epoch  68 Step:     7500 Batch Loss:     0.000043 Tokens per Sec: 10493700, Lr: 0.001000
2020-02-26 16:15:28,484 Epoch  68: total training loss 0.00454
2020-02-26 16:15:28,484 EPOCH 69
2020-02-26 16:15:38,371 Epoch  69: total training loss 0.00451
2020-02-26 16:15:38,372 EPOCH 70
2020-02-26 16:15:46,659 Epoch  70 Step:     7750 Batch Loss:     0.000044 Tokens per Sec: 10141389, Lr: 0.001000
2020-02-26 16:15:48,448 Epoch  70: total training loss 0.00453
2020-02-26 16:15:48,448 EPOCH 71
2020-02-26 16:15:58,766 Epoch  71: total training loss 0.00452
2020-02-26 16:15:58,767 EPOCH 72
2020-02-26 16:16:08,938 Epoch  72: total training loss 0.00458
2020-02-26 16:16:08,938 EPOCH 73
2020-02-26 16:16:09,762 Epoch  73 Step:     8000 Batch Loss:     0.000048 Tokens per Sec:  9960298, Lr: 0.001000
2020-02-26 16:16:19,128 Epoch  73: total training loss 0.00452
2020-02-26 16:16:19,129 EPOCH 74
2020-02-26 16:16:29,138 Epoch  74: total training loss 0.00466
2020-02-26 16:16:29,138 EPOCH 75
2020-02-26 16:16:32,296 Epoch  75 Step:     8250 Batch Loss:     0.000049 Tokens per Sec: 10238213, Lr: 0.001000
2020-02-26 16:16:39,070 Epoch  75: total training loss 0.00451
2020-02-26 16:16:39,070 EPOCH 76
2020-02-26 16:16:49,004 Epoch  76: total training loss 0.00453
2020-02-26 16:16:49,004 EPOCH 77
2020-02-26 16:16:54,978 Epoch  77 Step:     8500 Batch Loss:     0.000042 Tokens per Sec: 10198578, Lr: 0.001000
2020-02-26 16:16:59,091 Epoch  77: total training loss 0.00475
2020-02-26 16:16:59,092 EPOCH 78
2020-02-26 16:17:09,035 Epoch  78: total training loss 0.00449
2020-02-26 16:17:09,036 EPOCH 79
2020-02-26 16:17:17,076 Epoch  79 Step:     8750 Batch Loss:     0.000049 Tokens per Sec: 10301419, Lr: 0.001000
2020-02-26 16:17:18,922 Epoch  79: total training loss 0.00452
2020-02-26 16:17:18,922 EPOCH 80
2020-02-26 16:17:28,840 Epoch  80: total training loss 0.00454
2020-02-26 16:17:28,841 EPOCH 81
2020-02-26 16:17:39,150 Epoch  81: total training loss 0.00457
2020-02-26 16:17:39,151 EPOCH 82
2020-02-26 16:17:39,940 Epoch  82 Step:     9000 Batch Loss:     0.000033 Tokens per Sec:  9825104, Lr: 0.001000
2020-02-26 16:17:49,477 Epoch  82: total training loss 0.00450
2020-02-26 16:17:49,478 EPOCH 83
2020-02-26 16:17:59,866 Epoch  83: total training loss 0.00443
2020-02-26 16:17:59,866 EPOCH 84
2020-02-26 16:18:03,228 Epoch  84 Step:     9250 Batch Loss:     0.000040 Tokens per Sec:  9809474, Lr: 0.001000
2020-02-26 16:18:10,077 Epoch  84: total training loss 0.00456
2020-02-26 16:18:10,078 EPOCH 85
2020-02-26 16:18:20,127 Epoch  85: total training loss 0.00445
2020-02-26 16:18:20,127 EPOCH 86
2020-02-26 16:18:26,053 Epoch  86 Step:     9500 Batch Loss:     0.000030 Tokens per Sec: 10378461, Lr: 0.001000
2020-02-26 16:18:30,155 Epoch  86: total training loss 0.00440
2020-02-26 16:18:30,155 EPOCH 87
2020-02-26 16:18:40,054 Epoch  87: total training loss 0.00458
2020-02-26 16:18:40,054 EPOCH 88
2020-02-26 16:18:48,503 Epoch  88 Step:     9750 Batch Loss:     0.000039 Tokens per Sec: 10248762, Lr: 0.001000
2020-02-26 16:18:50,117 Epoch  88: total training loss 0.00447
2020-02-26 16:18:50,117 EPOCH 89
2020-02-26 16:19:00,036 Epoch  89: total training loss 0.00441
2020-02-26 16:19:00,036 EPOCH 90
2020-02-26 16:19:09,955 Epoch  90: total training loss 0.00441
2020-02-26 16:19:09,956 EPOCH 91
2020-02-26 16:19:10,744 Epoch  91 Step:    10000 Batch Loss:     0.000034 Tokens per Sec:  9734516, Lr: 0.001000
2020-02-26 16:19:10,745 Model noise rate: 5
2020-02-26 16:19:59,173 Hooray! New best validation result [dtw]!
2020-02-26 16:19:59,174 Saving new checkpoint.
2020-02-26 16:20:15,315 Validation result at epoch  91, step    10000: Val DTW Score:  30.70, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0389, GT DTW Score:      nan, duration: 64.5702s
2020-02-26 16:20:24,623 Epoch  91: total training loss 0.00450
2020-02-26 16:20:24,623 EPOCH 92
2020-02-26 16:20:34,558 Epoch  92: total training loss 0.00442
2020-02-26 16:20:34,558 EPOCH 93
2020-02-26 16:20:38,021 Epoch  93 Step:    10250 Batch Loss:     0.000031 Tokens per Sec: 10380485, Lr: 0.001000
2020-02-26 16:20:44,516 Epoch  93: total training loss 0.00447
2020-02-26 16:20:44,516 EPOCH 94
2020-02-26 16:20:54,454 Epoch  94: total training loss 0.00439
2020-02-26 16:20:54,455 EPOCH 95
2020-02-26 16:21:00,112 Epoch  95 Step:    10500 Batch Loss:     0.000024 Tokens per Sec: 10284739, Lr: 0.001000
2020-02-26 16:21:04,339 Epoch  95: total training loss 0.00434
2020-02-26 16:21:04,339 EPOCH 96
2020-02-26 16:21:14,121 Epoch  96: total training loss 0.00441
2020-02-26 16:21:14,123 EPOCH 97
2020-02-26 16:21:22,673 Epoch  97 Step:    10750 Batch Loss:     0.000047 Tokens per Sec: 10044859, Lr: 0.001000
2020-02-26 16:21:24,256 Epoch  97: total training loss 0.00444
2020-02-26 16:21:24,256 EPOCH 98
2020-02-26 16:21:34,496 Epoch  98: total training loss 0.00444
2020-02-26 16:21:34,497 EPOCH 99
2020-02-26 16:21:44,713 Epoch  99: total training loss 0.00436
2020-02-26 16:21:44,713 EPOCH 100
2020-02-26 16:21:45,593 Epoch 100 Step:    11000 Batch Loss:     0.000047 Tokens per Sec:  9642383, Lr: 0.001000
2020-02-26 16:21:54,788 Epoch 100: total training loss 0.00433
2020-02-26 16:21:54,788 EPOCH 101
2020-02-26 16:22:04,810 Epoch 101: total training loss 0.00442
2020-02-26 16:22:04,811 EPOCH 102
2020-02-26 16:22:08,296 Epoch 102 Step:    11250 Batch Loss:     0.000040 Tokens per Sec: 10575974, Lr: 0.001000
2020-02-26 16:22:14,750 Epoch 102: total training loss 0.00434
2020-02-26 16:22:14,752 EPOCH 103
2020-02-26 16:22:24,849 Epoch 103: total training loss 0.00439
2020-02-26 16:22:24,849 EPOCH 104
2020-02-26 16:22:30,982 Epoch 104 Step:    11500 Batch Loss:     0.000035 Tokens per Sec: 10376359, Lr: 0.001000
2020-02-26 16:22:34,828 Epoch 104: total training loss 0.00435
2020-02-26 16:22:34,829 EPOCH 105
2020-02-26 16:22:44,919 Epoch 105: total training loss 0.00434
2020-02-26 16:22:44,919 EPOCH 106
2020-02-26 16:22:53,343 Epoch 106 Step:    11750 Batch Loss:     0.000027 Tokens per Sec: 10445064, Lr: 0.001000
2020-02-26 16:22:54,772 Epoch 106: total training loss 0.00441
2020-02-26 16:22:54,772 EPOCH 107
2020-02-26 16:23:04,587 Epoch 107: total training loss 0.00434
2020-02-26 16:23:04,588 EPOCH 108
2020-02-26 16:23:14,541 Epoch 108: total training loss 0.00434
2020-02-26 16:23:14,541 EPOCH 109
2020-02-26 16:23:15,580 Epoch 109 Step:    12000 Batch Loss:     0.000035 Tokens per Sec:  9725717, Lr: 0.001000
2020-02-26 16:23:24,764 Epoch 109: total training loss 0.00436
2020-02-26 16:23:24,764 EPOCH 110
2020-02-26 16:23:34,987 Epoch 110: total training loss 0.00437
2020-02-26 16:23:34,988 EPOCH 111
2020-02-26 16:23:38,507 Epoch 111 Step:    12250 Batch Loss:     0.000034 Tokens per Sec: 10164024, Lr: 0.001000
2020-02-26 16:23:44,929 Epoch 111: total training loss 0.00441
2020-02-26 16:23:44,930 EPOCH 112
2020-02-26 16:23:54,860 Epoch 112: total training loss 0.00430
2020-02-26 16:23:54,861 EPOCH 113
2020-02-26 16:24:00,949 Epoch 113 Step:    12500 Batch Loss:     0.000043 Tokens per Sec: 10477469, Lr: 0.001000
2020-02-26 16:24:04,655 Epoch 113: total training loss 0.00435
2020-02-26 16:24:04,656 EPOCH 114
2020-02-26 16:24:14,355 Epoch 114: total training loss 0.00445
2020-02-26 16:24:14,356 EPOCH 115
2020-02-26 16:24:22,860 Epoch 115 Step:    12750 Batch Loss:     0.000028 Tokens per Sec: 10438096, Lr: 0.001000
2020-02-26 16:24:24,175 Epoch 115: total training loss 0.00434
2020-02-26 16:24:24,176 EPOCH 116
2020-02-26 16:24:34,228 Epoch 116: total training loss 0.00430
2020-02-26 16:24:34,229 EPOCH 117
2020-02-26 16:24:44,169 Epoch 117: total training loss 0.00433
2020-02-26 16:24:44,172 EPOCH 118
2020-02-26 16:24:45,405 Epoch 118 Step:    13000 Batch Loss:     0.000037 Tokens per Sec: 10421171, Lr: 0.001000
2020-02-26 16:24:54,144 Epoch 118: total training loss 0.00443
2020-02-26 16:24:54,146 EPOCH 119
2020-02-26 16:25:04,749 Epoch 119: total training loss 0.00432
2020-02-26 16:25:04,750 EPOCH 120
2020-02-26 16:25:08,883 Epoch 120 Step:    13250 Batch Loss:     0.000041 Tokens per Sec:  9663587, Lr: 0.001000
2020-02-26 16:25:15,525 Epoch 120: total training loss 0.00424
2020-02-26 16:25:15,525 EPOCH 121
2020-02-26 16:25:26,006 Epoch 121: total training loss 0.00438
2020-02-26 16:25:26,007 EPOCH 122
2020-02-26 16:25:32,377 Epoch 122 Step:    13500 Batch Loss:     0.000027 Tokens per Sec:  9863016, Lr: 0.001000
2020-02-26 16:25:36,437 Epoch 122: total training loss 0.00436
2020-02-26 16:25:36,438 EPOCH 123
2020-02-26 16:25:46,638 Epoch 123: total training loss 0.00435
2020-02-26 16:25:46,638 EPOCH 124
2020-02-26 16:25:55,504 Epoch 124 Step:    13750 Batch Loss:     0.000033 Tokens per Sec: 10159423, Lr: 0.001000
2020-02-26 16:25:56,776 Epoch 124: total training loss 0.00432
2020-02-26 16:25:56,776 EPOCH 125
2020-02-26 16:26:06,807 Epoch 125: total training loss 0.00426
2020-02-26 16:26:06,808 EPOCH 126
2020-02-26 16:26:16,790 Epoch 126: total training loss 0.00425
2020-02-26 16:26:16,792 EPOCH 127
2020-02-26 16:26:18,140 Epoch 127 Step:    14000 Batch Loss:     0.000024 Tokens per Sec: 10549900, Lr: 0.001000
2020-02-26 16:26:26,774 Epoch 127: total training loss 0.00434
2020-02-26 16:26:26,774 EPOCH 128
2020-02-26 16:26:36,777 Epoch 128: total training loss 0.00431
2020-02-26 16:26:36,779 EPOCH 129
2020-02-26 16:26:40,530 Epoch 129 Step:    14250 Batch Loss:     0.000032 Tokens per Sec:  9895275, Lr: 0.001000
2020-02-26 16:26:47,054 Epoch 129: total training loss 0.00426
2020-02-26 16:26:47,055 EPOCH 130
2020-02-26 16:26:57,212 Epoch 130: total training loss 0.00426
2020-02-26 16:26:57,212 EPOCH 131
2020-02-26 16:27:03,861 Epoch 131 Step:    14500 Batch Loss:     0.000045 Tokens per Sec: 10019253, Lr: 0.001000
2020-02-26 16:27:07,524 Epoch 131: total training loss 0.00425
2020-02-26 16:27:07,524 EPOCH 132
2020-02-26 16:27:17,752 Epoch 132: total training loss 0.00431
2020-02-26 16:27:17,752 EPOCH 133
2020-02-26 16:27:26,884 Epoch 133 Step:    14750 Batch Loss:     0.000032 Tokens per Sec:  9953303, Lr: 0.001000
2020-02-26 16:27:28,044 Epoch 133: total training loss 0.00432
2020-02-26 16:27:28,045 EPOCH 134
2020-02-26 16:27:38,158 Epoch 134: total training loss 0.00433
2020-02-26 16:27:38,159 EPOCH 135
2020-02-26 16:27:48,232 Epoch 135: total training loss 0.00435
2020-02-26 16:27:48,233 EPOCH 136
2020-02-26 16:27:49,694 Epoch 136 Step:    15000 Batch Loss:     0.000032 Tokens per Sec:  9973186, Lr: 0.001000
2020-02-26 16:27:58,258 Epoch 136: total training loss 0.00436
2020-02-26 16:27:58,258 EPOCH 137
2020-02-26 16:28:08,069 Epoch 137: total training loss 0.00438
2020-02-26 16:28:08,069 EPOCH 138
2020-02-26 16:28:11,847 Epoch 138 Step:    15250 Batch Loss:     0.000024 Tokens per Sec: 10407426, Lr: 0.001000
2020-02-26 16:28:18,054 Epoch 138: total training loss 0.00433
2020-02-26 16:28:18,055 EPOCH 139
2020-02-26 16:28:28,233 Epoch 139: total training loss 0.00427
2020-02-26 16:28:28,233 EPOCH 140
2020-02-26 16:28:34,988 Epoch 140 Step:    15500 Batch Loss:     0.000037 Tokens per Sec:  9910513, Lr: 0.001000
2020-02-26 16:28:38,741 Epoch 140: total training loss 0.00423
2020-02-26 16:28:38,742 EPOCH 141
2020-02-26 16:28:49,013 Epoch 141: total training loss 0.00429
2020-02-26 16:28:49,014 EPOCH 142
2020-02-26 16:28:58,136 Epoch 142 Step:    15750 Batch Loss:     0.000048 Tokens per Sec: 10003069, Lr: 0.001000
2020-02-26 16:28:59,235 Epoch 142: total training loss 0.00439
2020-02-26 16:28:59,235 EPOCH 143
2020-02-26 16:29:09,369 Epoch 143: total training loss 0.00429
2020-02-26 16:29:09,370 EPOCH 144
2020-02-26 16:29:19,434 Epoch 144: total training loss 0.00431
2020-02-26 16:29:19,434 EPOCH 145
2020-02-26 16:29:20,774 Epoch 145 Step:    16000 Batch Loss:     0.000039 Tokens per Sec:  9780012, Lr: 0.001000
2020-02-26 16:29:29,332 Epoch 145: total training loss 0.00431
2020-02-26 16:29:29,333 EPOCH 146
2020-02-26 16:29:39,196 Epoch 146: total training loss 0.00433
2020-02-26 16:29:39,197 EPOCH 147
2020-02-26 16:29:43,143 Epoch 147 Step:    16250 Batch Loss:     0.000020 Tokens per Sec: 10209398, Lr: 0.001000
2020-02-26 16:29:49,080 Epoch 147: total training loss 0.00432
2020-02-26 16:29:49,081 EPOCH 148
2020-02-26 16:29:59,054 Epoch 148: total training loss 0.00438
2020-02-26 16:29:59,055 EPOCH 149
2020-02-26 16:30:05,574 Epoch 149 Step:    16500 Batch Loss:     0.000039 Tokens per Sec:  9897143, Lr: 0.001000
2020-02-26 16:30:09,493 Epoch 149: total training loss 0.00433
2020-02-26 16:30:09,493 EPOCH 150
2020-02-26 16:30:20,154 Epoch 150: total training loss 0.00439
2020-02-26 16:30:20,154 EPOCH 151
2020-02-26 16:30:29,943 Epoch 151 Step:    16750 Batch Loss:     0.000045 Tokens per Sec:  9520275, Lr: 0.001000
2020-02-26 16:30:31,014 Epoch 151: total training loss 0.00426
2020-02-26 16:30:31,015 EPOCH 152
2020-02-26 16:30:41,727 Epoch 152: total training loss 0.00432
2020-02-26 16:30:41,727 EPOCH 153
2020-02-26 16:30:52,425 Epoch 153: total training loss 0.00432
2020-02-26 16:30:52,426 EPOCH 154
2020-02-26 16:30:54,254 Epoch 154 Step:    17000 Batch Loss:     0.000048 Tokens per Sec: 10284825, Lr: 0.001000
2020-02-26 16:31:02,401 Epoch 154: total training loss 0.00438
2020-02-26 16:31:02,401 EPOCH 155
2020-02-26 16:31:12,434 Epoch 155: total training loss 0.00433
2020-02-26 16:31:12,435 EPOCH 156
2020-02-26 16:31:16,525 Epoch 156 Step:    17250 Batch Loss:     0.000050 Tokens per Sec: 10307985, Lr: 0.001000
2020-02-26 16:31:22,362 Epoch 156: total training loss 0.00438
2020-02-26 16:31:22,363 EPOCH 157
2020-02-26 16:31:32,455 Epoch 157: total training loss 0.00434
2020-02-26 16:31:32,456 EPOCH 158
2020-02-26 16:31:39,093 Epoch 158 Step:    17500 Batch Loss:     0.000045 Tokens per Sec: 10327708, Lr: 0.001000
2020-02-26 16:31:42,517 Epoch 158: total training loss 0.00431
2020-02-26 16:31:42,518 EPOCH 159
2020-02-26 16:31:52,549 Epoch 159: total training loss 0.00429
2020-02-26 16:31:52,550 EPOCH 160
2020-02-26 16:32:01,491 Epoch 160 Step:    17750 Batch Loss:     0.000034 Tokens per Sec: 10384896, Lr: 0.001000
2020-02-26 16:32:02,464 Epoch 160: total training loss 0.00428
2020-02-26 16:32:02,464 EPOCH 161
2020-02-26 16:32:12,465 Epoch 161: total training loss 0.00432
2020-02-26 16:32:12,467 EPOCH 162
2020-02-26 16:32:22,760 Epoch 162: total training loss 0.00433
2020-02-26 16:32:22,760 EPOCH 163
2020-02-26 16:32:24,466 Epoch 163 Step:    18000 Batch Loss:     0.000039 Tokens per Sec:  9852158, Lr: 0.001000
2020-02-26 16:32:33,134 Epoch 163: total training loss 0.00439
2020-02-26 16:32:33,135 EPOCH 164
2020-02-26 16:32:43,941 Epoch 164: total training loss 0.00429
2020-02-26 16:32:43,941 EPOCH 165
2020-02-26 16:32:48,621 Epoch 165 Step:    18250 Batch Loss:     0.000040 Tokens per Sec:  9386826, Lr: 0.001000
2020-02-26 16:32:54,822 Epoch 165: total training loss 0.00430
2020-02-26 16:32:54,822 EPOCH 166
2020-02-26 16:33:05,458 Epoch 166: total training loss 0.00435
2020-02-26 16:33:05,458 EPOCH 167
2020-02-26 16:33:12,274 Epoch 167 Step:    18500 Batch Loss:     0.000051 Tokens per Sec: 10373381, Lr: 0.001000
2020-02-26 16:33:15,367 Epoch 167: total training loss 0.00436
2020-02-26 16:33:15,367 EPOCH 168
2020-02-26 16:33:25,401 Epoch 168: total training loss 0.00435
2020-02-26 16:33:25,402 EPOCH 169
2020-02-26 16:33:34,687 Epoch 169 Step:    18750 Batch Loss:     0.000020 Tokens per Sec: 10185129, Lr: 0.001000
2020-02-26 16:33:35,518 Epoch 169: total training loss 0.00434
2020-02-26 16:33:35,518 EPOCH 170
2020-02-26 16:33:45,542 Epoch 170: total training loss 0.00433
2020-02-26 16:33:45,543 EPOCH 171
2020-02-26 16:33:55,394 Epoch 171: total training loss 0.00430
2020-02-26 16:33:55,395 EPOCH 172
2020-02-26 16:33:57,079 Epoch 172 Step:    19000 Batch Loss:     0.000041 Tokens per Sec: 10501683, Lr: 0.001000
2020-02-26 16:34:05,184 Epoch 172: total training loss 0.00433
2020-02-26 16:34:05,185 EPOCH 173
2020-02-26 16:34:15,442 Epoch 173: total training loss 0.00427
2020-02-26 16:34:15,442 EPOCH 174
2020-02-26 16:34:19,728 Epoch 174 Step:    19250 Batch Loss:     0.000029 Tokens per Sec: 10130506, Lr: 0.001000
2020-02-26 16:34:25,491 Epoch 174: total training loss 0.00433
2020-02-26 16:34:25,492 EPOCH 175
2020-02-26 16:34:35,856 Epoch 175: total training loss 0.00432
2020-02-26 16:34:35,857 EPOCH 176
2020-02-26 16:34:42,869 Epoch 176 Step:    19500 Batch Loss:     0.000032 Tokens per Sec: 10029693, Lr: 0.001000
2020-02-26 16:34:46,079 Epoch 176: total training loss 0.00431
2020-02-26 16:34:46,079 EPOCH 177
2020-02-26 16:34:56,122 Epoch 177: total training loss 0.00426
2020-02-26 16:34:56,123 EPOCH 178
2020-02-26 16:35:05,376 Epoch 178 Step:    19750 Batch Loss:     0.000037 Tokens per Sec: 10361711, Lr: 0.001000
2020-02-26 16:35:06,059 Epoch 178: total training loss 0.00431
2020-02-26 16:35:06,059 EPOCH 179
2020-02-26 16:35:15,993 Epoch 179: total training loss 0.00430
2020-02-26 16:35:15,993 EPOCH 180
2020-02-26 16:35:25,852 Epoch 180: total training loss 0.00434
2020-02-26 16:35:25,852 EPOCH 181
2020-02-26 16:35:27,659 Epoch 181 Step:    20000 Batch Loss:     0.000041 Tokens per Sec: 10118970, Lr: 0.001000
2020-02-26 16:35:27,659 Model noise rate: 5
2020-02-26 16:36:14,563 Hooray! New best validation result [dtw]!
2020-02-26 16:36:14,565 Saving new checkpoint.
2020-02-26 16:36:30,020 Validation result at epoch 181, step    20000: Val DTW Score:  23.21, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0152, GT DTW Score:      nan, duration: 62.3604s
2020-02-26 16:36:38,362 Epoch 181: total training loss 0.00432
2020-02-26 16:36:38,363 EPOCH 182
2020-02-26 16:36:48,214 Epoch 182: total training loss 0.00434
2020-02-26 16:36:48,214 EPOCH 183
2020-02-26 16:36:52,536 Epoch 183 Step:    20250 Batch Loss:     0.000042 Tokens per Sec: 10424316, Lr: 0.001000
2020-02-26 16:36:58,082 Epoch 183: total training loss 0.00425
2020-02-26 16:36:58,082 EPOCH 184
2020-02-26 16:37:07,881 Epoch 184: total training loss 0.00435
2020-02-26 16:37:07,882 EPOCH 185
2020-02-26 16:37:14,645 Epoch 185 Step:    20500 Batch Loss:     0.000054 Tokens per Sec: 10494562, Lr: 0.001000
2020-02-26 16:37:17,756 Epoch 185: total training loss 0.00427
2020-02-26 16:37:17,757 EPOCH 186
2020-02-26 16:37:27,599 Epoch 186: total training loss 0.00428
2020-02-26 16:37:27,600 EPOCH 187
2020-02-26 16:37:36,965 Epoch 187 Step:    20750 Batch Loss:     0.000042 Tokens per Sec: 10438965, Lr: 0.001000
2020-02-26 16:37:37,541 Epoch 187: total training loss 0.00426
2020-02-26 16:37:37,542 EPOCH 188
2020-02-26 16:37:47,510 Epoch 188: total training loss 0.00430
2020-02-26 16:37:47,511 EPOCH 189
2020-02-26 16:37:57,381 Epoch 189: total training loss 0.00430
2020-02-26 16:37:57,382 EPOCH 190
2020-02-26 16:37:59,179 Epoch 190 Step:    21000 Batch Loss:     0.000041 Tokens per Sec: 10433194, Lr: 0.001000
2020-02-26 16:38:07,152 Epoch 190: total training loss 0.00426
2020-02-26 16:38:07,152 EPOCH 191
2020-02-26 16:38:17,069 Epoch 191: total training loss 0.00415
2020-02-26 16:38:17,069 EPOCH 192
2020-02-26 16:38:21,276 Epoch 192 Step:    21250 Batch Loss:     0.000039 Tokens per Sec: 10208108, Lr: 0.001000
2020-02-26 16:38:26,912 Epoch 192: total training loss 0.00423
2020-02-26 16:38:26,912 EPOCH 193
2020-02-26 16:38:37,028 Epoch 193: total training loss 0.00420
2020-02-26 16:38:37,039 EPOCH 194
2020-02-26 16:38:44,488 Epoch 194 Step:    21500 Batch Loss:     0.000032 Tokens per Sec:  9660152, Lr: 0.001000
2020-02-26 16:38:47,586 Epoch 194: total training loss 0.00424
2020-02-26 16:38:47,587 EPOCH 195
2020-02-26 16:38:57,721 Epoch 195: total training loss 0.00425
2020-02-26 16:38:57,722 EPOCH 196
2020-02-26 16:39:07,454 Epoch 196 Step:    21750 Batch Loss:     0.000044 Tokens per Sec: 10042096, Lr: 0.001000
2020-02-26 16:39:08,000 Epoch 196: total training loss 0.00415
2020-02-26 16:39:08,000 EPOCH 197
2020-02-26 16:39:18,231 Epoch 197: total training loss 0.00419
2020-02-26 16:39:18,231 EPOCH 198
2020-02-26 16:39:28,062 Epoch 198: total training loss 0.00425
2020-02-26 16:39:28,062 EPOCH 199
2020-02-26 16:39:29,997 Epoch 199 Step:    22000 Batch Loss:     0.000047 Tokens per Sec: 10208609, Lr: 0.001000
2020-02-26 16:39:37,958 Epoch 199: total training loss 0.00420
2020-02-26 16:39:37,959 EPOCH 200
2020-02-26 16:39:47,974 Epoch 200: total training loss 0.00425
2020-02-26 16:39:47,975 EPOCH 201
2020-02-26 16:39:52,378 Epoch 201 Step:    22250 Batch Loss:     0.000044 Tokens per Sec: 10330040, Lr: 0.001000
2020-02-26 16:39:57,934 Epoch 201: total training loss 0.00419
2020-02-26 16:39:57,935 EPOCH 202
2020-02-26 16:40:07,972 Epoch 202: total training loss 0.00414
2020-02-26 16:40:07,973 EPOCH 203
2020-02-26 16:40:14,827 Epoch 203 Step:    22500 Batch Loss:     0.000028 Tokens per Sec: 10108419, Lr: 0.001000
2020-02-26 16:40:18,060 Epoch 203: total training loss 0.00423
2020-02-26 16:40:18,061 EPOCH 204
2020-02-26 16:40:28,642 Epoch 204: total training loss 0.00413
2020-02-26 16:40:28,643 EPOCH 205
2020-02-26 16:40:38,665 Epoch 205 Step:    22750 Batch Loss:     0.000028 Tokens per Sec:  9737820, Lr: 0.001000
2020-02-26 16:40:39,171 Epoch 205: total training loss 0.00413
2020-02-26 16:40:39,171 EPOCH 206
2020-02-26 16:40:49,752 Epoch 206: total training loss 0.00418
2020-02-26 16:40:49,753 EPOCH 207
2020-02-26 16:41:00,098 Epoch 207: total training loss 0.00414
2020-02-26 16:41:00,099 EPOCH 208
2020-02-26 16:41:02,292 Epoch 208 Step:    23000 Batch Loss:     0.000031 Tokens per Sec:  9724321, Lr: 0.001000
2020-02-26 16:41:10,299 Epoch 208: total training loss 0.00417
2020-02-26 16:41:10,300 EPOCH 209
2020-02-26 16:41:20,315 Epoch 209: total training loss 0.00410
2020-02-26 16:41:20,316 EPOCH 210
2020-02-26 16:41:25,037 Epoch 210 Step:    23250 Batch Loss:     0.000035 Tokens per Sec: 10173791, Lr: 0.001000
2020-02-26 16:41:30,380 Epoch 210: total training loss 0.00414
2020-02-26 16:41:30,380 EPOCH 211
2020-02-26 16:41:40,450 Epoch 211: total training loss 0.00405
2020-02-26 16:41:40,450 EPOCH 212
2020-02-26 16:41:47,491 Epoch 212 Step:    23500 Batch Loss:     0.000016 Tokens per Sec: 10371868, Lr: 0.001000
2020-02-26 16:41:50,391 Epoch 212: total training loss 0.00412
2020-02-26 16:41:50,392 EPOCH 213
2020-02-26 16:42:00,419 Epoch 213: total training loss 0.00406
2020-02-26 16:42:00,420 EPOCH 214
2020-02-26 16:42:10,316 Epoch 214 Step:    23750 Batch Loss:     0.000037 Tokens per Sec:  9969591, Lr: 0.001000
2020-02-26 16:42:10,779 Epoch 214: total training loss 0.00419
2020-02-26 16:42:10,779 EPOCH 215
2020-02-26 16:42:20,961 Epoch 215: total training loss 0.00407
2020-02-26 16:42:20,962 EPOCH 216
2020-02-26 16:42:31,004 Epoch 216: total training loss 0.00409
2020-02-26 16:42:31,005 EPOCH 217
2020-02-26 16:42:33,159 Epoch 217 Step:    24000 Batch Loss:     0.000023 Tokens per Sec: 10181172, Lr: 0.001000
2020-02-26 16:42:41,219 Epoch 217: total training loss 0.00402
2020-02-26 16:42:41,220 EPOCH 218
2020-02-26 16:42:51,460 Epoch 218: total training loss 0.00407
2020-02-26 16:42:51,460 EPOCH 219
2020-02-26 16:42:55,887 Epoch 219 Step:    24250 Batch Loss:     0.000039 Tokens per Sec:  9986320, Lr: 0.001000
2020-02-26 16:43:01,439 Epoch 219: total training loss 0.00411
2020-02-26 16:43:01,440 EPOCH 220
2020-02-26 16:43:11,371 Epoch 220: total training loss 0.00404
2020-02-26 16:43:11,371 EPOCH 221
2020-02-26 16:43:18,768 Epoch 221 Step:    24500 Batch Loss:     0.000046 Tokens per Sec: 10170864, Lr: 0.001000
2020-02-26 16:43:21,567 Epoch 221: total training loss 0.00402
2020-02-26 16:43:21,568 EPOCH 222
2020-02-26 16:43:31,503 Epoch 222: total training loss 0.00407
2020-02-26 16:43:31,503 EPOCH 223
2020-02-26 16:43:41,220 Epoch 223 Step:    24750 Batch Loss:     0.000031 Tokens per Sec: 10330934, Lr: 0.001000
2020-02-26 16:43:41,470 Epoch 223: total training loss 0.00405
2020-02-26 16:43:41,470 EPOCH 224
2020-02-26 16:43:51,434 Epoch 224: total training loss 0.00409
2020-02-26 16:43:51,435 EPOCH 225
2020-02-26 16:44:01,449 Epoch 225: total training loss 0.00402
2020-02-26 16:44:01,449 EPOCH 226
2020-02-26 16:44:03,810 Epoch 226 Step:    25000 Batch Loss:     0.000038 Tokens per Sec: 10215853, Lr: 0.001000
2020-02-26 16:44:11,618 Epoch 226: total training loss 0.00403
2020-02-26 16:44:11,618 EPOCH 227
2020-02-26 16:44:21,879 Epoch 227: total training loss 0.00405
2020-02-26 16:44:21,880 EPOCH 228
2020-02-26 16:44:26,789 Epoch 228 Step:    25250 Batch Loss:     0.000024 Tokens per Sec: 10472613, Lr: 0.001000
2020-02-26 16:44:31,751 Epoch 228: total training loss 0.00403
2020-02-26 16:44:31,752 EPOCH 229
2020-02-26 16:44:41,770 Epoch 229: total training loss 0.00398
2020-02-26 16:44:41,770 EPOCH 230
2020-02-26 16:44:48,991 Epoch 230 Step:    25500 Batch Loss:     0.000035 Tokens per Sec: 10302562, Lr: 0.001000
2020-02-26 16:44:51,657 Epoch 230: total training loss 0.00402
2020-02-26 16:44:51,658 EPOCH 231
2020-02-26 16:45:01,450 Epoch 231: total training loss 0.00397
2020-02-26 16:45:01,451 EPOCH 232
2020-02-26 16:45:11,229 Epoch 232 Step:    25750 Batch Loss:     0.000043 Tokens per Sec: 10409026, Lr: 0.001000
2020-02-26 16:45:11,418 Epoch 232: total training loss 0.00395
2020-02-26 16:45:11,418 EPOCH 233
2020-02-26 16:45:21,401 Epoch 233: total training loss 0.00399
2020-02-26 16:45:21,402 EPOCH 234
2020-02-26 16:45:31,736 Epoch 234: total training loss 0.00394
2020-02-26 16:45:31,736 EPOCH 235
2020-02-26 16:45:34,191 Epoch 235 Step:    26000 Batch Loss:     0.000047 Tokens per Sec:  9610891, Lr: 0.001000
2020-02-26 16:45:42,324 Epoch 235: total training loss 0.00399
2020-02-26 16:45:42,325 EPOCH 236
2020-02-26 16:45:52,942 Epoch 236: total training loss 0.00390
2020-02-26 16:45:52,943 EPOCH 237
2020-02-26 16:45:58,284 Epoch 237 Step:    26250 Batch Loss:     0.000030 Tokens per Sec:  9418285, Lr: 0.001000
2020-02-26 16:46:03,776 Epoch 237: total training loss 0.00388
2020-02-26 16:46:03,776 EPOCH 238
2020-02-26 16:46:14,346 Epoch 238: total training loss 0.00386
2020-02-26 16:46:14,347 EPOCH 239
2020-02-26 16:46:21,779 Epoch 239 Step:    26500 Batch Loss:     0.000026 Tokens per Sec: 10147424, Lr: 0.001000
2020-02-26 16:46:24,456 Epoch 239: total training loss 0.00386
2020-02-26 16:46:24,456 EPOCH 240
2020-02-26 16:46:34,502 Epoch 240: total training loss 0.00389
2020-02-26 16:46:34,503 EPOCH 241
2020-02-26 16:46:44,462 Epoch 241 Step:    26750 Batch Loss:     0.000040 Tokens per Sec: 10211814, Lr: 0.001000
2020-02-26 16:46:44,535 Epoch 241: total training loss 0.00389
2020-02-26 16:46:44,535 EPOCH 242
2020-02-26 16:46:54,505 Epoch 242: total training loss 0.00387
2020-02-26 16:46:54,505 EPOCH 243
2020-02-26 16:47:04,467 Epoch 243: total training loss 0.00384
2020-02-26 16:47:04,468 EPOCH 244
2020-02-26 16:47:06,982 Epoch 244 Step:    27000 Batch Loss:     0.000040 Tokens per Sec: 10464431, Lr: 0.001000
2020-02-26 16:47:14,535 Epoch 244: total training loss 0.00381
2020-02-26 16:47:14,537 EPOCH 245
2020-02-26 16:47:24,827 Epoch 245: total training loss 0.00381
2020-02-26 16:47:24,828 EPOCH 246
2020-02-26 16:47:29,810 Epoch 246 Step:    27250 Batch Loss:     0.000029 Tokens per Sec: 10003626, Lr: 0.001000
2020-02-26 16:47:35,044 Epoch 246: total training loss 0.00386
2020-02-26 16:47:35,044 EPOCH 247
2020-02-26 16:47:45,280 Epoch 247: total training loss 0.00379
2020-02-26 16:47:45,281 EPOCH 248
2020-02-26 16:47:53,289 Epoch 248 Step:    27500 Batch Loss:     0.000028 Tokens per Sec:  9830353, Lr: 0.001000
2020-02-26 16:47:55,856 Epoch 248: total training loss 0.00377
2020-02-26 16:47:55,857 EPOCH 249
2020-02-26 16:48:06,164 Epoch 249: total training loss 0.00380
2020-02-26 16:48:06,165 EPOCH 250
2020-02-26 16:48:16,226 Epoch 250 Step:    27750 Batch Loss:     0.000034 Tokens per Sec: 10224523, Lr: 0.001000
2020-02-26 16:48:16,227 Epoch 250: total training loss 0.00374
2020-02-26 16:48:16,227 EPOCH 251
2020-02-26 16:48:26,131 Epoch 251: total training loss 0.00382
2020-02-26 16:48:26,132 EPOCH 252
2020-02-26 16:48:36,072 Epoch 252: total training loss 0.00372
2020-02-26 16:48:36,073 EPOCH 253
2020-02-26 16:48:38,577 Epoch 253 Step:    28000 Batch Loss:     0.000035 Tokens per Sec: 10356949, Lr: 0.001000
2020-02-26 16:48:45,818 Epoch 253: total training loss 0.00371
2020-02-26 16:48:45,818 EPOCH 254
2020-02-26 16:48:55,732 Epoch 254: total training loss 0.00383
2020-02-26 16:48:55,733 EPOCH 255
2020-02-26 16:49:00,862 Epoch 255 Step:    28250 Batch Loss:     0.000024 Tokens per Sec: 10464870, Lr: 0.001000
2020-02-26 16:49:05,715 Epoch 255: total training loss 0.00371
2020-02-26 16:49:05,716 EPOCH 256
2020-02-26 16:49:15,995 Epoch 256: total training loss 0.00368
2020-02-26 16:49:15,995 EPOCH 257
2020-02-26 16:49:23,725 Epoch 257 Step:    28500 Batch Loss:     0.000040 Tokens per Sec:  9937336, Lr: 0.001000
2020-02-26 16:49:26,345 Epoch 257: total training loss 0.00369
2020-02-26 16:49:26,346 EPOCH 258
2020-02-26 16:49:36,423 Epoch 258: total training loss 0.00369
2020-02-26 16:49:36,423 EPOCH 259
2020-02-26 16:49:46,681 Epoch 259: total training loss 0.00361
2020-02-26 16:49:46,682 EPOCH 260
2020-02-26 16:49:46,810 Epoch 260 Step:    28750 Batch Loss:     0.000041 Tokens per Sec:  8684084, Lr: 0.001000
2020-02-26 16:49:56,722 Epoch 260: total training loss 0.00360
2020-02-26 16:49:56,722 EPOCH 261
2020-02-26 16:50:06,852 Epoch 261: total training loss 0.00361
2020-02-26 16:50:06,852 EPOCH 262
2020-02-26 16:50:09,428 Epoch 262 Step:    29000 Batch Loss:     0.000033 Tokens per Sec: 10310669, Lr: 0.001000
2020-02-26 16:50:16,905 Epoch 262: total training loss 0.00367
2020-02-26 16:50:16,906 EPOCH 263
2020-02-26 16:50:27,157 Epoch 263: total training loss 0.00360
2020-02-26 16:50:27,159 EPOCH 264
2020-02-26 16:50:32,376 Epoch 264 Step:    29250 Batch Loss:     0.000037 Tokens per Sec: 10278238, Lr: 0.001000
2020-02-26 16:50:37,137 Epoch 264: total training loss 0.00351
2020-02-26 16:50:37,137 EPOCH 265
2020-02-26 16:50:47,001 Epoch 265: total training loss 0.00354
2020-02-26 16:50:47,002 EPOCH 266
2020-02-26 16:50:54,467 Epoch 266 Step:    29500 Batch Loss:     0.000043 Tokens per Sec: 10444286, Lr: 0.001000
2020-02-26 16:50:56,940 Epoch 266: total training loss 0.00351
2020-02-26 16:50:56,940 EPOCH 267
2020-02-26 16:51:07,093 Epoch 267: total training loss 0.00351
2020-02-26 16:51:07,093 EPOCH 268
2020-02-26 16:51:17,593 Epoch 268: total training loss 0.00356
2020-02-26 16:51:17,593 EPOCH 269
2020-02-26 16:51:17,778 Epoch 269 Step:    29750 Batch Loss:     0.000030 Tokens per Sec:  9046903, Lr: 0.001000
2020-02-26 16:51:27,682 Epoch 269: total training loss 0.00355
2020-02-26 16:51:27,683 EPOCH 270
2020-02-26 16:51:38,101 Epoch 270: total training loss 0.00356
2020-02-26 16:51:38,102 EPOCH 271
2020-02-26 16:51:40,810 Epoch 271 Step:    30000 Batch Loss:     0.000029 Tokens per Sec:  9743796, Lr: 0.001000
2020-02-26 16:51:40,811 Model noise rate: 5
2020-02-26 16:52:27,974 Hooray! New best validation result [dtw]!
2020-02-26 16:52:27,974 Saving new checkpoint.
2020-02-26 16:52:43,713 Validation result at epoch 271, step    30000: Val DTW Score:  12.47, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0160, GT DTW Score:      nan, duration: 62.9017s
2020-02-26 16:52:51,650 Epoch 271: total training loss 0.00348
2020-02-26 16:52:51,652 EPOCH 272
2020-02-26 16:53:02,304 Epoch 272: total training loss 0.00347
2020-02-26 16:53:02,305 EPOCH 273
2020-02-26 16:53:07,781 Epoch 273 Step:    30250 Batch Loss:     0.000046 Tokens per Sec:  9708031, Lr: 0.001000
2020-02-26 16:53:12,971 Epoch 273: total training loss 0.00342
2020-02-26 16:53:12,971 EPOCH 274
2020-02-26 16:53:23,465 Epoch 274: total training loss 0.00341
2020-02-26 16:53:23,465 EPOCH 275
2020-02-26 16:53:31,352 Epoch 275 Step:    30500 Batch Loss:     0.000025 Tokens per Sec: 10219344, Lr: 0.001000
2020-02-26 16:53:33,558 Epoch 275: total training loss 0.00340
2020-02-26 16:53:33,558 EPOCH 276
2020-02-26 16:53:43,526 Epoch 276: total training loss 0.00339
2020-02-26 16:53:43,527 EPOCH 277
2020-02-26 16:53:53,551 Epoch 277: total training loss 0.00337
2020-02-26 16:53:53,552 EPOCH 278
2020-02-26 16:53:53,845 Epoch 278 Step:    30750 Batch Loss:     0.000035 Tokens per Sec:  9557913, Lr: 0.001000
2020-02-26 16:54:03,561 Epoch 278: total training loss 0.00338
2020-02-26 16:54:03,562 EPOCH 279
2020-02-26 16:54:13,462 Epoch 279: total training loss 0.00339
2020-02-26 16:54:13,463 EPOCH 280
2020-02-26 16:54:16,296 Epoch 280 Step:    31000 Batch Loss:     0.000027 Tokens per Sec: 10037744, Lr: 0.001000
2020-02-26 16:54:23,408 Epoch 280: total training loss 0.00330
2020-02-26 16:54:23,409 EPOCH 281
2020-02-26 16:54:33,373 Epoch 281: total training loss 0.00334
2020-02-26 16:54:33,375 EPOCH 282
2020-02-26 16:54:38,627 Epoch 282 Step:    31250 Batch Loss:     0.000025 Tokens per Sec: 10133077, Lr: 0.001000
2020-02-26 16:54:43,466 Epoch 282: total training loss 0.00334
2020-02-26 16:54:43,466 EPOCH 283
2020-02-26 16:54:53,569 Epoch 283: total training loss 0.00328
2020-02-26 16:54:53,570 EPOCH 284
2020-02-26 16:55:01,419 Epoch 284 Step:    31500 Batch Loss:     0.000018 Tokens per Sec: 10180084, Lr: 0.001000
2020-02-26 16:55:03,660 Epoch 284: total training loss 0.00331
2020-02-26 16:55:03,661 EPOCH 285
2020-02-26 16:55:13,706 Epoch 285: total training loss 0.00329
2020-02-26 16:55:13,707 EPOCH 286
2020-02-26 16:55:23,885 Epoch 286: total training loss 0.00322
2020-02-26 16:55:23,888 EPOCH 287
2020-02-26 16:55:24,197 Epoch 287 Step:    31750 Batch Loss:     0.000039 Tokens per Sec:  9456756, Lr: 0.001000
2020-02-26 16:55:33,790 Epoch 287: total training loss 0.00327
2020-02-26 16:55:33,791 EPOCH 288
2020-02-26 16:55:43,786 Epoch 288: total training loss 0.00329
2020-02-26 16:55:43,788 EPOCH 289
2020-02-26 16:55:46,744 Epoch 289 Step:    32000 Batch Loss:     0.000047 Tokens per Sec: 10118128, Lr: 0.001000
2020-02-26 16:55:53,880 Epoch 289: total training loss 0.00325
2020-02-26 16:55:53,880 EPOCH 290
2020-02-26 16:56:03,781 Epoch 290: total training loss 0.00322
2020-02-26 16:56:03,782 EPOCH 291
2020-02-26 16:56:09,261 Epoch 291 Step:    32250 Batch Loss:     0.000030 Tokens per Sec: 10506205, Lr: 0.001000
2020-02-26 16:56:13,638 Epoch 291: total training loss 0.00318
2020-02-26 16:56:13,639 EPOCH 292
2020-02-26 16:56:23,583 Epoch 292: total training loss 0.00325
2020-02-26 16:56:23,585 EPOCH 293
2020-02-26 16:56:31,698 Epoch 293 Step:    32500 Batch Loss:     0.000019 Tokens per Sec: 10035521, Lr: 0.001000
2020-02-26 16:56:33,846 Epoch 293: total training loss 0.00323
2020-02-26 16:56:33,846 EPOCH 294
2020-02-26 16:56:44,120 Epoch 294: total training loss 0.00325
2020-02-26 16:56:44,120 EPOCH 295
2020-02-26 16:56:54,222 Epoch 295: total training loss 0.00323
2020-02-26 16:56:54,222 EPOCH 296
2020-02-26 16:56:54,689 Epoch 296 Step:    32750 Batch Loss:     0.000035 Tokens per Sec:  9875013, Lr: 0.001000
2020-02-26 16:57:04,380 Epoch 296: total training loss 0.00325
2020-02-26 16:57:04,380 EPOCH 297
2020-02-26 16:57:14,518 Epoch 297: total training loss 0.00315
2020-02-26 16:57:14,518 EPOCH 298
2020-02-26 16:57:17,489 Epoch 298 Step:    33000 Batch Loss:     0.000029 Tokens per Sec: 10173325, Lr: 0.001000
2020-02-26 16:57:24,634 Epoch 298: total training loss 0.00313
2020-02-26 16:57:24,635 EPOCH 299
2020-02-26 16:57:34,675 Epoch 299: total training loss 0.00314
2020-02-26 16:57:34,675 EPOCH 300
2020-02-26 16:57:40,190 Epoch 300 Step:    33250 Batch Loss:     0.000026 Tokens per Sec: 10238851, Lr: 0.001000
2020-02-26 16:57:44,820 Epoch 300: total training loss 0.00310
2020-02-26 16:57:44,821 EPOCH 301
2020-02-26 16:57:54,838 Epoch 301: total training loss 0.00313
2020-02-26 16:57:54,838 EPOCH 302
2020-02-26 16:58:02,997 Epoch 302 Step:    33500 Batch Loss:     0.000024 Tokens per Sec: 10299896, Lr: 0.001000
2020-02-26 16:58:04,908 Epoch 302: total training loss 0.00302
2020-02-26 16:58:04,909 EPOCH 303
2020-02-26 16:58:14,926 Epoch 303: total training loss 0.00309
2020-02-26 16:58:14,927 EPOCH 304
2020-02-26 16:58:25,295 Epoch 304: total training loss 0.00309
2020-02-26 16:58:25,295 EPOCH 305
2020-02-26 16:58:25,945 Epoch 305 Step:    33750 Batch Loss:     0.000030 Tokens per Sec:  9624385, Lr: 0.001000
2020-02-26 16:58:35,693 Epoch 305: total training loss 0.00305
2020-02-26 16:58:35,693 EPOCH 306
2020-02-26 16:58:45,988 Epoch 306: total training loss 0.00302
2020-02-26 16:58:45,989 EPOCH 307
2020-02-26 16:58:49,129 Epoch 307 Step:    34000 Batch Loss:     0.000017 Tokens per Sec:  9873487, Lr: 0.001000
2020-02-26 16:58:56,381 Epoch 307: total training loss 0.00309
2020-02-26 16:58:56,382 EPOCH 308
2020-02-26 16:59:06,553 Epoch 308: total training loss 0.00297
2020-02-26 16:59:06,554 EPOCH 309
2020-02-26 16:59:12,100 Epoch 309 Step:    34250 Batch Loss:     0.000036 Tokens per Sec: 10194379, Lr: 0.001000
2020-02-26 16:59:16,571 Epoch 309: total training loss 0.00306
2020-02-26 16:59:16,571 EPOCH 310
2020-02-26 16:59:26,672 Epoch 310: total training loss 0.00300
2020-02-26 16:59:26,672 EPOCH 311
2020-02-26 16:59:34,690 Epoch 311 Step:    34500 Batch Loss:     0.000016 Tokens per Sec: 10241752, Lr: 0.001000
2020-02-26 16:59:36,658 Epoch 311: total training loss 0.00312
2020-02-26 16:59:36,659 EPOCH 312
2020-02-26 16:59:46,767 Epoch 312: total training loss 0.00291
2020-02-26 16:59:46,768 EPOCH 313
2020-02-26 16:59:56,744 Epoch 313: total training loss 0.00294
2020-02-26 16:59:56,744 EPOCH 314
2020-02-26 16:59:57,336 Epoch 314 Step:    34750 Batch Loss:     0.000022 Tokens per Sec:  9450375, Lr: 0.001000
2020-02-26 17:00:06,706 Epoch 314: total training loss 0.00297
2020-02-26 17:00:06,706 EPOCH 315
2020-02-26 17:00:16,528 Epoch 315: total training loss 0.00292
2020-02-26 17:00:16,528 EPOCH 316
2020-02-26 17:00:19,743 Epoch 316 Step:    35000 Batch Loss:     0.000032 Tokens per Sec: 10033087, Lr: 0.001000
2020-02-26 17:00:26,526 Epoch 316: total training loss 0.00298
2020-02-26 17:00:26,528 EPOCH 317
2020-02-26 17:00:36,567 Epoch 317: total training loss 0.00286
2020-02-26 17:00:36,567 EPOCH 318
2020-02-26 17:00:42,169 Epoch 318 Step:    35250 Batch Loss:     0.000028 Tokens per Sec: 10372438, Lr: 0.001000
2020-02-26 17:00:46,473 Epoch 318: total training loss 0.00287
2020-02-26 17:00:46,473 EPOCH 319
2020-02-26 17:00:56,612 Epoch 319: total training loss 0.00283
2020-02-26 17:00:56,613 EPOCH 320
2020-02-26 17:01:05,239 Epoch 320 Step:    35500 Batch Loss:     0.000011 Tokens per Sec:  9845371, Lr: 0.001000
2020-02-26 17:01:07,035 Epoch 320: total training loss 0.00288
2020-02-26 17:01:07,036 EPOCH 321
2020-02-26 17:01:17,170 Epoch 321: total training loss 0.00286
2020-02-26 17:01:17,171 EPOCH 322
2020-02-26 17:01:27,404 Epoch 322: total training loss 0.00283
2020-02-26 17:01:27,404 EPOCH 323
2020-02-26 17:01:28,237 Epoch 323 Step:    35750 Batch Loss:     0.000029 Tokens per Sec:  9602604, Lr: 0.001000
2020-02-26 17:01:37,675 Epoch 323: total training loss 0.00284
2020-02-26 17:01:37,676 EPOCH 324
2020-02-26 17:01:47,773 Epoch 324: total training loss 0.00280
2020-02-26 17:01:47,773 EPOCH 325
2020-02-26 17:01:50,862 Epoch 325 Step:    36000 Batch Loss:     0.000019 Tokens per Sec: 10077571, Lr: 0.001000
2020-02-26 17:01:57,753 Epoch 325: total training loss 0.00282
2020-02-26 17:01:57,754 EPOCH 326
2020-02-26 17:02:07,948 Epoch 326: total training loss 0.00284
2020-02-26 17:02:07,948 EPOCH 327
2020-02-26 17:02:13,691 Epoch 327 Step:    36250 Batch Loss:     0.000036 Tokens per Sec: 10135167, Lr: 0.001000
2020-02-26 17:02:18,081 Epoch 327: total training loss 0.00281
2020-02-26 17:02:18,081 EPOCH 328
2020-02-26 17:02:28,019 Epoch 328: total training loss 0.00273
2020-02-26 17:02:28,020 EPOCH 329
2020-02-26 17:02:36,030 Epoch 329 Step:    36500 Batch Loss:     0.000024 Tokens per Sec: 10253185, Lr: 0.001000
2020-02-26 17:02:37,988 Epoch 329: total training loss 0.00274
2020-02-26 17:02:37,989 EPOCH 330
2020-02-26 17:02:48,055 Epoch 330: total training loss 0.00272
2020-02-26 17:02:48,056 EPOCH 331
2020-02-26 17:02:58,317 Epoch 331: total training loss 0.00275
2020-02-26 17:02:58,319 EPOCH 332
2020-02-26 17:02:59,164 Epoch 332 Step:    36750 Batch Loss:     0.000016 Tokens per Sec:  9388010, Lr: 0.001000
2020-02-26 17:03:08,808 Epoch 332: total training loss 0.00274
2020-02-26 17:03:08,810 EPOCH 333
2020-02-26 17:03:19,227 Epoch 333: total training loss 0.00282
2020-02-26 17:03:19,228 EPOCH 334
2020-02-26 17:03:22,560 Epoch 334 Step:    37000 Batch Loss:     0.000015 Tokens per Sec:  9693766, Lr: 0.001000
2020-02-26 17:03:29,833 Epoch 334: total training loss 0.00272
2020-02-26 17:03:29,834 EPOCH 335
2020-02-26 17:03:40,066 Epoch 335: total training loss 0.00270
2020-02-26 17:03:40,067 EPOCH 336
2020-02-26 17:03:46,068 Epoch 336 Step:    37250 Batch Loss:     0.000034 Tokens per Sec: 10184126, Lr: 0.001000
2020-02-26 17:03:50,145 Epoch 336: total training loss 0.00266
2020-02-26 17:03:50,146 EPOCH 337
2020-02-26 17:04:00,110 Epoch 337: total training loss 0.00268
2020-02-26 17:04:00,111 EPOCH 338
2020-02-26 17:04:08,450 Epoch 338 Step:    37500 Batch Loss:     0.000024 Tokens per Sec: 10199865, Lr: 0.001000
2020-02-26 17:04:10,113 Epoch 338: total training loss 0.00281
2020-02-26 17:04:10,113 EPOCH 339
2020-02-26 17:04:20,029 Epoch 339: total training loss 0.00270
2020-02-26 17:04:20,030 EPOCH 340
2020-02-26 17:04:29,968 Epoch 340: total training loss 0.00274
2020-02-26 17:04:29,968 EPOCH 341
2020-02-26 17:04:30,944 Epoch 341 Step:    37750 Batch Loss:     0.000025 Tokens per Sec: 10769398, Lr: 0.001000
2020-02-26 17:04:39,830 Epoch 341: total training loss 0.00267
2020-02-26 17:04:39,831 EPOCH 342
2020-02-26 17:04:50,058 Epoch 342: total training loss 0.00269
2020-02-26 17:04:50,058 EPOCH 343
2020-02-26 17:04:53,711 Epoch 343 Step:    38000 Batch Loss:     0.000018 Tokens per Sec: 10249069, Lr: 0.001000
2020-02-26 17:05:00,173 Epoch 343: total training loss 0.00262
2020-02-26 17:05:00,174 EPOCH 344
2020-02-26 17:05:10,189 Epoch 344: total training loss 0.00264
2020-02-26 17:05:10,190 EPOCH 345
2020-02-26 17:05:16,260 Epoch 345 Step:    38250 Batch Loss:     0.000027 Tokens per Sec: 10058478, Lr: 0.001000
2020-02-26 17:05:20,379 Epoch 345: total training loss 0.00261
2020-02-26 17:05:20,379 EPOCH 346
2020-02-26 17:05:30,374 Epoch 346: total training loss 0.00258
2020-02-26 17:05:30,376 EPOCH 347
2020-02-26 17:05:38,646 Epoch 347 Step:    38500 Batch Loss:     0.000025 Tokens per Sec: 10418069, Lr: 0.001000
2020-02-26 17:05:40,198 Epoch 347: total training loss 0.00263
2020-02-26 17:05:40,198 EPOCH 348
2020-02-26 17:05:50,131 Epoch 348: total training loss 0.00265
2020-02-26 17:05:50,145 EPOCH 349
2020-02-26 17:06:00,116 Epoch 349: total training loss 0.00259
2020-02-26 17:06:00,118 EPOCH 350
2020-02-26 17:06:01,154 Epoch 350 Step:    38750 Batch Loss:     0.000032 Tokens per Sec: 10076873, Lr: 0.001000
2020-02-26 17:06:10,091 Epoch 350: total training loss 0.00266
2020-02-26 17:06:10,091 EPOCH 351
2020-02-26 17:06:19,997 Epoch 351: total training loss 0.00253
2020-02-26 17:06:19,999 EPOCH 352
2020-02-26 17:06:23,512 Epoch 352 Step:    39000 Batch Loss:     0.000022 Tokens per Sec: 10226674, Lr: 0.001000
2020-02-26 17:06:30,091 Epoch 352: total training loss 0.00255
2020-02-26 17:06:30,092 EPOCH 353
2020-02-26 17:06:40,387 Epoch 353: total training loss 0.00255
2020-02-26 17:06:40,388 EPOCH 354
2020-02-26 17:06:46,282 Epoch 354 Step:    39250 Batch Loss:     0.000015 Tokens per Sec: 10164602, Lr: 0.001000
2020-02-26 17:06:50,633 Epoch 354: total training loss 0.00256
2020-02-26 17:06:50,634 EPOCH 355
2020-02-26 17:07:00,996 Epoch 355: total training loss 0.00251
2020-02-26 17:07:00,997 EPOCH 356
2020-02-26 17:07:09,821 Epoch 356 Step:    39500 Batch Loss:     0.000021 Tokens per Sec: 10145411, Lr: 0.001000
2020-02-26 17:07:11,222 Epoch 356: total training loss 0.00252
2020-02-26 17:07:11,222 EPOCH 357
2020-02-26 17:07:21,159 Epoch 357: total training loss 0.00252
2020-02-26 17:07:21,159 EPOCH 358
2020-02-26 17:07:31,066 Epoch 358: total training loss 0.00256
2020-02-26 17:07:31,067 EPOCH 359
2020-02-26 17:07:32,197 Epoch 359 Step:    39750 Batch Loss:     0.000019 Tokens per Sec: 10163471, Lr: 0.001000
2020-02-26 17:07:40,981 Epoch 359: total training loss 0.00246
2020-02-26 17:07:40,982 EPOCH 360
2020-02-26 17:07:50,711 Epoch 360: total training loss 0.00252
2020-02-26 17:07:50,717 EPOCH 361
2020-02-26 17:07:54,168 Epoch 361 Step:    40000 Batch Loss:     0.000014 Tokens per Sec: 10189611, Lr: 0.001000
2020-02-26 17:07:54,168 Model noise rate: 5
2020-02-26 17:08:43,825 Hooray! New best validation result [dtw]!
2020-02-26 17:08:43,842 Saving new checkpoint.
2020-02-26 17:09:01,224 Validation result at epoch 361, step    40000: Val DTW Score:  11.36, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0278, GT DTW Score:      nan, duration: 67.0556s
2020-02-26 17:09:08,020 Epoch 361: total training loss 0.00258
2020-02-26 17:09:08,021 EPOCH 362
2020-02-26 17:09:18,331 Epoch 362: total training loss 0.00250
2020-02-26 17:09:18,331 EPOCH 363
2020-02-26 17:09:24,564 Epoch 363 Step:    40250 Batch Loss:     0.000033 Tokens per Sec:  9876074, Lr: 0.001000
2020-02-26 17:09:28,637 Epoch 363: total training loss 0.00250
2020-02-26 17:09:28,638 EPOCH 364
2020-02-26 17:09:38,828 Epoch 364: total training loss 0.00242
2020-02-26 17:09:38,829 EPOCH 365
2020-02-26 17:09:47,349 Epoch 365 Step:    40500 Batch Loss:     0.000021 Tokens per Sec: 10320867, Lr: 0.001000
2020-02-26 17:09:48,766 Epoch 365: total training loss 0.00246
2020-02-26 17:09:48,766 EPOCH 366
2020-02-26 17:09:58,525 Epoch 366: total training loss 0.00254
2020-02-26 17:09:58,525 EPOCH 367
2020-02-26 17:10:08,281 Epoch 367: total training loss 0.00247
2020-02-26 17:10:08,282 EPOCH 368
2020-02-26 17:10:09,543 Epoch 368 Step:    40750 Batch Loss:     0.000021 Tokens per Sec: 10585354, Lr: 0.001000
2020-02-26 17:10:18,213 Epoch 368: total training loss 0.00237
2020-02-26 17:10:18,213 EPOCH 369
2020-02-26 17:10:28,032 Epoch 369: total training loss 0.00247
2020-02-26 17:10:28,033 EPOCH 370
2020-02-26 17:10:31,737 Epoch 370 Step:    41000 Batch Loss:     0.000029 Tokens per Sec: 10169489, Lr: 0.001000
2020-02-26 17:10:38,133 Epoch 370: total training loss 0.00237
2020-02-26 17:10:38,134 EPOCH 371
2020-02-26 17:10:48,426 Epoch 371: total training loss 0.00237
2020-02-26 17:10:48,426 EPOCH 372
2020-02-26 17:10:54,745 Epoch 372 Step:    41250 Batch Loss:     0.000015 Tokens per Sec:  9965927, Lr: 0.001000
2020-02-26 17:10:58,765 Epoch 372: total training loss 0.00242
2020-02-26 17:10:58,765 EPOCH 373
2020-02-26 17:11:08,933 Epoch 373: total training loss 0.00233
2020-02-26 17:11:08,933 EPOCH 374
2020-02-26 17:11:17,833 Epoch 374 Step:    41500 Batch Loss:     0.000023 Tokens per Sec: 10100834, Lr: 0.001000
2020-02-26 17:11:19,147 Epoch 374: total training loss 0.00237
2020-02-26 17:11:19,147 EPOCH 375
2020-02-26 17:11:29,420 Epoch 375: total training loss 0.00233
2020-02-26 17:11:29,420 EPOCH 376
2020-02-26 17:11:39,421 Epoch 376: total training loss 0.00235
2020-02-26 17:11:39,421 EPOCH 377
2020-02-26 17:11:40,617 Epoch 377 Step:    41750 Batch Loss:     0.000025 Tokens per Sec: 10306894, Lr: 0.001000
2020-02-26 17:11:49,045 Epoch 377: total training loss 0.00241
2020-02-26 17:11:49,045 EPOCH 378
2020-02-26 17:11:58,800 Epoch 378: total training loss 0.00238
2020-02-26 17:11:58,800 EPOCH 379
2020-02-26 17:12:02,528 Epoch 379 Step:    42000 Batch Loss:     0.000010 Tokens per Sec: 10184100, Lr: 0.001000
2020-02-26 17:12:08,745 Epoch 379: total training loss 0.00236
2020-02-26 17:12:08,745 EPOCH 380
2020-02-26 17:12:18,996 Epoch 380: total training loss 0.00232
2020-02-26 17:12:18,996 EPOCH 381
2020-02-26 17:12:25,265 Epoch 381 Step:    42250 Batch Loss:     0.000022 Tokens per Sec: 10084868, Lr: 0.001000
2020-02-26 17:12:29,145 Epoch 381: total training loss 0.00245
2020-02-26 17:12:29,146 EPOCH 382
2020-02-26 17:12:39,473 Epoch 382: total training loss 0.00233
2020-02-26 17:12:39,473 EPOCH 383
2020-02-26 17:12:48,436 Epoch 383 Step:    42500 Batch Loss:     0.000030 Tokens per Sec: 10054043, Lr: 0.001000
2020-02-26 17:12:49,684 Epoch 383: total training loss 0.00231
2020-02-26 17:12:49,685 EPOCH 384
2020-02-26 17:12:59,905 Epoch 384: total training loss 0.00230
2020-02-26 17:12:59,906 EPOCH 385
2020-02-26 17:13:10,085 Epoch 385: total training loss 0.00235
2020-02-26 17:13:10,086 EPOCH 386
2020-02-26 17:13:11,520 Epoch 386 Step:    42750 Batch Loss:     0.000016 Tokens per Sec: 10645928, Lr: 0.001000
2020-02-26 17:13:20,041 Epoch 386: total training loss 0.00232
2020-02-26 17:13:20,041 EPOCH 387
2020-02-26 17:13:30,024 Epoch 387: total training loss 0.00227
2020-02-26 17:13:30,024 EPOCH 388
2020-02-26 17:13:33,890 Epoch 388 Step:    43000 Batch Loss:     0.000023 Tokens per Sec: 10239589, Lr: 0.001000
2020-02-26 17:13:40,018 Epoch 388: total training loss 0.00232
2020-02-26 17:13:40,019 EPOCH 389
2020-02-26 17:13:49,971 Epoch 389: total training loss 0.00228
2020-02-26 17:13:49,972 EPOCH 390
2020-02-26 17:13:56,492 Epoch 390 Step:    43250 Batch Loss:     0.000022 Tokens per Sec: 10319788, Lr: 0.001000
2020-02-26 17:13:59,927 Epoch 390: total training loss 0.00225
2020-02-26 17:13:59,928 EPOCH 391
2020-02-26 17:14:09,837 Epoch 391: total training loss 0.00224
2020-02-26 17:14:09,838 EPOCH 392
2020-02-26 17:14:18,733 Epoch 392 Step:    43500 Batch Loss:     0.000019 Tokens per Sec: 10286442, Lr: 0.001000
2020-02-26 17:14:19,857 Epoch 392: total training loss 0.00224
2020-02-26 17:14:19,857 EPOCH 393
2020-02-26 17:14:29,777 Epoch 393: total training loss 0.00227
2020-02-26 17:14:29,777 EPOCH 394
2020-02-26 17:14:39,679 Epoch 394: total training loss 0.00230
2020-02-26 17:14:39,679 EPOCH 395
2020-02-26 17:14:41,187 Epoch 395 Step:    43750 Batch Loss:     0.000011 Tokens per Sec: 10036571, Lr: 0.001000
2020-02-26 17:14:49,772 Epoch 395: total training loss 0.00226
2020-02-26 17:14:49,773 EPOCH 396
2020-02-26 17:14:59,751 Epoch 396: total training loss 0.00216
2020-02-26 17:14:59,751 EPOCH 397
2020-02-26 17:15:03,671 Epoch 397 Step:    44000 Batch Loss:     0.000016 Tokens per Sec: 10100920, Lr: 0.001000
2020-02-26 17:15:09,704 Epoch 397: total training loss 0.00219
2020-02-26 17:15:09,705 EPOCH 398
2020-02-26 17:15:19,672 Epoch 398: total training loss 0.00219
2020-02-26 17:15:19,674 EPOCH 399
2020-02-26 17:15:26,079 Epoch 399 Step:    44250 Batch Loss:     0.000020 Tokens per Sec: 10182814, Lr: 0.001000
2020-02-26 17:15:29,775 Epoch 399: total training loss 0.00218
2020-02-26 17:15:29,775 EPOCH 400
2020-02-26 17:15:39,873 Epoch 400: total training loss 0.00220
2020-02-26 17:15:39,873 EPOCH 401
2020-02-26 17:15:49,090 Epoch 401 Step:    44500 Batch Loss:     0.000019 Tokens per Sec: 10174625, Lr: 0.001000
2020-02-26 17:15:50,085 Epoch 401: total training loss 0.00217
2020-02-26 17:15:50,085 EPOCH 402
2020-02-26 17:16:00,393 Epoch 402: total training loss 0.00218
2020-02-26 17:16:00,394 EPOCH 403
2020-02-26 17:16:10,505 Epoch 403: total training loss 0.00216
2020-02-26 17:16:10,507 EPOCH 404
2020-02-26 17:16:12,021 Epoch 404 Step:    44750 Batch Loss:     0.000028 Tokens per Sec: 10074519, Lr: 0.001000
2020-02-26 17:16:20,599 Epoch 404: total training loss 0.00221
2020-02-26 17:16:20,599 EPOCH 405
2020-02-26 17:16:30,626 Epoch 405: total training loss 0.00224
2020-02-26 17:16:30,627 EPOCH 406
2020-02-26 17:16:34,861 Epoch 406 Step:    45000 Batch Loss:     0.000020 Tokens per Sec: 10124369, Lr: 0.001000
2020-02-26 17:16:40,709 Epoch 406: total training loss 0.00219
2020-02-26 17:16:40,709 EPOCH 407
2020-02-26 17:16:50,756 Epoch 407: total training loss 0.00219
2020-02-26 17:16:50,756 EPOCH 408
2020-02-26 17:16:57,215 Epoch 408 Step:    45250 Batch Loss:     0.000020 Tokens per Sec: 10168329, Lr: 0.001000
2020-02-26 17:17:00,733 Epoch 408: total training loss 0.00213
2020-02-26 17:17:00,734 EPOCH 409
2020-02-26 17:17:10,868 Epoch 409: total training loss 0.00210
2020-02-26 17:17:10,869 EPOCH 410
2020-02-26 17:17:20,136 Epoch 410 Step:    45500 Batch Loss:     0.000020 Tokens per Sec: 10105968, Lr: 0.001000
2020-02-26 17:17:21,042 Epoch 410: total training loss 0.00213
2020-02-26 17:17:21,042 EPOCH 411
2020-02-26 17:17:31,117 Epoch 411: total training loss 0.00213
2020-02-26 17:17:31,117 EPOCH 412
2020-02-26 17:17:41,258 Epoch 412: total training loss 0.00214
2020-02-26 17:17:41,258 EPOCH 413
2020-02-26 17:17:43,011 Epoch 413 Step:    45750 Batch Loss:     0.000028 Tokens per Sec: 10258071, Lr: 0.001000
2020-02-26 17:17:51,321 Epoch 413: total training loss 0.00218
2020-02-26 17:17:51,322 EPOCH 414
2020-02-26 17:18:01,380 Epoch 414: total training loss 0.00213
2020-02-26 17:18:01,381 EPOCH 415
2020-02-26 17:18:05,574 Epoch 415 Step:    46000 Batch Loss:     0.000011 Tokens per Sec: 10094144, Lr: 0.001000
2020-02-26 17:18:11,401 Epoch 415: total training loss 0.00210
2020-02-26 17:18:11,402 EPOCH 416
2020-02-26 17:18:21,446 Epoch 416: total training loss 0.00214
2020-02-26 17:18:21,447 EPOCH 417
2020-02-26 17:18:28,317 Epoch 417 Step:    46250 Batch Loss:     0.000022 Tokens per Sec: 10213737, Lr: 0.001000
2020-02-26 17:18:31,470 Epoch 417: total training loss 0.00211
2020-02-26 17:18:31,470 EPOCH 418
2020-02-26 17:18:41,480 Epoch 418: total training loss 0.00213
2020-02-26 17:18:41,481 EPOCH 419
2020-02-26 17:18:50,527 Epoch 419 Step:    46500 Batch Loss:     0.000012 Tokens per Sec: 10399157, Lr: 0.001000
2020-02-26 17:18:51,349 Epoch 419: total training loss 0.00207
2020-02-26 17:18:51,350 EPOCH 420
2020-02-26 17:19:01,434 Epoch 420: total training loss 0.00207
2020-02-26 17:19:01,435 EPOCH 421
2020-02-26 17:19:11,582 Epoch 421: total training loss 0.00202
2020-02-26 17:19:11,582 EPOCH 422
2020-02-26 17:19:13,321 Epoch 422 Step:    46750 Batch Loss:     0.000020 Tokens per Sec: 10095922, Lr: 0.001000
2020-02-26 17:19:21,862 Epoch 422: total training loss 0.00204
2020-02-26 17:19:21,863 EPOCH 423
2020-02-26 17:19:32,127 Epoch 423: total training loss 0.00206
2020-02-26 17:19:32,127 EPOCH 424
2020-02-26 17:19:36,397 Epoch 424 Step:    47000 Batch Loss:     0.000021 Tokens per Sec:  9989969, Lr: 0.001000
2020-02-26 17:19:42,305 Epoch 424: total training loss 0.00214
2020-02-26 17:19:42,306 EPOCH 425
2020-02-26 17:19:52,344 Epoch 425: total training loss 0.00202
2020-02-26 17:19:52,344 EPOCH 426
2020-02-26 17:19:59,067 Epoch 426 Step:    47250 Batch Loss:     0.000012 Tokens per Sec: 10187177, Lr: 0.001000
2020-02-26 17:20:02,349 Epoch 426: total training loss 0.00207
2020-02-26 17:20:02,349 EPOCH 427
2020-02-26 17:20:12,408 Epoch 427: total training loss 0.00208
2020-02-26 17:20:12,409 EPOCH 428
2020-02-26 17:20:21,880 Epoch 428 Step:    47500 Batch Loss:     0.000012 Tokens per Sec: 10145175, Lr: 0.001000
2020-02-26 17:20:22,573 Epoch 428: total training loss 0.00200
2020-02-26 17:20:22,574 EPOCH 429
2020-02-26 17:20:32,691 Epoch 429: total training loss 0.00195
2020-02-26 17:20:32,691 EPOCH 430
2020-02-26 17:20:42,879 Epoch 430: total training loss 0.00199
2020-02-26 17:20:42,880 EPOCH 431
2020-02-26 17:20:44,803 Epoch 431 Step:    47750 Batch Loss:     0.000013 Tokens per Sec: 10048659, Lr: 0.001000
2020-02-26 17:20:53,126 Epoch 431: total training loss 0.00200
2020-02-26 17:20:53,128 EPOCH 432
2020-02-26 17:21:03,469 Epoch 432: total training loss 0.00207
2020-02-26 17:21:03,470 EPOCH 433
2020-02-26 17:21:07,939 Epoch 433 Step:    48000 Batch Loss:     0.000015 Tokens per Sec: 10054504, Lr: 0.001000
2020-02-26 17:21:13,776 Epoch 433: total training loss 0.00200
2020-02-26 17:21:13,777 EPOCH 434
2020-02-26 17:21:24,154 Epoch 434: total training loss 0.00200
2020-02-26 17:21:24,155 EPOCH 435
2020-02-26 17:21:31,185 Epoch 435 Step:    48250 Batch Loss:     0.000011 Tokens per Sec:  9933567, Lr: 0.001000
2020-02-26 17:21:34,398 Epoch 435: total training loss 0.00194
2020-02-26 17:21:34,398 EPOCH 436
2020-02-26 17:21:44,348 Epoch 436: total training loss 0.00198
2020-02-26 17:21:44,348 EPOCH 437
2020-02-26 17:21:53,528 Epoch 437 Step:    48500 Batch Loss:     0.000011 Tokens per Sec: 10379805, Lr: 0.001000
2020-02-26 17:21:54,218 Epoch 437: total training loss 0.00195
2020-02-26 17:21:54,219 EPOCH 438
2020-02-26 17:22:04,206 Epoch 438: total training loss 0.00190
2020-02-26 17:22:04,206 EPOCH 439
2020-02-26 17:22:14,157 Epoch 439: total training loss 0.00193
2020-02-26 17:22:14,158 EPOCH 440
2020-02-26 17:22:16,118 Epoch 440 Step:    48750 Batch Loss:     0.000017 Tokens per Sec: 10517222, Lr: 0.001000
2020-02-26 17:22:24,204 Epoch 440: total training loss 0.00196
2020-02-26 17:22:24,205 EPOCH 441
2020-02-26 17:22:34,508 Epoch 441: total training loss 0.00192
2020-02-26 17:22:34,509 EPOCH 442
2020-02-26 17:22:39,034 Epoch 442 Step:    49000 Batch Loss:     0.000014 Tokens per Sec:  9903525, Lr: 0.001000
2020-02-26 17:22:44,845 Epoch 442: total training loss 0.00199
2020-02-26 17:22:44,845 EPOCH 443
2020-02-26 17:22:55,100 Epoch 443: total training loss 0.00193
2020-02-26 17:22:55,101 EPOCH 444
2020-02-26 17:23:02,196 Epoch 444 Step:    49250 Batch Loss:     0.000016 Tokens per Sec: 10228412, Lr: 0.001000
2020-02-26 17:23:05,174 Epoch 444: total training loss 0.00191
2020-02-26 17:23:05,174 EPOCH 445
2020-02-26 17:23:15,188 Epoch 445: total training loss 0.00190
2020-02-26 17:23:15,189 EPOCH 446
2020-02-26 17:23:24,628 Epoch 446 Step:    49500 Batch Loss:     0.000022 Tokens per Sec: 10185361, Lr: 0.001000
2020-02-26 17:23:25,231 Epoch 446: total training loss 0.00194
2020-02-26 17:23:25,231 EPOCH 447
2020-02-26 17:23:35,295 Epoch 447: total training loss 0.00192
2020-02-26 17:23:35,296 EPOCH 448
2020-02-26 17:23:45,380 Epoch 448: total training loss 0.00195
2020-02-26 17:23:45,380 EPOCH 449
2020-02-26 17:23:47,615 Epoch 449 Step:    49750 Batch Loss:     0.000024 Tokens per Sec: 10513910, Lr: 0.001000
2020-02-26 17:23:55,442 Epoch 449: total training loss 0.00195
2020-02-26 17:23:55,443 EPOCH 450
2020-02-26 17:24:05,553 Epoch 450: total training loss 0.00193
2020-02-26 17:24:05,555 EPOCH 451
2020-02-26 17:24:10,272 Epoch 451 Step:    50000 Batch Loss:     0.000013 Tokens per Sec:  9977743, Lr: 0.001000
2020-02-26 17:24:10,272 Model noise rate: 5
2020-02-26 17:24:58,939 Hooray! New best validation result [dtw]!
2020-02-26 17:24:58,940 Saving new checkpoint.
2020-02-26 17:25:14,690 Validation result at epoch 451, step    50000: Val DTW Score:  10.90, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0325, GT DTW Score:      nan, duration: 64.4173s
2020-02-26 17:25:20,390 Epoch 451: total training loss 0.00188
2020-02-26 17:25:20,390 EPOCH 452
2020-02-26 17:25:30,296 Epoch 452: total training loss 0.00190
2020-02-26 17:25:30,297 EPOCH 453
2020-02-26 17:25:37,057 Epoch 453 Step:    50250 Batch Loss:     0.000017 Tokens per Sec: 10503041, Lr: 0.001000
2020-02-26 17:25:40,020 Epoch 453: total training loss 0.00193
2020-02-26 17:25:40,021 EPOCH 454
2020-02-26 17:25:49,950 Epoch 454: total training loss 0.00192
2020-02-26 17:25:49,952 EPOCH 455
2020-02-26 17:25:59,629 Epoch 455 Step:    50500 Batch Loss:     0.000017 Tokens per Sec: 10119866, Lr: 0.001000
2020-02-26 17:26:00,166 Epoch 455: total training loss 0.00188
2020-02-26 17:26:00,167 EPOCH 456
2020-02-26 17:26:10,495 Epoch 456: total training loss 0.00187
2020-02-26 17:26:10,518 EPOCH 457
2020-02-26 17:26:21,223 Epoch 457: total training loss 0.00188
2020-02-26 17:26:21,223 EPOCH 458
2020-02-26 17:26:23,526 Epoch 458 Step:    50750 Batch Loss:     0.000016 Tokens per Sec:  9514185, Lr: 0.001000
2020-02-26 17:26:31,756 Epoch 458: total training loss 0.00187
2020-02-26 17:26:31,762 EPOCH 459
2020-02-26 17:26:41,978 Epoch 459: total training loss 0.00187
2020-02-26 17:26:41,979 EPOCH 460
2020-02-26 17:26:46,695 Epoch 460 Step:    51000 Batch Loss:     0.000019 Tokens per Sec: 10060700, Lr: 0.001000
2020-02-26 17:26:52,175 Epoch 460: total training loss 0.00187
2020-02-26 17:26:52,175 EPOCH 461
2020-02-26 17:27:02,209 Epoch 461: total training loss 0.00185
2020-02-26 17:27:02,210 EPOCH 462
2020-02-26 17:27:09,363 Epoch 462 Step:    51250 Batch Loss:     0.000016 Tokens per Sec: 10170853, Lr: 0.001000
2020-02-26 17:27:12,185 Epoch 462: total training loss 0.00196
2020-02-26 17:27:12,185 EPOCH 463
2020-02-26 17:27:22,050 Epoch 463: total training loss 0.00187
2020-02-26 17:27:22,051 EPOCH 464
2020-02-26 17:27:31,730 Epoch 464 Step:    51500 Batch Loss:     0.000012 Tokens per Sec: 10261783, Lr: 0.001000
2020-02-26 17:27:32,082 Epoch 464: total training loss 0.00184
2020-02-26 17:27:32,082 EPOCH 465
2020-02-26 17:27:42,356 Epoch 465: total training loss 0.00183
2020-02-26 17:27:42,357 EPOCH 466
2020-02-26 17:27:52,706 Epoch 466: total training loss 0.00183
2020-02-26 17:27:52,707 EPOCH 467
2020-02-26 17:27:54,808 Epoch 467 Step:    51750 Batch Loss:     0.000013 Tokens per Sec:  9950456, Lr: 0.001000
2020-02-26 17:28:03,010 Epoch 467: total training loss 0.00185
2020-02-26 17:28:03,011 EPOCH 468
2020-02-26 17:28:13,360 Epoch 468: total training loss 0.00181
2020-02-26 17:28:13,361 EPOCH 469
2020-02-26 17:28:18,109 Epoch 469 Step:    52000 Batch Loss:     0.000017 Tokens per Sec: 10003718, Lr: 0.001000
2020-02-26 17:28:23,525 Epoch 469: total training loss 0.00181
2020-02-26 17:28:23,525 EPOCH 470
2020-02-26 17:28:33,515 Epoch 470: total training loss 0.00184
2020-02-26 17:28:33,516 EPOCH 471
2020-02-26 17:28:40,975 Epoch 471 Step:    52250 Batch Loss:     0.000018 Tokens per Sec: 10232358, Lr: 0.001000
2020-02-26 17:28:43,567 Epoch 471: total training loss 0.00179
2020-02-26 17:28:43,568 EPOCH 472
2020-02-26 17:28:53,513 Epoch 472: total training loss 0.00191
2020-02-26 17:28:53,529 EPOCH 473
2020-02-26 17:29:03,213 Epoch 473 Step:    52500 Batch Loss:     0.000014 Tokens per Sec: 10330015, Lr: 0.001000
2020-02-26 17:29:03,473 Epoch 473: total training loss 0.00182
2020-02-26 17:29:03,473 EPOCH 474
2020-02-26 17:29:13,660 Epoch 474: total training loss 0.00178
2020-02-26 17:29:13,661 EPOCH 475
2020-02-26 17:29:24,275 Epoch 475: total training loss 0.00177
2020-02-26 17:29:24,275 EPOCH 476
2020-02-26 17:29:26,721 Epoch 476 Step:    52750 Batch Loss:     0.000018 Tokens per Sec:  9736643, Lr: 0.001000
2020-02-26 17:29:34,833 Epoch 476: total training loss 0.00184
2020-02-26 17:29:34,834 EPOCH 477
2020-02-26 17:29:45,277 Epoch 477: total training loss 0.00191
2020-02-26 17:29:45,278 EPOCH 478
2020-02-26 17:29:50,285 Epoch 478 Step:    53000 Batch Loss:     0.000011 Tokens per Sec: 10031149, Lr: 0.001000
2020-02-26 17:29:55,490 Epoch 478: total training loss 0.00189
2020-02-26 17:29:55,490 EPOCH 479
2020-02-26 17:30:05,691 Epoch 479: total training loss 0.00175
2020-02-26 17:30:05,692 EPOCH 480
2020-02-26 17:30:12,996 Epoch 480 Step:    53250 Batch Loss:     0.000013 Tokens per Sec: 10139296, Lr: 0.001000
2020-02-26 17:30:15,831 Epoch 480: total training loss 0.00172
2020-02-26 17:30:15,831 EPOCH 481
2020-02-26 17:30:25,911 Epoch 481: total training loss 0.00177
2020-02-26 17:30:25,911 EPOCH 482
2020-02-26 17:30:35,842 Epoch 482 Step:    53500 Batch Loss:     0.000019 Tokens per Sec: 10154439, Lr: 0.001000
2020-02-26 17:30:36,037 Epoch 482: total training loss 0.00174
2020-02-26 17:30:36,037 EPOCH 483
2020-02-26 17:30:46,070 Epoch 483: total training loss 0.00174
2020-02-26 17:30:46,071 EPOCH 484
2020-02-26 17:30:56,091 Epoch 484: total training loss 0.00182
2020-02-26 17:30:56,092 EPOCH 485
2020-02-26 17:30:58,446 Epoch 485 Step:    53750 Batch Loss:     0.000013 Tokens per Sec: 10220700, Lr: 0.001000
2020-02-26 17:31:06,366 Epoch 485: total training loss 0.00185
2020-02-26 17:31:06,367 EPOCH 486
2020-02-26 17:31:16,752 Epoch 486: total training loss 0.00174
2020-02-26 17:31:16,753 EPOCH 487
2020-02-26 17:31:21,776 Epoch 487 Step:    54000 Batch Loss:     0.000021 Tokens per Sec:  9829991, Lr: 0.001000
2020-02-26 17:31:27,297 Epoch 487: total training loss 0.00171
2020-02-26 17:31:27,298 EPOCH 488
2020-02-26 17:31:37,869 Epoch 488: total training loss 0.00171
2020-02-26 17:31:37,869 EPOCH 489
2020-02-26 17:31:45,702 Epoch 489 Step:    54250 Batch Loss:     0.000013 Tokens per Sec:  9607802, Lr: 0.001000
2020-02-26 17:31:48,417 Epoch 489: total training loss 0.00172
2020-02-26 17:31:48,417 EPOCH 490
2020-02-26 17:31:58,581 Epoch 490: total training loss 0.00179
2020-02-26 17:31:58,581 EPOCH 491
2020-02-26 17:32:08,560 Epoch 491 Step:    54500 Batch Loss:     0.000013 Tokens per Sec: 10241398, Lr: 0.001000
2020-02-26 17:32:08,641 Epoch 491: total training loss 0.00174
2020-02-26 17:32:08,641 EPOCH 492
2020-02-26 17:32:18,707 Epoch 492: total training loss 0.00172
2020-02-26 17:32:18,708 EPOCH 493
2020-02-26 17:32:28,720 Epoch 493: total training loss 0.00174
2020-02-26 17:32:28,720 EPOCH 494
2020-02-26 17:32:31,210 Epoch 494 Step:    54750 Batch Loss:     0.000014 Tokens per Sec:  9947237, Lr: 0.001000
2020-02-26 17:32:38,799 Epoch 494: total training loss 0.00169
2020-02-26 17:32:38,799 EPOCH 495
2020-02-26 17:32:48,819 Epoch 495: total training loss 0.00169
2020-02-26 17:32:48,820 EPOCH 496
2020-02-26 17:32:53,748 Epoch 496 Step:    55000 Batch Loss:     0.000016 Tokens per Sec: 10164675, Lr: 0.001000
2020-02-26 17:32:58,967 Epoch 496: total training loss 0.00170
2020-02-26 17:32:58,968 EPOCH 497
2020-02-26 17:33:09,207 Epoch 497: total training loss 0.00168
2020-02-26 17:33:09,208 EPOCH 498
2020-02-26 17:33:16,850 Epoch 498 Step:    55250 Batch Loss:     0.000011 Tokens per Sec: 10024579, Lr: 0.001000
2020-02-26 17:33:19,418 Epoch 498: total training loss 0.00171
2020-02-26 17:33:19,418 EPOCH 499
2020-02-26 17:33:29,501 Epoch 499: total training loss 0.00175
2020-02-26 17:33:29,501 EPOCH 500
2020-02-26 17:33:39,609 Epoch 500 Step:    55500 Batch Loss:     0.000021 Tokens per Sec: 10193646, Lr: 0.001000
2020-02-26 17:33:39,610 Epoch 500: total training loss 0.00170
2020-02-26 17:33:39,610 EPOCH 501
2020-02-26 17:33:49,746 Epoch 501: total training loss 0.00167
2020-02-26 17:33:49,746 EPOCH 502
2020-02-26 17:33:59,963 Epoch 502: total training loss 0.00169
2020-02-26 17:33:59,964 EPOCH 503
2020-02-26 17:34:02,549 Epoch 503 Step:    55750 Batch Loss:     0.000011 Tokens per Sec:  9795731, Lr: 0.001000
2020-02-26 17:34:10,191 Epoch 503: total training loss 0.00174
2020-02-26 17:34:10,191 EPOCH 504
2020-02-26 17:34:20,202 Epoch 504: total training loss 0.00171
2020-02-26 17:34:20,202 EPOCH 505
2020-02-26 17:34:25,236 Epoch 505 Step:    56000 Batch Loss:     0.000009 Tokens per Sec: 10288150, Lr: 0.001000
2020-02-26 17:34:30,255 Epoch 505: total training loss 0.00168
2020-02-26 17:34:30,256 EPOCH 506
2020-02-26 17:34:40,321 Epoch 506: total training loss 0.00167
2020-02-26 17:34:40,322 EPOCH 507
2020-02-26 17:34:47,995 Epoch 507 Step:    56250 Batch Loss:     0.000012 Tokens per Sec: 10186337, Lr: 0.001000
2020-02-26 17:34:50,431 Epoch 507: total training loss 0.00166
2020-02-26 17:34:50,431 EPOCH 508
2020-02-26 17:35:00,375 Epoch 508: total training loss 0.00178
2020-02-26 17:35:00,375 EPOCH 509
2020-02-26 17:35:10,410 Epoch 509: total training loss 0.00176
2020-02-26 17:35:10,411 EPOCH 510
2020-02-26 17:35:10,506 Epoch 510 Step:    56500 Batch Loss:     0.000014 Tokens per Sec:  7535934, Lr: 0.001000
2020-02-26 17:35:20,693 Epoch 510: total training loss 0.00162
2020-02-26 17:35:20,693 EPOCH 511
2020-02-26 17:35:30,801 Epoch 511: total training loss 0.00168
2020-02-26 17:35:30,802 EPOCH 512
2020-02-26 17:35:33,412 Epoch 512 Step:    56750 Batch Loss:     0.000015 Tokens per Sec: 10032256, Lr: 0.001000
2020-02-26 17:35:40,987 Epoch 512: total training loss 0.00163
2020-02-26 17:35:40,988 EPOCH 513
2020-02-26 17:35:51,103 Epoch 513: total training loss 0.00165
2020-02-26 17:35:51,103 EPOCH 514
2020-02-26 17:35:56,201 Epoch 514 Step:    57000 Batch Loss:     0.000015 Tokens per Sec: 10356673, Lr: 0.001000
2020-02-26 17:36:01,064 Epoch 514: total training loss 0.00163
2020-02-26 17:36:01,065 EPOCH 515
2020-02-26 17:36:10,974 Epoch 515: total training loss 0.00167
2020-02-26 17:36:10,975 EPOCH 516
2020-02-26 17:36:18,582 Epoch 516 Step:    57250 Batch Loss:     0.000013 Tokens per Sec: 10296483, Lr: 0.001000
2020-02-26 17:36:20,981 Epoch 516: total training loss 0.00163
2020-02-26 17:36:20,981 EPOCH 517
2020-02-26 17:36:30,809 Epoch 517: total training loss 0.00164
2020-02-26 17:36:30,815 EPOCH 518
2020-02-26 17:36:40,857 Epoch 518: total training loss 0.00161
2020-02-26 17:36:40,858 EPOCH 519
2020-02-26 17:36:41,039 Epoch 519 Step:    57500 Batch Loss:     0.000013 Tokens per Sec:  8769847, Lr: 0.001000
2020-02-26 17:36:50,842 Epoch 519: total training loss 0.00171
2020-02-26 17:36:50,844 EPOCH 520
2020-02-26 17:37:01,211 Epoch 520: total training loss 0.00164
2020-02-26 17:37:01,212 EPOCH 521
2020-02-26 17:37:03,878 Epoch 521 Step:    57750 Batch Loss:     0.000017 Tokens per Sec:  9484719, Lr: 0.001000
2020-02-26 17:37:11,626 Epoch 521: total training loss 0.00163
2020-02-26 17:37:11,627 EPOCH 522
2020-02-26 17:37:22,182 Epoch 522: total training loss 0.00162
2020-02-26 17:37:22,183 EPOCH 523
2020-02-26 17:37:27,685 Epoch 523 Step:    58000 Batch Loss:     0.000015 Tokens per Sec:  9884189, Lr: 0.001000
2020-02-26 17:37:32,543 Epoch 523: total training loss 0.00166
2020-02-26 17:37:32,543 EPOCH 524
2020-02-26 17:37:42,972 Epoch 524: total training loss 0.00160
2020-02-26 17:37:42,972 EPOCH 525
2020-02-26 17:37:50,632 Epoch 525 Step:    58250 Batch Loss:     0.000011 Tokens per Sec: 10138014, Lr: 0.001000
2020-02-26 17:37:53,110 Epoch 525: total training loss 0.00160
2020-02-26 17:37:53,110 EPOCH 526
2020-02-26 17:38:03,175 Epoch 526: total training loss 0.00160
2020-02-26 17:38:03,177 EPOCH 527
2020-02-26 17:38:13,475 Epoch 527: total training loss 0.00162
2020-02-26 17:38:13,477 EPOCH 528
2020-02-26 17:38:13,785 Epoch 528 Step:    58500 Batch Loss:     0.000018 Tokens per Sec:  9462703, Lr: 0.001000
2020-02-26 17:38:23,725 Epoch 528: total training loss 0.00163
2020-02-26 17:38:23,726 EPOCH 529
2020-02-26 17:38:33,630 Epoch 529: total training loss 0.00159
2020-02-26 17:38:33,630 EPOCH 530
2020-02-26 17:38:36,458 Epoch 530 Step:    58750 Batch Loss:     0.000021 Tokens per Sec: 10237641, Lr: 0.001000
2020-02-26 17:38:43,563 Epoch 530: total training loss 0.00162
2020-02-26 17:38:43,564 EPOCH 531
2020-02-26 17:38:54,038 Epoch 531: total training loss 0.00160
2020-02-26 17:38:54,040 EPOCH 532
2020-02-26 17:38:59,742 Epoch 532 Step:    59000 Batch Loss:     0.000012 Tokens per Sec:  9757229, Lr: 0.001000
2020-02-26 17:39:04,560 Epoch 532: total training loss 0.00160
2020-02-26 17:39:04,560 EPOCH 533
2020-02-26 17:39:15,074 Epoch 533: total training loss 0.00156
2020-02-26 17:39:15,075 EPOCH 534
2020-02-26 17:39:23,536 Epoch 534 Step:    59250 Batch Loss:     0.000017 Tokens per Sec:  9675585, Lr: 0.001000
2020-02-26 17:39:25,745 Epoch 534: total training loss 0.00154
2020-02-26 17:39:25,746 EPOCH 535
2020-02-26 17:39:36,193 Epoch 535: total training loss 0.00162
2020-02-26 17:39:36,193 EPOCH 536
2020-02-26 17:39:46,287 Epoch 536: total training loss 0.00159
2020-02-26 17:39:46,288 EPOCH 537
2020-02-26 17:39:46,683 Epoch 537 Step:    59500 Batch Loss:     0.000013 Tokens per Sec:  9554015, Lr: 0.001000
2020-02-26 17:39:56,405 Epoch 537: total training loss 0.00156
2020-02-26 17:39:56,405 EPOCH 538
2020-02-26 17:40:06,426 Epoch 538: total training loss 0.00164
2020-02-26 17:40:06,427 EPOCH 539
2020-02-26 17:40:09,477 Epoch 539 Step:    59750 Batch Loss:     0.000021 Tokens per Sec: 10027318, Lr: 0.001000
2020-02-26 17:40:16,451 Epoch 539: total training loss 0.00175
2020-02-26 17:40:16,451 EPOCH 540
2020-02-26 17:40:26,431 Epoch 540: total training loss 0.00160
2020-02-26 17:40:26,432 EPOCH 541
2020-02-26 17:40:31,858 Epoch 541 Step:    60000 Batch Loss:     0.000011 Tokens per Sec: 10325994, Lr: 0.001000
2020-02-26 17:40:31,859 Model noise rate: 5
2020-02-26 17:41:20,160 Hooray! New best validation result [dtw]!
2020-02-26 17:41:20,161 Saving new checkpoint.
2020-02-26 17:41:36,333 Validation result at epoch 541, step    60000: Val DTW Score:  10.76, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0332, GT DTW Score:      nan, duration: 64.4738s
2020-02-26 17:41:40,967 Epoch 541: total training loss 0.00153
2020-02-26 17:41:40,967 EPOCH 542
2020-02-26 17:41:51,128 Epoch 542: total training loss 0.00154
2020-02-26 17:41:51,129 EPOCH 543
2020-02-26 17:41:59,038 Epoch 543 Step:    60250 Batch Loss:     0.000014 Tokens per Sec: 10140172, Lr: 0.001000
2020-02-26 17:42:01,237 Epoch 543: total training loss 0.00161
2020-02-26 17:42:01,238 EPOCH 544
2020-02-26 17:42:11,101 Epoch 544: total training loss 0.00158
2020-02-26 17:42:11,101 EPOCH 545
2020-02-26 17:42:21,122 Epoch 545: total training loss 0.00150
2020-02-26 17:42:21,122 EPOCH 546
2020-02-26 17:42:21,607 Epoch 546 Step:    60500 Batch Loss:     0.000013 Tokens per Sec:  9710966, Lr: 0.001000
2020-02-26 17:42:31,220 Epoch 546: total training loss 0.00157
2020-02-26 17:42:31,221 EPOCH 547
2020-02-26 17:42:41,475 Epoch 547: total training loss 0.00152
2020-02-26 17:42:41,476 EPOCH 548
2020-02-26 17:42:44,725 Epoch 548 Step:    60750 Batch Loss:     0.000012 Tokens per Sec: 10130125, Lr: 0.001000
2020-02-26 17:42:51,683 Epoch 548: total training loss 0.00152
2020-02-26 17:42:51,684 EPOCH 549
2020-02-26 17:43:01,961 Epoch 549: total training loss 0.00153
2020-02-26 17:43:01,962 EPOCH 550
2020-02-26 17:43:07,508 Epoch 550 Step:    61000 Batch Loss:     0.000017 Tokens per Sec:  9822761, Lr: 0.001000
2020-02-26 17:43:12,304 Epoch 550: total training loss 0.00164
2020-02-26 17:43:12,304 EPOCH 551
2020-02-26 17:43:22,379 Epoch 551: total training loss 0.00154
2020-02-26 17:43:22,379 EPOCH 552
2020-02-26 17:43:30,250 Epoch 552 Step:    61250 Batch Loss:     0.000015 Tokens per Sec: 10189721, Lr: 0.001000
2020-02-26 17:43:32,314 Epoch 552: total training loss 0.00154
2020-02-26 17:43:32,314 EPOCH 553
2020-02-26 17:43:42,224 Epoch 553: total training loss 0.00158
2020-02-26 17:43:42,225 EPOCH 554
2020-02-26 17:43:52,239 Epoch 554: total training loss 0.00161
2020-02-26 17:43:52,240 EPOCH 555
2020-02-26 17:43:52,885 Epoch 555 Step:    61500 Batch Loss:     0.000009 Tokens per Sec:  9850415, Lr: 0.001000
2020-02-26 17:44:02,300 Epoch 555: total training loss 0.00156
2020-02-26 17:44:02,300 EPOCH 556
2020-02-26 17:44:12,498 Epoch 556: total training loss 0.00152
2020-02-26 17:44:12,500 EPOCH 557
2020-02-26 17:44:15,667 Epoch 557 Step:    61750 Batch Loss:     0.000014 Tokens per Sec:  9644679, Lr: 0.001000
2020-02-26 17:44:22,909 Epoch 557: total training loss 0.00169
2020-02-26 17:44:22,910 EPOCH 558
2020-02-26 17:44:33,249 Epoch 558: total training loss 0.00153
2020-02-26 17:44:33,250 EPOCH 559
2020-02-26 17:44:39,124 Epoch 559 Step:    62000 Batch Loss:     0.000012 Tokens per Sec:  9834766, Lr: 0.001000
2020-02-26 17:44:43,773 Epoch 559: total training loss 0.00149
2020-02-26 17:44:43,774 EPOCH 560
2020-02-26 17:44:54,198 Epoch 560: total training loss 0.00148
2020-02-26 17:44:54,199 EPOCH 561
2020-02-26 17:45:02,453 Epoch 561 Step:    62250 Batch Loss:     0.000014 Tokens per Sec: 10165467, Lr: 0.001000
2020-02-26 17:45:04,422 Epoch 561: total training loss 0.00147
2020-02-26 17:45:04,423 EPOCH 562
2020-02-26 17:45:14,554 Epoch 562: total training loss 0.00153
2020-02-26 17:45:14,555 EPOCH 563
2020-02-26 17:45:24,791 Epoch 563: total training loss 0.00154
2020-02-26 17:45:24,792 EPOCH 564
2020-02-26 17:45:25,419 Epoch 564 Step:    62500 Batch Loss:     0.000012 Tokens per Sec:  9669150, Lr: 0.001000
2020-02-26 17:45:34,940 Epoch 564: total training loss 0.00156
2020-02-26 17:45:34,941 EPOCH 565
2020-02-26 17:45:45,111 Epoch 565: total training loss 0.00150
2020-02-26 17:45:45,112 EPOCH 566
2020-02-26 17:45:48,204 Epoch 566 Step:    62750 Batch Loss:     0.000010 Tokens per Sec: 10254410, Lr: 0.001000
2020-02-26 17:45:55,227 Epoch 566: total training loss 0.00150
2020-02-26 17:45:55,228 EPOCH 567
2020-02-26 17:46:05,427 Epoch 567: total training loss 0.00148
2020-02-26 17:46:05,427 EPOCH 568
2020-02-26 17:46:11,375 Epoch 568 Step:    63000 Batch Loss:     0.000013 Tokens per Sec:  9891824, Lr: 0.001000
2020-02-26 17:46:15,702 Epoch 568: total training loss 0.00150
2020-02-26 17:46:15,703 EPOCH 569
2020-02-26 17:46:25,940 Epoch 569: total training loss 0.00148
2020-02-26 17:46:25,941 EPOCH 570
2020-02-26 17:46:34,245 Epoch 570 Step:    63250 Batch Loss:     0.000019 Tokens per Sec: 10158902, Lr: 0.001000
2020-02-26 17:46:36,055 Epoch 570: total training loss 0.00152
2020-02-26 17:46:36,055 EPOCH 571
2020-02-26 17:46:46,047 Epoch 571: total training loss 0.00147
2020-02-26 17:46:46,047 EPOCH 572
2020-02-26 17:46:56,026 Epoch 572: total training loss 0.00167
2020-02-26 17:46:56,028 EPOCH 573
2020-02-26 17:46:56,793 Epoch 573 Step:    63500 Batch Loss:     0.000011 Tokens per Sec: 10223654, Lr: 0.001000
2020-02-26 17:47:06,116 Epoch 573: total training loss 0.00153
2020-02-26 17:47:06,118 EPOCH 574
2020-02-26 17:47:16,223 Epoch 574: total training loss 0.00150
2020-02-26 17:47:16,225 EPOCH 575
2020-02-26 17:47:19,593 Epoch 575 Step:    63750 Batch Loss:     0.000019 Tokens per Sec: 10225343, Lr: 0.001000
2020-02-26 17:47:26,312 Epoch 575: total training loss 0.00148
2020-02-26 17:47:26,312 EPOCH 576
2020-02-26 17:47:36,375 Epoch 576: total training loss 0.00146
2020-02-26 17:47:36,376 EPOCH 577
2020-02-26 17:47:41,982 Epoch 577 Step:    64000 Batch Loss:     0.000015 Tokens per Sec: 10165460, Lr: 0.001000
2020-02-26 17:47:46,511 Epoch 577: total training loss 0.00148
2020-02-26 17:47:46,511 EPOCH 578
2020-02-26 17:47:57,049 Epoch 578: total training loss 0.00150
2020-02-26 17:47:57,049 EPOCH 579
2020-02-26 17:48:05,590 Epoch 579 Step:    64250 Batch Loss:     0.000010 Tokens per Sec: 10011031, Lr: 0.001000
2020-02-26 17:48:07,267 Epoch 579: total training loss 0.00147
2020-02-26 17:48:07,267 EPOCH 580
2020-02-26 17:48:17,587 Epoch 580: total training loss 0.00148
2020-02-26 17:48:17,588 EPOCH 581
2020-02-26 17:48:27,946 Epoch 581: total training loss 0.00149
2020-02-26 17:48:27,947 EPOCH 582
2020-02-26 17:48:28,732 Epoch 582 Step:    64500 Batch Loss:     0.000011 Tokens per Sec:  9607636, Lr: 0.001000
2020-02-26 17:48:38,173 Epoch 582: total training loss 0.00149
2020-02-26 17:48:38,173 EPOCH 583
2020-02-26 17:48:48,370 Epoch 583: total training loss 0.00146
2020-02-26 17:48:48,371 EPOCH 584
2020-02-26 17:48:51,749 Epoch 584 Step:    64750 Batch Loss:     0.000021 Tokens per Sec: 10151050, Lr: 0.001000
2020-02-26 17:48:58,547 Epoch 584: total training loss 0.00145
2020-02-26 17:48:58,547 EPOCH 585
2020-02-26 17:49:08,667 Epoch 585: total training loss 0.00145
2020-02-26 17:49:08,667 EPOCH 586
2020-02-26 17:49:14,435 Epoch 586 Step:    65000 Batch Loss:     0.000020 Tokens per Sec: 10283042, Lr: 0.001000
2020-02-26 17:49:18,701 Epoch 586: total training loss 0.00145
2020-02-26 17:49:18,702 EPOCH 587
2020-02-26 17:49:28,710 Epoch 587: total training loss 0.00144
2020-02-26 17:49:28,712 EPOCH 588
2020-02-26 17:49:37,296 Epoch 588 Step:    65250 Batch Loss:     0.000013 Tokens per Sec:  9895772, Lr: 0.001000
2020-02-26 17:49:39,105 Epoch 588: total training loss 0.00143
2020-02-26 17:49:39,106 EPOCH 589
2020-02-26 17:49:49,638 Epoch 589: total training loss 0.00151
2020-02-26 17:49:49,639 EPOCH 590
2020-02-26 17:50:00,127 Epoch 590: total training loss 0.00149
2020-02-26 17:50:00,128 EPOCH 591
2020-02-26 17:50:01,186 Epoch 591 Step:    65500 Batch Loss:     0.000024 Tokens per Sec: 10001445, Lr: 0.001000
2020-02-26 17:50:10,569 Epoch 591: total training loss 0.00153
2020-02-26 17:50:10,570 EPOCH 592
2020-02-26 17:50:21,173 Epoch 592: total training loss 0.00145
2020-02-26 17:50:21,173 EPOCH 593
2020-02-26 17:50:24,697 Epoch 593 Step:    65750 Batch Loss:     0.000013 Tokens per Sec: 10414633, Lr: 0.001000
2020-02-26 17:50:31,287 Epoch 593: total training loss 0.00143
2020-02-26 17:50:31,287 EPOCH 594
2020-02-26 17:50:41,497 Epoch 594: total training loss 0.00160
2020-02-26 17:50:41,497 EPOCH 595
2020-02-26 17:50:47,465 Epoch 595 Step:    66000 Batch Loss:     0.000013 Tokens per Sec:  9998694, Lr: 0.001000
2020-02-26 17:50:51,615 Epoch 595: total training loss 0.00146
2020-02-26 17:50:51,616 EPOCH 596
2020-02-26 17:51:01,725 Epoch 596: total training loss 0.00146
2020-02-26 17:51:01,726 EPOCH 597
2020-02-26 17:51:10,240 Epoch 597 Step:    66250 Batch Loss:     0.000015 Tokens per Sec: 10253601, Lr: 0.001000
2020-02-26 17:51:11,770 Epoch 597: total training loss 0.00146
2020-02-26 17:51:11,771 EPOCH 598
2020-02-26 17:51:21,830 Epoch 598: total training loss 0.00143
2020-02-26 17:51:21,831 EPOCH 599
2020-02-26 17:51:32,242 Epoch 599: total training loss 0.00142
2020-02-26 17:51:32,242 EPOCH 600
2020-02-26 17:51:33,167 Epoch 600 Step:    66500 Batch Loss:     0.000011 Tokens per Sec:  9594681, Lr: 0.001000
2020-02-26 17:51:42,389 Epoch 600: total training loss 0.00142
2020-02-26 17:51:42,390 EPOCH 601
2020-02-26 17:51:52,668 Epoch 601: total training loss 0.00140
2020-02-26 17:51:52,668 EPOCH 602
2020-02-26 17:51:56,361 Epoch 602 Step:    66750 Batch Loss:     0.000007 Tokens per Sec: 10292142, Lr: 0.001000
2020-02-26 17:52:02,862 Epoch 602: total training loss 0.00144
2020-02-26 17:52:02,863 EPOCH 603
2020-02-26 17:52:12,934 Epoch 603: total training loss 0.00148
2020-02-26 17:52:12,935 EPOCH 604
2020-02-26 17:52:18,864 Epoch 604 Step:    67000 Batch Loss:     0.000005 Tokens per Sec: 10048894, Lr: 0.001000
2020-02-26 17:52:22,941 Epoch 604: total training loss 0.00146
2020-02-26 17:52:22,941 EPOCH 605
2020-02-26 17:52:32,908 Epoch 605: total training loss 0.00142
2020-02-26 17:52:32,909 EPOCH 606
2020-02-26 17:52:41,495 Epoch 606 Step:    67250 Batch Loss:     0.000014 Tokens per Sec: 10235728, Lr: 0.001000
2020-02-26 17:52:42,981 Epoch 606: total training loss 0.00141
2020-02-26 17:52:42,981 EPOCH 607
2020-02-26 17:52:53,156 Epoch 607: total training loss 0.00147
2020-02-26 17:52:53,158 EPOCH 608
2020-02-26 17:53:03,615 Epoch 608: total training loss 0.00142
2020-02-26 17:53:03,616 EPOCH 609
2020-02-26 17:53:04,667 Epoch 609 Step:    67500 Batch Loss:     0.000008 Tokens per Sec:  9559440, Lr: 0.001000
2020-02-26 17:53:13,777 Epoch 609: total training loss 0.00144
2020-02-26 17:53:13,777 EPOCH 610
2020-02-26 17:53:23,930 Epoch 610: total training loss 0.00144
2020-02-26 17:53:23,930 EPOCH 611
2020-02-26 17:53:27,562 Epoch 611 Step:    67750 Batch Loss:     0.000011 Tokens per Sec:  9979943, Lr: 0.001000
2020-02-26 17:53:34,290 Epoch 611: total training loss 0.00138
2020-02-26 17:53:34,291 EPOCH 612
2020-02-26 17:53:44,506 Epoch 612: total training loss 0.00137
2020-02-26 17:53:44,506 EPOCH 613
2020-02-26 17:53:50,710 Epoch 613 Step:    68000 Batch Loss:     0.000014 Tokens per Sec: 10360630, Lr: 0.001000
2020-02-26 17:53:54,484 Epoch 613: total training loss 0.00140
2020-02-26 17:53:54,485 EPOCH 614
2020-02-26 17:54:04,497 Epoch 614: total training loss 0.00140
2020-02-26 17:54:04,497 EPOCH 615
2020-02-26 17:54:13,199 Epoch 615 Step:    68250 Batch Loss:     0.000014 Tokens per Sec: 10178596, Lr: 0.001000
2020-02-26 17:54:14,564 Epoch 615: total training loss 0.00140
2020-02-26 17:54:14,565 EPOCH 616
2020-02-26 17:54:24,521 Epoch 616: total training loss 0.00139
2020-02-26 17:54:24,521 EPOCH 617
2020-02-26 17:54:34,481 Epoch 617: total training loss 0.00139
2020-02-26 17:54:34,482 EPOCH 618
2020-02-26 17:54:35,592 Epoch 618 Step:    68500 Batch Loss:     0.000007 Tokens per Sec:  9659161, Lr: 0.001000
2020-02-26 17:54:44,878 Epoch 618: total training loss 0.00140
2020-02-26 17:54:44,880 EPOCH 619
2020-02-26 17:54:55,371 Epoch 619: total training loss 0.00147
2020-02-26 17:54:55,372 EPOCH 620
2020-02-26 17:54:59,213 Epoch 620 Step:    68750 Batch Loss:     0.000009 Tokens per Sec:  9482915, Lr: 0.001000
2020-02-26 17:55:05,882 Epoch 620: total training loss 0.00144
2020-02-26 17:55:05,883 EPOCH 621
2020-02-26 17:55:16,466 Epoch 621: total training loss 0.00139
2020-02-26 17:55:16,467 EPOCH 622
2020-02-26 17:55:23,217 Epoch 622 Step:    69000 Batch Loss:     0.000014 Tokens per Sec:  9606907, Lr: 0.001000
2020-02-26 17:55:27,187 Epoch 622: total training loss 0.00137
2020-02-26 17:55:27,187 EPOCH 623
2020-02-26 17:55:37,393 Epoch 623: total training loss 0.00138
2020-02-26 17:55:37,394 EPOCH 624
2020-02-26 17:55:46,112 Epoch 624 Step:    69250 Batch Loss:     0.000010 Tokens per Sec: 10347346, Lr: 0.001000
2020-02-26 17:55:47,337 Epoch 624: total training loss 0.00140
2020-02-26 17:55:47,337 EPOCH 625
2020-02-26 17:55:57,354 Epoch 625: total training loss 0.00138
2020-02-26 17:55:57,354 EPOCH 626
2020-02-26 17:56:07,480 Epoch 626: total training loss 0.00136
2020-02-26 17:56:07,481 EPOCH 627
2020-02-26 17:56:08,691 Epoch 627 Step:    69500 Batch Loss:     0.000015 Tokens per Sec: 10230301, Lr: 0.001000
2020-02-26 17:56:17,372 Epoch 627: total training loss 0.00139
2020-02-26 17:56:17,372 EPOCH 628
2020-02-26 17:56:27,292 Epoch 628: total training loss 0.00138
2020-02-26 17:56:27,294 EPOCH 629
2020-02-26 17:56:31,323 Epoch 629 Step:    69750 Batch Loss:     0.000017 Tokens per Sec: 10215579, Lr: 0.001000
2020-02-26 17:56:37,307 Epoch 629: total training loss 0.00149
2020-02-26 17:56:37,308 EPOCH 630
2020-02-26 17:56:47,192 Epoch 630: total training loss 0.00141
2020-02-26 17:56:47,192 EPOCH 631
2020-02-26 17:56:53,301 Epoch 631 Step:    70000 Batch Loss:     0.000016 Tokens per Sec: 10422257, Lr: 0.001000
2020-02-26 17:56:53,301 Model noise rate: 5
2020-02-26 17:57:42,035 Hooray! New best validation result [dtw]!
2020-02-26 17:57:42,060 Saving new checkpoint.
2020-02-26 17:57:58,413 Validation result at epoch 631, step    70000: Val DTW Score:  10.64, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0344, GT DTW Score:      nan, duration: 65.1111s
2020-02-26 17:58:02,451 Epoch 631: total training loss 0.00136
2020-02-26 17:58:02,451 EPOCH 632
2020-02-26 17:58:12,593 Epoch 632: total training loss 0.00136
2020-02-26 17:58:12,594 EPOCH 633
2020-02-26 17:58:21,426 Epoch 633 Step:    70250 Batch Loss:     0.000017 Tokens per Sec: 10261823, Lr: 0.001000
2020-02-26 17:58:22,591 Epoch 633: total training loss 0.00137
2020-02-26 17:58:22,591 EPOCH 634
2020-02-26 17:58:32,483 Epoch 634: total training loss 0.00136
2020-02-26 17:58:32,483 EPOCH 635
2020-02-26 17:58:42,549 Epoch 635: total training loss 0.00140
2020-02-26 17:58:42,550 EPOCH 636
2020-02-26 17:58:43,801 Epoch 636 Step:    70500 Batch Loss:     0.000011 Tokens per Sec:  9621156, Lr: 0.001000
2020-02-26 17:58:52,507 Epoch 636: total training loss 0.00136
2020-02-26 17:58:52,509 EPOCH 637
2020-02-26 17:59:02,668 Epoch 637: total training loss 0.00146
2020-02-26 17:59:02,671 EPOCH 638
2020-02-26 17:59:06,658 Epoch 638 Step:    70750 Batch Loss:     0.000019 Tokens per Sec:  9847000, Lr: 0.001000
2020-02-26 17:59:12,987 Epoch 638: total training loss 0.00141
2020-02-26 17:59:12,988 EPOCH 639
2020-02-26 17:59:23,406 Epoch 639: total training loss 0.00136
2020-02-26 17:59:23,407 EPOCH 640
2020-02-26 17:59:30,043 Epoch 640 Step:    71000 Batch Loss:     0.000012 Tokens per Sec: 10063147, Lr: 0.001000
2020-02-26 17:59:33,577 Epoch 640: total training loss 0.00134
2020-02-26 17:59:33,578 EPOCH 641
2020-02-26 17:59:43,769 Epoch 641: total training loss 0.00134
2020-02-26 17:59:43,770 EPOCH 642
2020-02-26 17:59:52,918 Epoch 642 Step:    71250 Batch Loss:     0.000008 Tokens per Sec: 10111142, Lr: 0.001000
2020-02-26 17:59:53,998 Epoch 642: total training loss 0.00134
2020-02-26 17:59:53,998 EPOCH 643
2020-02-26 18:00:04,127 Epoch 643: total training loss 0.00137
2020-02-26 18:00:04,127 EPOCH 644
2020-02-26 18:00:14,322 Epoch 644: total training loss 0.00135
2020-02-26 18:00:14,323 EPOCH 645
2020-02-26 18:00:15,741 Epoch 645 Step:    71500 Batch Loss:     0.000008 Tokens per Sec: 10054551, Lr: 0.001000
2020-02-26 18:00:24,437 Epoch 645: total training loss 0.00133
2020-02-26 18:00:24,438 EPOCH 646
2020-02-26 18:00:34,370 Epoch 646: total training loss 0.00136
2020-02-26 18:00:34,370 EPOCH 647
2020-02-26 18:00:38,447 Epoch 647 Step:    71750 Batch Loss:     0.000017 Tokens per Sec: 10411251, Lr: 0.001000
2020-02-26 18:00:44,284 Epoch 647: total training loss 0.00138
2020-02-26 18:00:44,284 EPOCH 648
2020-02-26 18:00:54,667 Epoch 648: total training loss 0.00134
2020-02-26 18:00:54,667 EPOCH 649
2020-02-26 18:01:01,586 Epoch 649 Step:    72000 Batch Loss:     0.000012 Tokens per Sec:  9758784, Lr: 0.001000
2020-02-26 18:01:05,205 Epoch 649: total training loss 0.00136
2020-02-26 18:01:05,205 EPOCH 650
2020-02-26 18:01:15,512 Epoch 650: total training loss 0.00134
2020-02-26 18:01:15,513 EPOCH 651
2020-02-26 18:01:24,741 Epoch 651 Step:    72250 Batch Loss:     0.000006 Tokens per Sec:  9999065, Lr: 0.001000
2020-02-26 18:01:25,836 Epoch 651: total training loss 0.00130
2020-02-26 18:01:25,836 EPOCH 652
2020-02-26 18:01:36,062 Epoch 652: total training loss 0.00131
2020-02-26 18:01:36,062 EPOCH 653
2020-02-26 18:01:46,132 Epoch 653: total training loss 0.00142
2020-02-26 18:01:46,132 EPOCH 654
2020-02-26 18:01:47,823 Epoch 654 Step:    72500 Batch Loss:     0.000013 Tokens per Sec:  9924192, Lr: 0.001000
2020-02-26 18:01:56,168 Epoch 654: total training loss 0.00136
2020-02-26 18:01:56,168 EPOCH 655
2020-02-26 18:02:06,328 Epoch 655: total training loss 0.00131
2020-02-26 18:02:06,329 EPOCH 656
2020-02-26 18:02:10,430 Epoch 656 Step:    72750 Batch Loss:     0.000009 Tokens per Sec: 10200853, Lr: 0.001000
2020-02-26 18:02:16,365 Epoch 656: total training loss 0.00134
2020-02-26 18:02:16,365 EPOCH 657
2020-02-26 18:02:26,480 Epoch 657: total training loss 0.00131
2020-02-26 18:02:26,482 EPOCH 658
2020-02-26 18:02:33,232 Epoch 658 Step:    73000 Batch Loss:     0.000009 Tokens per Sec: 10224373, Lr: 0.001000
2020-02-26 18:02:36,579 Epoch 658: total training loss 0.00135
2020-02-26 18:02:36,580 EPOCH 659
2020-02-26 18:02:47,106 Epoch 659: total training loss 0.00136
2020-02-26 18:02:47,107 EPOCH 660
2020-02-26 18:02:56,534 Epoch 660 Step:    73250 Batch Loss:     0.000006 Tokens per Sec:  9749238, Lr: 0.001000
2020-02-26 18:02:57,588 Epoch 660: total training loss 0.00138
2020-02-26 18:02:57,588 EPOCH 661
2020-02-26 18:03:07,845 Epoch 661: total training loss 0.00132
2020-02-26 18:03:07,845 EPOCH 662
2020-02-26 18:03:18,063 Epoch 662: total training loss 0.00136
2020-02-26 18:03:18,064 EPOCH 663
2020-02-26 18:03:19,799 Epoch 663 Step:    73500 Batch Loss:     0.000012 Tokens per Sec: 10414679, Lr: 0.001000
2020-02-26 18:03:28,195 Epoch 663: total training loss 0.00133
2020-02-26 18:03:28,195 EPOCH 664
2020-02-26 18:03:38,026 Epoch 664: total training loss 0.00130
2020-02-26 18:03:38,027 EPOCH 665
2020-02-26 18:03:42,080 Epoch 665 Step:    73750 Batch Loss:     0.000009 Tokens per Sec: 10155759, Lr: 0.001000
2020-02-26 18:03:47,923 Epoch 665: total training loss 0.00129
2020-02-26 18:03:47,923 EPOCH 666
2020-02-26 18:03:58,026 Epoch 666: total training loss 0.00130
2020-02-26 18:03:58,026 EPOCH 667
2020-02-26 18:04:04,904 Epoch 667 Step:    74000 Batch Loss:     0.000011 Tokens per Sec: 10090801, Lr: 0.001000
2020-02-26 18:04:08,273 Epoch 667: total training loss 0.00130
2020-02-26 18:04:08,274 EPOCH 668
2020-02-26 18:04:18,504 Epoch 668: total training loss 0.00133
2020-02-26 18:04:18,504 EPOCH 669
2020-02-26 18:04:27,911 Epoch 669 Step:    74250 Batch Loss:     0.000020 Tokens per Sec: 10045133, Lr: 0.001000
2020-02-26 18:04:28,753 Epoch 669: total training loss 0.00131
2020-02-26 18:04:28,754 EPOCH 670
2020-02-26 18:04:38,977 Epoch 670: total training loss 0.00132
2020-02-26 18:04:38,978 EPOCH 671
2020-02-26 18:04:49,273 Epoch 671: total training loss 0.00129
2020-02-26 18:04:49,273 EPOCH 672
2020-02-26 18:04:51,159 Epoch 672 Step:    74500 Batch Loss:     0.000009 Tokens per Sec:  9850132, Lr: 0.001000
2020-02-26 18:04:59,485 Epoch 672: total training loss 0.00130
2020-02-26 18:04:59,485 EPOCH 673
2020-02-26 18:05:09,500 Epoch 673: total training loss 0.00135
2020-02-26 18:05:09,500 EPOCH 674
2020-02-26 18:05:13,852 Epoch 674 Step:    74750 Batch Loss:     0.000016 Tokens per Sec: 10262976, Lr: 0.001000
2020-02-26 18:05:19,603 Epoch 674: total training loss 0.00137
2020-02-26 18:05:19,603 EPOCH 675
2020-02-26 18:05:29,759 Epoch 675: total training loss 0.00131
2020-02-26 18:05:29,760 EPOCH 676
2020-02-26 18:05:36,447 Epoch 676 Step:    75000 Batch Loss:     0.000013 Tokens per Sec: 10224016, Lr: 0.001000
2020-02-26 18:05:39,736 Epoch 676: total training loss 0.00131
2020-02-26 18:05:39,736 EPOCH 677
2020-02-26 18:05:49,740 Epoch 677: total training loss 0.00128
2020-02-26 18:05:49,740 EPOCH 678
2020-02-26 18:05:58,954 Epoch 678 Step:    75250 Batch Loss:     0.000013 Tokens per Sec: 10256312, Lr: 0.001000
2020-02-26 18:05:59,716 Epoch 678: total training loss 0.00133
2020-02-26 18:05:59,716 EPOCH 679
2020-02-26 18:06:10,206 Epoch 679: total training loss 0.00129
2020-02-26 18:06:10,207 EPOCH 680
2020-02-26 18:06:20,836 Epoch 680: total training loss 0.00133
2020-02-26 18:06:20,837 EPOCH 681
2020-02-26 18:06:22,944 Epoch 681 Step:    75500 Batch Loss:     0.000015 Tokens per Sec:  9831043, Lr: 0.001000
2020-02-26 18:06:31,166 Epoch 681: total training loss 0.00129
2020-02-26 18:06:31,167 EPOCH 682
2020-02-26 18:06:41,534 Epoch 682: total training loss 0.00133
2020-02-26 18:06:41,534 EPOCH 683
2020-02-26 18:06:46,094 Epoch 683 Step:    75750 Batch Loss:     0.000009 Tokens per Sec:  9779136, Lr: 0.001000
2020-02-26 18:06:51,840 Epoch 683: total training loss 0.00136
2020-02-26 18:06:51,841 EPOCH 684
2020-02-26 18:07:01,729 Epoch 684: total training loss 0.00128
2020-02-26 18:07:01,730 EPOCH 685
2020-02-26 18:07:08,531 Epoch 685 Step:    76000 Batch Loss:     0.000008 Tokens per Sec: 10485032, Lr: 0.001000
2020-02-26 18:07:11,562 Epoch 685: total training loss 0.00126
2020-02-26 18:07:11,562 EPOCH 686
2020-02-26 18:07:21,466 Epoch 686: total training loss 0.00125
2020-02-26 18:07:21,467 EPOCH 687
2020-02-26 18:07:30,786 Epoch 687 Step:    76250 Batch Loss:     0.000007 Tokens per Sec: 10290923, Lr: 0.001000
2020-02-26 18:07:31,440 Epoch 687: total training loss 0.00128
2020-02-26 18:07:31,441 EPOCH 688
2020-02-26 18:07:41,427 Epoch 688: total training loss 0.00125
2020-02-26 18:07:41,500 EPOCH 689
2020-02-26 18:07:51,629 Epoch 689: total training loss 0.00128
2020-02-26 18:07:51,630 EPOCH 690
2020-02-26 18:07:53,660 Epoch 690 Step:    76500 Batch Loss:     0.000013 Tokens per Sec: 10131893, Lr: 0.001000
2020-02-26 18:08:01,924 Epoch 690: total training loss 0.00128
2020-02-26 18:08:01,924 EPOCH 691
2020-02-26 18:08:12,344 Epoch 691: total training loss 0.00126
2020-02-26 18:08:12,346 EPOCH 692
2020-02-26 18:08:17,178 Epoch 692 Step:    76750 Batch Loss:     0.000017 Tokens per Sec:  9561223, Lr: 0.001000
2020-02-26 18:08:23,038 Epoch 692: total training loss 0.00126
2020-02-26 18:08:23,038 EPOCH 693
2020-02-26 18:08:33,253 Epoch 693: total training loss 0.00137
2020-02-26 18:08:33,254 EPOCH 694
2020-02-26 18:08:40,231 Epoch 694 Step:    77000 Batch Loss:     0.000008 Tokens per Sec: 10221461, Lr: 0.001000
2020-02-26 18:08:43,281 Epoch 694: total training loss 0.00129
2020-02-26 18:08:43,281 EPOCH 695
2020-02-26 18:08:53,384 Epoch 695: total training loss 0.00126
2020-02-26 18:08:53,385 EPOCH 696
2020-02-26 18:09:02,907 Epoch 696 Step:    77250 Batch Loss:     0.000006 Tokens per Sec: 10129045, Lr: 0.001000
2020-02-26 18:09:03,553 Epoch 696: total training loss 0.00127
2020-02-26 18:09:03,554 EPOCH 697
2020-02-26 18:09:13,564 Epoch 697: total training loss 0.00143
2020-02-26 18:09:13,564 EPOCH 698
2020-02-26 18:09:23,495 Epoch 698: total training loss 0.00132
2020-02-26 18:09:23,495 EPOCH 699
2020-02-26 18:09:25,528 Epoch 699 Step:    77500 Batch Loss:     0.000013 Tokens per Sec: 10389970, Lr: 0.001000
2020-02-26 18:09:33,501 Epoch 699: total training loss 0.00125
2020-02-26 18:09:33,503 EPOCH 700
2020-02-26 18:09:43,652 Epoch 700: total training loss 0.00126
2020-02-26 18:09:43,653 EPOCH 701
2020-02-26 18:09:48,389 Epoch 701 Step:    77750 Batch Loss:     0.000021 Tokens per Sec:  9975569, Lr: 0.001000
2020-02-26 18:09:54,039 Epoch 701: total training loss 0.00125
2020-02-26 18:09:54,040 EPOCH 702
2020-02-26 18:10:04,518 Epoch 702: total training loss 0.00131
2020-02-26 18:10:04,519 EPOCH 703
2020-02-26 18:10:11,834 Epoch 703 Step:    78000 Batch Loss:     0.000009 Tokens per Sec:  9974356, Lr: 0.001000
2020-02-26 18:10:14,907 Epoch 703: total training loss 0.00129
2020-02-26 18:10:14,908 EPOCH 704
2020-02-26 18:10:24,842 Epoch 704: total training loss 0.00125
2020-02-26 18:10:24,842 EPOCH 705
2020-02-26 18:10:34,421 Epoch 705 Step:    78250 Batch Loss:     0.000010 Tokens per Sec: 10188110, Lr: 0.001000
2020-02-26 18:10:34,909 Epoch 705: total training loss 0.00129
2020-02-26 18:10:34,909 EPOCH 706
2020-02-26 18:10:44,980 Epoch 706: total training loss 0.00125
2020-02-26 18:10:44,981 EPOCH 707
2020-02-26 18:10:55,121 Epoch 707: total training loss 0.00123
2020-02-26 18:10:55,121 EPOCH 708
2020-02-26 18:10:57,078 Epoch 708 Step:    78500 Batch Loss:     0.000011 Tokens per Sec: 10166498, Lr: 0.001000
2020-02-26 18:11:05,074 Epoch 708: total training loss 0.00128
2020-02-26 18:11:05,075 EPOCH 709
2020-02-26 18:11:15,173 Epoch 709: total training loss 0.00130
2020-02-26 18:11:15,175 EPOCH 710
2020-02-26 18:11:19,814 Epoch 710 Step:    78750 Batch Loss:     0.000009 Tokens per Sec: 10218500, Lr: 0.001000
2020-02-26 18:11:25,413 Epoch 710: total training loss 0.00126
2020-02-26 18:11:25,413 EPOCH 711
2020-02-26 18:11:35,922 Epoch 711: total training loss 0.00123
2020-02-26 18:11:35,923 EPOCH 712
2020-02-26 18:11:43,281 Epoch 712 Step:    79000 Batch Loss:     0.000015 Tokens per Sec: 10034816, Lr: 0.001000
2020-02-26 18:11:46,182 Epoch 712: total training loss 0.00122
2020-02-26 18:11:46,182 EPOCH 713
2020-02-26 18:11:56,505 Epoch 713: total training loss 0.00121
2020-02-26 18:11:56,505 EPOCH 714
2020-02-26 18:12:06,473 Epoch 714 Step:    79250 Batch Loss:     0.000013 Tokens per Sec: 10002131, Lr: 0.001000
2020-02-26 18:12:06,786 Epoch 714: total training loss 0.00125
2020-02-26 18:12:06,787 EPOCH 715
2020-02-26 18:12:16,825 Epoch 715: total training loss 0.00127
2020-02-26 18:12:16,826 EPOCH 716
2020-02-26 18:12:27,031 Epoch 716: total training loss 0.00126
2020-02-26 18:12:27,031 EPOCH 717
2020-02-26 18:12:29,186 Epoch 717 Step:    79500 Batch Loss:     0.000014 Tokens per Sec: 10316197, Lr: 0.001000
2020-02-26 18:12:36,984 Epoch 717: total training loss 0.00125
2020-02-26 18:12:36,984 EPOCH 718
2020-02-26 18:12:47,042 Epoch 718: total training loss 0.00123
2020-02-26 18:12:47,042 EPOCH 719
2020-02-26 18:12:51,554 Epoch 719 Step:    79750 Batch Loss:     0.000012 Tokens per Sec: 10407914, Lr: 0.001000
2020-02-26 18:12:57,014 Epoch 719: total training loss 0.00125
2020-02-26 18:12:57,014 EPOCH 720
2020-02-26 18:13:07,011 Epoch 720: total training loss 0.00123
2020-02-26 18:13:07,013 EPOCH 721
2020-02-26 18:13:14,428 Epoch 721 Step:    80000 Batch Loss:     0.000012 Tokens per Sec:  9872754, Lr: 0.001000
2020-02-26 18:13:14,430 Model noise rate: 5
2020-02-26 18:14:04,022 Hooray! New best validation result [dtw]!
2020-02-26 18:14:04,022 Saving new checkpoint.
2020-02-26 18:14:19,507 Validation result at epoch 721, step    80000: Val DTW Score:  10.63, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0342, GT DTW Score:      nan, duration: 65.0764s
2020-02-26 18:14:22,420 Epoch 721: total training loss 0.00126
2020-02-26 18:14:22,421 EPOCH 722
2020-02-26 18:14:32,340 Epoch 722: total training loss 0.00124
2020-02-26 18:14:32,340 EPOCH 723
2020-02-26 18:14:42,027 Epoch 723 Step:    80250 Batch Loss:     0.000011 Tokens per Sec: 10289786, Lr: 0.001000
2020-02-26 18:14:42,337 Epoch 723: total training loss 0.00122
2020-02-26 18:14:42,337 EPOCH 724
2020-02-26 18:14:52,757 Epoch 724: total training loss 0.00123
2020-02-26 18:14:52,758 EPOCH 725
2020-02-26 18:15:03,133 Epoch 725: total training loss 0.00120
2020-02-26 18:15:03,134 EPOCH 726
2020-02-26 18:15:05,439 Epoch 726 Step:    80500 Batch Loss:     0.000012 Tokens per Sec:  9913425, Lr: 0.001000
2020-02-26 18:15:13,486 Epoch 726: total training loss 0.00126
2020-02-26 18:15:13,487 EPOCH 727
2020-02-26 18:15:23,631 Epoch 727: total training loss 0.00123
2020-02-26 18:15:23,631 EPOCH 728
2020-02-26 18:15:28,429 Epoch 728 Step:    80750 Batch Loss:     0.000006 Tokens per Sec:  9942901, Lr: 0.001000
2020-02-26 18:15:33,784 Epoch 728: total training loss 0.00125
2020-02-26 18:15:33,784 EPOCH 729
2020-02-26 18:15:43,887 Epoch 729: total training loss 0.00129
2020-02-26 18:15:43,888 EPOCH 730
2020-02-26 18:15:51,401 Epoch 730 Step:    81000 Batch Loss:     0.000018 Tokens per Sec: 10193871, Lr: 0.001000
2020-02-26 18:15:54,100 Epoch 730: total training loss 0.00121
2020-02-26 18:15:54,100 EPOCH 731
2020-02-26 18:16:04,147 Epoch 731: total training loss 0.00120
2020-02-26 18:16:04,148 EPOCH 732
2020-02-26 18:16:14,111 Epoch 732 Step:    81250 Batch Loss:     0.000014 Tokens per Sec: 10114885, Lr: 0.001000
2020-02-26 18:16:14,341 Epoch 732: total training loss 0.00119
2020-02-26 18:16:14,342 EPOCH 733
2020-02-26 18:16:24,464 Epoch 733: total training loss 0.00120
2020-02-26 18:16:24,464 EPOCH 734
2020-02-26 18:16:34,576 Epoch 734: total training loss 0.00121
2020-02-26 18:16:34,576 EPOCH 735
2020-02-26 18:16:36,977 Epoch 735 Step:    81500 Batch Loss:     0.000012 Tokens per Sec: 10525219, Lr: 0.001000
2020-02-26 18:16:44,601 Epoch 735: total training loss 0.00134
2020-02-26 18:16:44,602 EPOCH 736
2020-02-26 18:16:55,058 Epoch 736: total training loss 0.00128
2020-02-26 18:16:55,058 EPOCH 737
2020-02-26 18:16:59,920 Epoch 737 Step:    81750 Batch Loss:     0.000012 Tokens per Sec:  9739281, Lr: 0.001000
2020-02-26 18:17:05,514 Epoch 737: total training loss 0.00121
2020-02-26 18:17:05,514 EPOCH 738
2020-02-26 18:17:15,661 Epoch 738: total training loss 0.00124
2020-02-26 18:17:15,661 EPOCH 739
2020-02-26 18:17:23,241 Epoch 739 Step:    82000 Batch Loss:     0.000012 Tokens per Sec: 10207601, Lr: 0.001000
2020-02-26 18:17:25,766 Epoch 739: total training loss 0.00122
2020-02-26 18:17:25,767 EPOCH 740
2020-02-26 18:17:35,489 Epoch 740: total training loss 0.00122
2020-02-26 18:17:35,490 EPOCH 741
2020-02-26 18:17:45,326 Epoch 741 Step:    82250 Batch Loss:     0.000011 Tokens per Sec: 10443380, Lr: 0.001000
2020-02-26 18:17:45,378 Epoch 741: total training loss 0.00121
2020-02-26 18:17:45,378 EPOCH 742
2020-02-26 18:17:55,230 Epoch 742: total training loss 0.00123
2020-02-26 18:17:55,231 EPOCH 743
2020-02-26 18:18:05,272 Epoch 743: total training loss 0.00119
2020-02-26 18:18:05,274 EPOCH 744
2020-02-26 18:18:07,758 Epoch 744 Step:    82500 Batch Loss:     0.000011 Tokens per Sec: 10404919, Lr: 0.001000
2020-02-26 18:18:15,331 Epoch 744: total training loss 0.00118
2020-02-26 18:18:15,332 EPOCH 745
2020-02-26 18:18:25,335 Epoch 745: total training loss 0.00128
2020-02-26 18:18:25,336 EPOCH 746
2020-02-26 18:18:30,187 Epoch 746 Step:    82750 Batch Loss:     0.000009 Tokens per Sec: 10535411, Lr: 0.001000
2020-02-26 18:18:35,008 Epoch 746: total training loss 0.00122
2020-02-26 18:18:35,008 EPOCH 747
2020-02-26 18:18:44,726 Epoch 747: total training loss 0.00123
2020-02-26 18:18:44,726 EPOCH 748
2020-02-26 18:18:52,161 Epoch 748 Step:    83000 Batch Loss:     0.000007 Tokens per Sec: 10375556, Lr: 0.001000
2020-02-26 18:18:54,573 Epoch 748: total training loss 0.00123
2020-02-26 18:18:54,573 EPOCH 749
2020-02-26 18:19:04,526 Epoch 749: total training loss 0.00118
2020-02-26 18:19:04,528 EPOCH 750
2020-02-26 18:19:14,740 Epoch 750 Step:    83250 Batch Loss:     0.000010 Tokens per Sec: 10084662, Lr: 0.001000
2020-02-26 18:19:14,741 Epoch 750: total training loss 0.00120
2020-02-26 18:19:14,741 EPOCH 751
2020-02-26 18:19:24,965 Epoch 751: total training loss 0.00118
2020-02-26 18:19:24,965 EPOCH 752
2020-02-26 18:19:35,203 Epoch 752: total training loss 0.00119
2020-02-26 18:19:35,203 EPOCH 753
2020-02-26 18:19:37,806 Epoch 753 Step:    83500 Batch Loss:     0.000010 Tokens per Sec:  9668194, Lr: 0.001000
2020-02-26 18:19:45,448 Epoch 753: total training loss 0.00119
2020-02-26 18:19:45,448 EPOCH 754
2020-02-26 18:19:55,597 Epoch 754: total training loss 0.00120
2020-02-26 18:19:55,597 EPOCH 755
2020-02-26 18:20:00,727 Epoch 755 Step:    83750 Batch Loss:     0.000010 Tokens per Sec: 10463260, Lr: 0.001000
2020-02-26 18:20:05,457 Epoch 755: total training loss 0.00120
2020-02-26 18:20:05,458 EPOCH 756
2020-02-26 18:20:15,425 Epoch 756: total training loss 0.00123
2020-02-26 18:20:15,426 EPOCH 757
2020-02-26 18:20:22,894 Epoch 757 Step:    84000 Batch Loss:     0.000007 Tokens per Sec: 10421116, Lr: 0.001000
2020-02-26 18:20:25,243 Epoch 757: total training loss 0.00130
2020-02-26 18:20:25,243 EPOCH 758
2020-02-26 18:20:35,161 Epoch 758: total training loss 0.00124
2020-02-26 18:20:35,161 EPOCH 759
2020-02-26 18:20:45,340 Epoch 759: total training loss 0.00121
2020-02-26 18:20:45,341 EPOCH 760
2020-02-26 18:20:45,456 Epoch 760 Step:    84250 Batch Loss:     0.000010 Tokens per Sec:  8647092, Lr: 0.001000
2020-02-26 18:20:55,593 Epoch 760: total training loss 0.00118
2020-02-26 18:20:55,596 EPOCH 761
2020-02-26 18:21:06,229 Epoch 761: total training loss 0.00119
2020-02-26 18:21:06,230 EPOCH 762
2020-02-26 18:21:08,906 Epoch 762 Step:    84500 Batch Loss:     0.000010 Tokens per Sec:  9799376, Lr: 0.001000
2020-02-26 18:21:16,647 Epoch 762: total training loss 0.00120
2020-02-26 18:21:16,647 EPOCH 763
2020-02-26 18:21:27,280 Epoch 763: total training loss 0.00118
2020-02-26 18:21:27,280 EPOCH 764
2020-02-26 18:21:32,704 Epoch 764 Step:    84750 Batch Loss:     0.000009 Tokens per Sec:  9963700, Lr: 0.001000
2020-02-26 18:21:37,550 Epoch 764: total training loss 0.00118
2020-02-26 18:21:37,551 EPOCH 765
2020-02-26 18:21:47,598 Epoch 765: total training loss 0.00117
2020-02-26 18:21:47,598 EPOCH 766
2020-02-26 18:21:55,190 Epoch 766 Step:    85000 Batch Loss:     0.000007 Tokens per Sec: 10165337, Lr: 0.001000
2020-02-26 18:21:57,640 Epoch 766: total training loss 0.00117
2020-02-26 18:21:57,640 EPOCH 767
2020-02-26 18:22:07,803 Epoch 767: total training loss 0.00121
2020-02-26 18:22:07,803 EPOCH 768
2020-02-26 18:22:17,787 Epoch 768: total training loss 0.00118
2020-02-26 18:22:17,787 EPOCH 769
2020-02-26 18:22:17,983 Epoch 769 Step:    85250 Batch Loss:     0.000013 Tokens per Sec:  8755734, Lr: 0.001000
2020-02-26 18:22:27,745 Epoch 769: total training loss 0.00117
2020-02-26 18:22:27,746 EPOCH 770
2020-02-26 18:22:37,774 Epoch 770: total training loss 0.00116
2020-02-26 18:22:37,776 EPOCH 771
2020-02-26 18:22:40,559 Epoch 771 Step:    85500 Batch Loss:     0.000006 Tokens per Sec: 10189336, Lr: 0.001000
2020-02-26 18:22:47,893 Epoch 771: total training loss 0.00116
2020-02-26 18:22:47,894 EPOCH 772
2020-02-26 18:22:58,125 Epoch 772: total training loss 0.00121
2020-02-26 18:22:58,125 EPOCH 773
2020-02-26 18:23:03,344 Epoch 773 Step:    85750 Batch Loss:     0.000009 Tokens per Sec: 10071599, Lr: 0.001000
2020-02-26 18:23:08,240 Epoch 773: total training loss 0.00121
2020-02-26 18:23:08,240 EPOCH 774
2020-02-26 18:23:18,835 Epoch 774: total training loss 0.00124
2020-02-26 18:23:18,835 EPOCH 775
2020-02-26 18:23:26,766 Epoch 775 Step:    86000 Batch Loss:     0.000004 Tokens per Sec:  9903212, Lr: 0.001000
2020-02-26 18:23:29,176 Epoch 775: total training loss 0.00117
2020-02-26 18:23:29,176 EPOCH 776
2020-02-26 18:23:39,289 Epoch 776: total training loss 0.00116
2020-02-26 18:23:39,289 EPOCH 777
2020-02-26 18:23:49,499 Epoch 777: total training loss 0.00119
2020-02-26 18:23:49,501 EPOCH 778
2020-02-26 18:23:49,798 Epoch 778 Step:    86250 Batch Loss:     0.000013 Tokens per Sec:  9131317, Lr: 0.001000
2020-02-26 18:23:59,710 Epoch 778: total training loss 0.00122
2020-02-26 18:23:59,710 EPOCH 779
2020-02-26 18:24:09,620 Epoch 779: total training loss 0.00117
2020-02-26 18:24:09,620 EPOCH 780
2020-02-26 18:24:12,345 Epoch 780 Step:    86500 Batch Loss:     0.000013 Tokens per Sec: 10289306, Lr: 0.001000
2020-02-26 18:24:19,497 Epoch 780: total training loss 0.00118
2020-02-26 18:24:19,499 EPOCH 781
2020-02-26 18:24:29,761 Epoch 781: total training loss 0.00118
2020-02-26 18:24:29,762 EPOCH 782
2020-02-26 18:24:35,185 Epoch 782 Step:    86750 Batch Loss:     0.000011 Tokens per Sec: 10026959, Lr: 0.001000
2020-02-26 18:24:40,150 Epoch 782: total training loss 0.00123
2020-02-26 18:24:40,150 EPOCH 783
2020-02-26 18:24:50,294 Epoch 783: total training loss 0.00134
2020-02-26 18:24:50,294 EPOCH 784
2020-02-26 18:24:58,234 Epoch 784 Step:    87000 Batch Loss:     0.000007 Tokens per Sec:  9889846, Lr: 0.001000
2020-02-26 18:25:00,619 Epoch 784: total training loss 0.00115
2020-02-26 18:25:00,619 EPOCH 785
2020-02-26 18:25:10,698 Epoch 785: total training loss 0.00114
2020-02-26 18:25:10,698 EPOCH 786
2020-02-26 18:25:20,604 Epoch 786: total training loss 0.00117
2020-02-26 18:25:20,604 EPOCH 787
2020-02-26 18:25:20,959 Epoch 787 Step:    87250 Batch Loss:     0.000007 Tokens per Sec:  8894312, Lr: 0.001000
2020-02-26 18:25:30,629 Epoch 787: total training loss 0.00120
2020-02-26 18:25:30,630 EPOCH 788
2020-02-26 18:25:40,558 Epoch 788: total training loss 0.00116
2020-02-26 18:25:40,559 EPOCH 789
2020-02-26 18:25:43,615 Epoch 789 Step:    87500 Batch Loss:     0.000015 Tokens per Sec: 10245872, Lr: 0.001000
2020-02-26 18:25:50,608 Epoch 789: total training loss 0.00114
2020-02-26 18:25:50,610 EPOCH 790
2020-02-26 18:26:01,018 Epoch 790: total training loss 0.00117
2020-02-26 18:26:01,019 EPOCH 791
2020-02-26 18:26:06,582 Epoch 791 Step:    87750 Batch Loss:     0.000011 Tokens per Sec:  9752049, Lr: 0.001000
2020-02-26 18:26:11,526 Epoch 791: total training loss 0.00114
2020-02-26 18:26:11,526 EPOCH 792
2020-02-26 18:26:22,053 Epoch 792: total training loss 0.00115
2020-02-26 18:26:22,054 EPOCH 793
2020-02-26 18:26:30,483 Epoch 793 Step:    88000 Batch Loss:     0.000010 Tokens per Sec:  9844922, Lr: 0.001000
2020-02-26 18:26:32,606 Epoch 793: total training loss 0.00113
2020-02-26 18:26:32,606 EPOCH 794
2020-02-26 18:26:42,944 Epoch 794: total training loss 0.00121
2020-02-26 18:26:42,945 EPOCH 795
2020-02-26 18:26:52,863 Epoch 795: total training loss 0.00125
2020-02-26 18:26:52,866 EPOCH 796
2020-02-26 18:26:53,326 Epoch 796 Step:    88250 Batch Loss:     0.000013 Tokens per Sec:  9597481, Lr: 0.001000
2020-02-26 18:27:02,941 Epoch 796: total training loss 0.00115
2020-02-26 18:27:02,941 EPOCH 797
2020-02-26 18:27:12,977 Epoch 797: total training loss 0.00114
2020-02-26 18:27:12,978 EPOCH 798
2020-02-26 18:27:15,837 Epoch 798 Step:    88500 Batch Loss:     0.000008 Tokens per Sec: 10106663, Lr: 0.001000
2020-02-26 18:27:22,968 Epoch 798: total training loss 0.00115
2020-02-26 18:27:22,969 EPOCH 799
2020-02-26 18:27:32,923 Epoch 799: total training loss 0.00118
2020-02-26 18:27:32,923 EPOCH 800
2020-02-26 18:27:38,241 Epoch 800 Step:    88750 Batch Loss:     0.000012 Tokens per Sec: 10279809, Lr: 0.001000
2020-02-26 18:27:42,886 Epoch 800: total training loss 0.00124
2020-02-26 18:27:42,887 EPOCH 801
2020-02-26 18:27:52,933 Epoch 801: total training loss 0.00116
2020-02-26 18:27:52,933 EPOCH 802
2020-02-26 18:28:00,876 Epoch 802 Step:    89000 Batch Loss:     0.000009 Tokens per Sec: 10222877, Lr: 0.001000
2020-02-26 18:28:03,021 Epoch 802: total training loss 0.00118
2020-02-26 18:28:03,021 EPOCH 803
2020-02-26 18:28:13,352 Epoch 803: total training loss 0.00113
2020-02-26 18:28:13,354 EPOCH 804
2020-02-26 18:28:23,800 Epoch 804: total training loss 0.00113
2020-02-26 18:28:24,436 EPOCH 805
2020-02-26 18:28:25,032 Epoch 805 Step:    89250 Batch Loss:     0.000010 Tokens per Sec: 10198507, Lr: 0.001000
2020-02-26 18:28:34,529 Epoch 805: total training loss 0.00117
2020-02-26 18:28:34,530 EPOCH 806
2020-02-26 18:28:44,761 Epoch 806: total training loss 0.00113
2020-02-26 18:28:44,761 EPOCH 807
2020-02-26 18:28:47,861 Epoch 807 Step:    89500 Batch Loss:     0.000008 Tokens per Sec:  9904744, Lr: 0.001000
2020-02-26 18:28:54,865 Epoch 807: total training loss 0.00114
2020-02-26 18:28:54,865 EPOCH 808
2020-02-26 18:29:04,786 Epoch 808: total training loss 0.00113
2020-02-26 18:29:04,787 EPOCH 809
2020-02-26 18:29:10,293 Epoch 809 Step:    89750 Batch Loss:     0.000006 Tokens per Sec: 10440377, Lr: 0.001000
2020-02-26 18:29:14,713 Epoch 809: total training loss 0.00113
2020-02-26 18:29:14,713 EPOCH 810
2020-02-26 18:29:24,743 Epoch 810: total training loss 0.00112
2020-02-26 18:29:24,744 EPOCH 811
2020-02-26 18:29:33,061 Epoch 811 Step:    90000 Batch Loss:     0.000007 Tokens per Sec: 10050230, Lr: 0.001000
2020-02-26 18:29:33,062 Model noise rate: 5
2020-02-26 18:30:35,312 Validation result at epoch 811, step    90000: Val DTW Score:  10.64, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0355, GT DTW Score:      nan, duration: 62.2496s
2020-02-26 18:30:37,285 Epoch 811: total training loss 0.00111
2020-02-26 18:30:37,285 EPOCH 812
2020-02-26 18:30:47,327 Epoch 812: total training loss 0.00114
2020-02-26 18:30:47,327 EPOCH 813
2020-02-26 18:30:57,344 Epoch 813: total training loss 0.00121
2020-02-26 18:30:57,345 EPOCH 814
2020-02-26 18:30:58,047 Epoch 814 Step:    90250 Batch Loss:     0.000010 Tokens per Sec: 10241250, Lr: 0.001000
2020-02-26 18:31:07,241 Epoch 814: total training loss 0.00116
2020-02-26 18:31:07,242 EPOCH 815
2020-02-26 18:31:17,675 Epoch 815: total training loss 0.00115
2020-02-26 18:31:17,675 EPOCH 816
2020-02-26 18:31:20,915 Epoch 816 Step:    90500 Batch Loss:     0.000008 Tokens per Sec:  9842231, Lr: 0.001000
2020-02-26 18:31:27,953 Epoch 816: total training loss 0.00118
2020-02-26 18:31:27,953 EPOCH 817
2020-02-26 18:31:38,155 Epoch 817: total training loss 0.00114
2020-02-26 18:31:38,156 EPOCH 818
2020-02-26 18:31:43,835 Epoch 818 Step:    90750 Batch Loss:     0.000007 Tokens per Sec: 10186712, Lr: 0.001000
2020-02-26 18:31:48,238 Epoch 818: total training loss 0.00115
2020-02-26 18:31:48,238 EPOCH 819
2020-02-26 18:31:58,507 Epoch 819: total training loss 0.00111
2020-02-26 18:31:58,507 EPOCH 820
2020-02-26 18:32:06,713 Epoch 820 Step:    91000 Batch Loss:     0.000011 Tokens per Sec: 10075433, Lr: 0.001000
2020-02-26 18:32:08,648 Epoch 820: total training loss 0.00112
2020-02-26 18:32:08,648 EPOCH 821
2020-02-26 18:32:18,731 Epoch 821: total training loss 0.00112
2020-02-26 18:32:18,732 EPOCH 822
2020-02-26 18:32:28,841 Epoch 822: total training loss 0.00112
2020-02-26 18:32:28,841 EPOCH 823
2020-02-26 18:32:29,543 Epoch 823 Step:    91250 Batch Loss:     0.000007 Tokens per Sec: 10193459, Lr: 0.001000
2020-02-26 18:32:38,851 Epoch 823: total training loss 0.00115
2020-02-26 18:32:38,851 EPOCH 824
2020-02-26 18:32:48,873 Epoch 824: total training loss 0.00111
2020-02-26 18:32:48,873 EPOCH 825
2020-02-26 18:32:52,209 Epoch 825 Step:    91500 Batch Loss:     0.000012 Tokens per Sec: 10145592, Lr: 0.001000
2020-02-26 18:32:58,942 Epoch 825: total training loss 0.00114
2020-02-26 18:32:58,943 EPOCH 826
2020-02-26 18:33:09,153 Epoch 826: total training loss 0.00114
2020-02-26 18:33:09,154 EPOCH 827
2020-02-26 18:33:15,201 Epoch 827 Step:    91750 Batch Loss:     0.000009 Tokens per Sec:  9692830, Lr: 0.001000
2020-02-26 18:33:19,698 Epoch 827: total training loss 0.00113
2020-02-26 18:33:19,699 EPOCH 828
2020-02-26 18:33:30,355 Epoch 828: total training loss 0.00111
2020-02-26 18:33:30,355 EPOCH 829
2020-02-26 18:33:39,183 Epoch 829 Step:    92000 Batch Loss:     0.000011 Tokens per Sec:  9856706, Lr: 0.001000
2020-02-26 18:33:40,850 Epoch 829: total training loss 0.00112
2020-02-26 18:33:40,850 EPOCH 830
2020-02-26 18:33:51,226 Epoch 830: total training loss 0.00112
2020-02-26 18:33:51,227 EPOCH 831
2020-02-26 18:34:01,381 Epoch 831: total training loss 0.00111
2020-02-26 18:34:01,382 EPOCH 832
2020-02-26 18:34:02,390 Epoch 832 Step:    92250 Batch Loss:     0.000012 Tokens per Sec: 10460269, Lr: 0.001000
2020-02-26 18:34:11,338 Epoch 832: total training loss 0.00117
2020-02-26 18:34:11,339 EPOCH 833
2020-02-26 18:34:21,366 Epoch 833: total training loss 0.00112
2020-02-26 18:34:21,366 EPOCH 834
2020-02-26 18:34:24,990 Epoch 834 Step:    92500 Batch Loss:     0.000008 Tokens per Sec: 10403284, Lr: 0.001000
2020-02-26 18:34:31,462 Epoch 834: total training loss 0.00110
2020-02-26 18:34:31,463 EPOCH 835
2020-02-26 18:34:41,382 Epoch 835: total training loss 0.00113
2020-02-26 18:34:41,382 EPOCH 836
2020-02-26 18:34:47,200 Epoch 836 Step:    92750 Batch Loss:     0.000016 Tokens per Sec: 10236644, Lr: 0.001000
2020-02-26 18:34:51,473 Epoch 836: total training loss 0.00113
2020-02-26 18:34:51,473 EPOCH 837
2020-02-26 18:35:01,722 Epoch 837: total training loss 0.00110
2020-02-26 18:35:01,723 EPOCH 838
2020-02-26 18:35:10,311 Epoch 838 Step:    93000 Batch Loss:     0.000010 Tokens per Sec: 10057826, Lr: 0.001000
2020-02-26 18:35:11,956 Epoch 838: total training loss 0.00114
2020-02-26 18:35:11,956 EPOCH 839
2020-02-26 18:35:22,204 Epoch 839: total training loss 0.00112
2020-02-26 18:35:22,205 EPOCH 840
2020-02-26 18:35:32,539 Epoch 840: total training loss 0.00111
2020-02-26 18:35:32,540 EPOCH 841
2020-02-26 18:35:33,519 Epoch 841 Step:    93250 Batch Loss:     0.000010 Tokens per Sec:  9980996, Lr: 0.001000
2020-02-26 18:35:42,572 Epoch 841: total training loss 0.00117
2020-02-26 18:35:42,572 EPOCH 842
2020-02-26 18:35:52,538 Epoch 842: total training loss 0.00110
2020-02-26 18:35:52,539 EPOCH 843
2020-02-26 18:35:55,968 Epoch 843 Step:    93500 Batch Loss:     0.000010 Tokens per Sec: 10143038, Lr: 0.001000
2020-02-26 18:36:02,531 Epoch 843: total training loss 0.00109
2020-02-26 18:36:02,531 EPOCH 844
2020-02-26 18:36:12,462 Epoch 844: total training loss 0.00109
2020-02-26 18:36:12,462 EPOCH 845
2020-02-26 18:36:18,194 Epoch 845 Step:    93750 Batch Loss:     0.000016 Tokens per Sec: 10259690, Lr: 0.001000
2020-02-26 18:36:22,460 Epoch 845: total training loss 0.00118
2020-02-26 18:36:22,461 EPOCH 846
2020-02-26 18:36:32,698 Epoch 846: total training loss 0.00110
2020-02-26 18:36:32,699 EPOCH 847
2020-02-26 18:36:41,579 Epoch 847 Step:    94000 Batch Loss:     0.000008 Tokens per Sec:  9677164, Lr: 0.001000
2020-02-26 18:36:43,235 Epoch 847: total training loss 0.00111
2020-02-26 18:36:43,235 EPOCH 848
2020-02-26 18:36:53,742 Epoch 848: total training loss 0.00114
2020-02-26 18:36:53,743 EPOCH 849
2020-02-26 18:37:04,362 Epoch 849: total training loss 0.00112
2020-02-26 18:37:04,362 EPOCH 850
2020-02-26 18:37:05,579 Epoch 850 Step:    94250 Batch Loss:     0.000012 Tokens per Sec:  9647503, Lr: 0.001000
2020-02-26 18:37:14,780 Epoch 850: total training loss 0.00108
2020-02-26 18:37:14,781 EPOCH 851
2020-02-26 18:37:24,893 Epoch 851: total training loss 0.00121
2020-02-26 18:37:24,901 EPOCH 852
2020-02-26 18:37:28,485 Epoch 852 Step:    94500 Batch Loss:     0.000008 Tokens per Sec: 10036075, Lr: 0.001000
2020-02-26 18:37:34,995 Epoch 852: total training loss 0.00116
2020-02-26 18:37:34,995 EPOCH 853
2020-02-26 18:37:45,058 Epoch 853: total training loss 0.00113
2020-02-26 18:37:45,058 EPOCH 854
2020-02-26 18:37:50,932 Epoch 854 Step:    94750 Batch Loss:     0.000009 Tokens per Sec: 10168104, Lr: 0.001000
2020-02-26 18:37:54,964 Epoch 854: total training loss 0.00109
2020-02-26 18:37:54,964 EPOCH 855
2020-02-26 18:38:04,706 Epoch 855: total training loss 0.00110
2020-02-26 18:38:04,707 EPOCH 856
2020-02-26 18:38:13,090 Epoch 856 Step:    95000 Batch Loss:     0.000010 Tokens per Sec: 10406299, Lr: 0.001000
2020-02-26 18:38:14,531 Epoch 856: total training loss 0.00108
2020-02-26 18:38:14,531 EPOCH 857
2020-02-26 18:38:24,685 Epoch 857: total training loss 0.00108
2020-02-26 18:38:24,685 EPOCH 858
2020-02-26 18:38:35,051 Epoch 858: total training loss 0.00107
2020-02-26 18:38:35,052 EPOCH 859
2020-02-26 18:38:36,228 Epoch 859 Step:    95250 Batch Loss:     0.000004 Tokens per Sec:  8842553, Lr: 0.001000
2020-02-26 18:38:45,558 Epoch 859: total training loss 0.00106
2020-02-26 18:38:45,565 EPOCH 860
2020-02-26 18:38:56,139 Epoch 860: total training loss 0.00109
2020-02-26 18:38:56,140 EPOCH 861
2020-02-26 18:38:59,888 Epoch 861 Step:    95500 Batch Loss:     0.000011 Tokens per Sec:  9519802, Lr: 0.001000
2020-02-26 18:39:06,785 Epoch 861: total training loss 0.00109
2020-02-26 18:39:06,786 EPOCH 862
2020-02-26 18:39:17,014 Epoch 862: total training loss 0.00113
2020-02-26 18:39:17,014 EPOCH 863
2020-02-26 18:39:23,407 Epoch 863 Step:    95750 Batch Loss:     0.000011 Tokens per Sec: 10027661, Lr: 0.001000
2020-02-26 18:39:27,277 Epoch 863: total training loss 0.00119
2020-02-26 18:39:27,278 EPOCH 864
2020-02-26 18:39:37,574 Epoch 864: total training loss 0.00109
2020-02-26 18:39:37,574 EPOCH 865
2020-02-26 18:39:46,503 Epoch 865 Step:    96000 Batch Loss:     0.000007 Tokens per Sec: 10020648, Lr: 0.001000
2020-02-26 18:39:47,847 Epoch 865: total training loss 0.00108
2020-02-26 18:39:47,848 EPOCH 866
2020-02-26 18:39:57,732 Epoch 866: total training loss 0.00111
2020-02-26 18:39:57,732 EPOCH 867
2020-02-26 18:40:07,427 Epoch 867: total training loss 0.00110
2020-02-26 18:40:07,427 EPOCH 868
2020-02-26 18:40:08,453 Epoch 868 Step:    96250 Batch Loss:     0.000013 Tokens per Sec: 10058764, Lr: 0.001000
2020-02-26 18:40:17,136 Epoch 868: total training loss 0.00110
2020-02-26 18:40:17,136 EPOCH 869
2020-02-26 18:40:26,900 Epoch 869: total training loss 0.00110
2020-02-26 18:40:26,900 EPOCH 870
2020-02-26 18:40:30,310 Epoch 870 Step:    96500 Batch Loss:     0.000013 Tokens per Sec: 10471907, Lr: 0.001000
2020-02-26 18:40:36,641 Epoch 870: total training loss 0.00109
2020-02-26 18:40:36,641 EPOCH 871
2020-02-26 18:40:46,651 Epoch 871: total training loss 0.00115
2020-02-26 18:40:46,652 EPOCH 872
2020-02-26 18:40:52,780 Epoch 872 Step:    96750 Batch Loss:     0.000011 Tokens per Sec: 10102633, Lr: 0.001000
2020-02-26 18:40:56,781 Epoch 872: total training loss 0.00111
2020-02-26 18:40:56,781 EPOCH 873
2020-02-26 18:41:07,091 Epoch 873: total training loss 0.00110
2020-02-26 18:41:07,092 EPOCH 874
2020-02-26 18:41:16,347 Epoch 874 Step:    97000 Batch Loss:     0.000014 Tokens per Sec:  9849021, Lr: 0.001000
2020-02-26 18:41:17,606 Epoch 874: total training loss 0.00110
2020-02-26 18:41:17,606 EPOCH 875
2020-02-26 18:41:28,066 Epoch 875: total training loss 0.00112
2020-02-26 18:41:28,067 EPOCH 876
2020-02-26 18:41:38,327 Epoch 876: total training loss 0.00108
2020-02-26 18:41:38,327 EPOCH 877
2020-02-26 18:41:39,526 Epoch 877 Step:    97250 Batch Loss:     0.000015 Tokens per Sec:  9947656, Lr: 0.001000
2020-02-26 18:41:48,415 Epoch 877: total training loss 0.00108
2020-02-26 18:41:48,416 EPOCH 878
2020-02-26 18:41:58,605 Epoch 878: total training loss 0.00106
2020-02-26 18:41:58,607 EPOCH 879
2020-02-26 18:42:02,466 Epoch 879 Step:    97500 Batch Loss:     0.000010 Tokens per Sec: 10051852, Lr: 0.001000
2020-02-26 18:42:08,894 Epoch 879: total training loss 0.00103
2020-02-26 18:42:08,894 EPOCH 880
2020-02-26 18:42:18,909 Epoch 880: total training loss 0.00105
2020-02-26 18:42:18,909 EPOCH 881
2020-02-26 18:42:24,801 Epoch 881 Step:    97750 Batch Loss:     0.000008 Tokens per Sec: 10304931, Lr: 0.001000
2020-02-26 18:42:28,773 Epoch 881: total training loss 0.00107
2020-02-26 18:42:28,773 EPOCH 882
2020-02-26 18:42:38,700 Epoch 882: total training loss 0.00125
2020-02-26 18:42:38,701 EPOCH 883
2020-02-26 18:42:47,571 Epoch 883 Step:    98000 Batch Loss:     0.000012 Tokens per Sec: 10245673, Lr: 0.001000
2020-02-26 18:42:48,776 Epoch 883: total training loss 0.00111
2020-02-26 18:42:48,776 EPOCH 884
2020-02-26 18:42:58,915 Epoch 884: total training loss 0.00107
2020-02-26 18:42:58,921 EPOCH 885
2020-02-26 18:43:09,392 Epoch 885: total training loss 0.00106
2020-02-26 18:43:09,393 EPOCH 886
2020-02-26 18:43:10,807 Epoch 886 Step:    98250 Batch Loss:     0.000012 Tokens per Sec:  9981481, Lr: 0.001000
2020-02-26 18:43:19,674 Epoch 886: total training loss 0.00109
2020-02-26 18:43:19,675 EPOCH 887
2020-02-26 18:43:29,683 Epoch 887: total training loss 0.00113
2020-02-26 18:43:29,683 EPOCH 888
2020-02-26 18:43:33,472 Epoch 888 Step:    98500 Batch Loss:     0.000018 Tokens per Sec: 10110199, Lr: 0.001000
2020-02-26 18:43:39,761 Epoch 888: total training loss 0.00116
2020-02-26 18:43:39,762 EPOCH 889
2020-02-26 18:43:49,930 Epoch 889: total training loss 0.00108
2020-02-26 18:43:49,930 EPOCH 890
2020-02-26 18:43:56,549 Epoch 890 Step:    98750 Batch Loss:     0.000014 Tokens per Sec: 10063550, Lr: 0.001000
2020-02-26 18:44:00,051 Epoch 890: total training loss 0.00106
2020-02-26 18:44:00,051 EPOCH 891
2020-02-26 18:44:10,222 Epoch 891: total training loss 0.00104
2020-02-26 18:44:10,222 EPOCH 892
2020-02-26 18:44:19,197 Epoch 892 Step:    99000 Batch Loss:     0.000011 Tokens per Sec: 10241965, Lr: 0.001000
2020-02-26 18:44:20,300 Epoch 892: total training loss 0.00105
2020-02-26 18:44:20,300 EPOCH 893
2020-02-26 18:44:30,721 Epoch 893: total training loss 0.00106
2020-02-26 18:44:30,721 EPOCH 894
2020-02-26 18:44:41,279 Epoch 894: total training loss 0.00115
2020-02-26 18:44:41,280 EPOCH 895
2020-02-26 18:44:42,759 Epoch 895 Step:    99250 Batch Loss:     0.000008 Tokens per Sec:  9863805, Lr: 0.001000
2020-02-26 18:44:51,533 Epoch 895: total training loss 0.00113
2020-02-26 18:44:51,533 EPOCH 896
2020-02-26 18:45:01,825 Epoch 896: total training loss 0.00105
2020-02-26 18:45:01,825 EPOCH 897
2020-02-26 18:45:05,866 Epoch 897 Step:    99500 Batch Loss:     0.000011 Tokens per Sec:  9790926, Lr: 0.001000
2020-02-26 18:45:12,189 Epoch 897: total training loss 0.00104
2020-02-26 18:45:12,189 EPOCH 898
2020-02-26 18:45:22,407 Epoch 898: total training loss 0.00105
2020-02-26 18:45:22,407 EPOCH 899
2020-02-26 18:45:28,711 Epoch 899 Step:    99750 Batch Loss:     0.000007 Tokens per Sec: 10218864, Lr: 0.001000
2020-02-26 18:45:32,433 Epoch 899: total training loss 0.00108
2020-02-26 18:45:32,434 EPOCH 900
2020-02-26 18:45:42,405 Epoch 900: total training loss 0.00109
2020-02-26 18:45:42,406 EPOCH 901
2020-02-26 18:45:51,360 Epoch 901 Step:   100000 Batch Loss:     0.000011 Tokens per Sec: 10218873, Lr: 0.001000
2020-02-26 18:45:51,361 Model noise rate: 5
2020-02-26 18:46:38,955 Hooray! New best validation result [dtw]!
2020-02-26 18:46:38,957 Saving new checkpoint.
2020-02-26 18:46:55,843 Validation result at epoch 901, step   100000: Val DTW Score:  10.60, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0355, GT DTW Score:      nan, duration: 64.4823s
2020-02-26 18:46:56,914 Epoch 901: total training loss 0.00107
2020-02-26 18:46:56,914 EPOCH 902
2020-02-26 18:47:06,989 Epoch 902: total training loss 0.00105
2020-02-26 18:47:06,989 EPOCH 903
2020-02-26 18:47:16,796 Epoch 903: total training loss 0.00103
2020-02-26 18:47:16,797 EPOCH 904
2020-02-26 18:47:18,230 Epoch 904 Step:   100250 Batch Loss:     0.000010 Tokens per Sec: 10235721, Lr: 0.001000
2020-02-26 18:47:26,695 Epoch 904: total training loss 0.00104
2020-02-26 18:47:26,696 EPOCH 905
2020-02-26 18:47:36,691 Epoch 905: total training loss 0.00104
2020-02-26 18:47:36,691 EPOCH 906
2020-02-26 18:47:40,993 Epoch 906 Step:   100500 Batch Loss:     0.000009 Tokens per Sec: 10333084, Lr: 0.001000
2020-02-26 18:47:46,702 Epoch 906: total training loss 0.00104
2020-02-26 18:47:46,704 EPOCH 907
2020-02-26 18:47:56,900 Epoch 907: total training loss 0.00106
2020-02-26 18:47:56,901 EPOCH 908
2020-02-26 18:48:03,699 Epoch 908 Step:   100750 Batch Loss:     0.000014 Tokens per Sec:  9956154, Lr: 0.001000
2020-02-26 18:48:07,153 Epoch 908: total training loss 0.00104
2020-02-26 18:48:07,154 EPOCH 909
2020-02-26 18:48:17,212 Epoch 909: total training loss 0.00105
2020-02-26 18:48:17,213 EPOCH 910
2020-02-26 18:48:26,303 Epoch 910 Step:   101000 Batch Loss:     0.000013 Tokens per Sec: 10353074, Lr: 0.001000
2020-02-26 18:48:27,157 Epoch 910: total training loss 0.00106
2020-02-26 18:48:27,158 EPOCH 911
2020-02-26 18:48:37,018 Epoch 911: total training loss 0.00106
2020-02-26 18:48:37,019 EPOCH 912
2020-02-26 18:48:46,905 Epoch 912: total training loss 0.00120
2020-02-26 18:48:46,906 EPOCH 913
2020-02-26 18:48:48,531 Epoch 913 Step:   101250 Batch Loss:     0.000009 Tokens per Sec: 10124183, Lr: 0.001000
2020-02-26 18:48:56,845 Epoch 913: total training loss 0.00109
2020-02-26 18:48:56,846 EPOCH 914
2020-02-26 18:49:06,963 Epoch 914: total training loss 0.00102
2020-02-26 18:49:06,965 EPOCH 915
2020-02-26 18:49:11,213 Epoch 915 Step:   101500 Batch Loss:     0.000010 Tokens per Sec: 10319599, Lr: 0.001000
2020-02-26 18:49:16,949 Epoch 915: total training loss 0.00102
2020-02-26 18:49:16,950 EPOCH 916
2020-02-26 18:49:27,047 Epoch 916: total training loss 0.00107
2020-02-26 18:49:27,049 EPOCH 917
2020-02-26 18:49:33,854 Epoch 917 Step:   101750 Batch Loss:     0.000012 Tokens per Sec: 10043088, Lr: 0.001000
2020-02-26 18:49:37,403 Epoch 917: total training loss 0.00103
2020-02-26 18:49:37,404 EPOCH 918
2020-02-26 18:49:47,992 Epoch 918: total training loss 0.00105
2020-02-26 18:49:47,993 EPOCH 919
2020-02-26 18:49:57,643 Epoch 919 Step:   102000 Batch Loss:     0.000007 Tokens per Sec:  9846274, Lr: 0.001000
2020-02-26 18:49:58,481 Epoch 919: total training loss 0.00104
2020-02-26 18:49:58,482 EPOCH 920
2020-02-26 18:50:08,794 Epoch 920: total training loss 0.00104
2020-02-26 18:50:08,794 EPOCH 921
2020-02-26 18:50:19,004 Epoch 921: total training loss 0.00104
2020-02-26 18:50:19,004 EPOCH 922
2020-02-26 18:50:20,801 Epoch 922 Step:   102250 Batch Loss:     0.000006 Tokens per Sec: 10470208, Lr: 0.001000
2020-02-26 18:50:29,134 Epoch 922: total training loss 0.00104
2020-02-26 18:50:29,135 EPOCH 923
2020-02-26 18:50:39,074 Epoch 923: total training loss 0.00105
2020-02-26 18:50:39,075 EPOCH 924
2020-02-26 18:50:43,434 Epoch 924 Step:   102500 Batch Loss:     0.000016 Tokens per Sec: 10297668, Lr: 0.001000
2020-02-26 18:50:49,089 Epoch 924: total training loss 0.00105
2020-02-26 18:50:49,089 EPOCH 925
2020-02-26 18:50:59,109 Epoch 925: total training loss 0.00101
2020-02-26 18:50:59,110 EPOCH 926
2020-02-26 18:51:05,641 Epoch 926 Step:   102750 Batch Loss:     0.000016 Tokens per Sec: 10196406, Lr: 0.001000
2020-02-26 18:51:09,057 Epoch 926: total training loss 0.00106
2020-02-26 18:51:09,058 EPOCH 927
2020-02-26 18:51:19,195 Epoch 927: total training loss 0.00103
2020-02-26 18:51:19,195 EPOCH 928
2020-02-26 18:51:28,709 Epoch 928 Step:   103000 Batch Loss:     0.000010 Tokens per Sec: 10063286, Lr: 0.001000
2020-02-26 18:51:29,384 Epoch 928: total training loss 0.00104
2020-02-26 18:51:29,384 EPOCH 929
2020-02-26 18:51:39,586 Epoch 929: total training loss 0.00106
2020-02-26 18:51:39,587 EPOCH 930
2020-02-26 18:51:49,993 Epoch 930: total training loss 0.00103
2020-02-26 18:51:49,994 EPOCH 931
2020-02-26 18:51:52,229 Epoch 931 Step:   103250 Batch Loss:     0.000015 Tokens per Sec:  9757837, Lr: 0.001000
2020-02-26 18:52:00,526 Epoch 931: total training loss 0.00103
2020-02-26 18:52:00,526 EPOCH 932
2020-02-26 18:52:10,611 Epoch 932: total training loss 0.00107
2020-02-26 18:52:10,612 EPOCH 933
2020-02-26 18:52:14,903 Epoch 933 Step:   103500 Batch Loss:     0.000010 Tokens per Sec:  9912816, Lr: 0.001000
2020-02-26 18:52:20,819 Epoch 933: total training loss 0.00129
2020-02-26 18:52:20,819 EPOCH 934
2020-02-26 18:52:31,035 Epoch 934: total training loss 0.00106
2020-02-26 18:52:31,035 EPOCH 935
2020-02-26 18:52:37,868 Epoch 935 Step:   103750 Batch Loss:     0.000006 Tokens per Sec: 10230309, Lr: 0.001000
2020-02-26 18:52:41,042 Epoch 935: total training loss 0.00103
2020-02-26 18:52:41,043 EPOCH 936
2020-02-26 18:52:50,931 Epoch 936: total training loss 0.00102
2020-02-26 18:52:50,931 EPOCH 937
2020-02-26 18:53:00,260 Epoch 937 Step:   104000 Batch Loss:     0.000009 Tokens per Sec: 10262526, Lr: 0.001000
2020-02-26 18:53:00,943 Epoch 937: total training loss 0.00101
2020-02-26 18:53:00,944 EPOCH 938
2020-02-26 18:53:11,273 Epoch 938: total training loss 0.00102
2020-02-26 18:53:11,274 EPOCH 939
2020-02-26 18:53:21,575 Epoch 939: total training loss 0.00101
2020-02-26 18:53:21,575 EPOCH 940
2020-02-26 18:53:23,593 Epoch 940 Step:   104250 Batch Loss:     0.000012 Tokens per Sec: 10133133, Lr: 0.001000
2020-02-26 18:53:31,778 Epoch 940: total training loss 0.00111
2020-02-26 18:53:31,779 EPOCH 941
2020-02-26 18:53:42,097 Epoch 941: total training loss 0.00110
2020-02-26 18:53:42,098 EPOCH 942
2020-02-26 18:53:46,438 Epoch 942 Step:   104500 Batch Loss:     0.000006 Tokens per Sec: 10007191, Lr: 0.001000
2020-02-26 18:53:52,231 Epoch 942: total training loss 0.00104
2020-02-26 18:53:52,232 EPOCH 943
2020-02-26 18:54:02,380 Epoch 943: total training loss 0.00101
2020-02-26 18:54:02,381 EPOCH 944
2020-02-26 18:54:09,412 Epoch 944 Step:   104750 Batch Loss:     0.000007 Tokens per Sec: 10114462, Lr: 0.001000
2020-02-26 18:54:12,569 Epoch 944: total training loss 0.00103
2020-02-26 18:54:12,570 EPOCH 945
2020-02-26 18:54:22,695 Epoch 945: total training loss 0.00103
2020-02-26 18:54:22,696 EPOCH 946
2020-02-26 18:54:32,198 Epoch 946 Step:   105000 Batch Loss:     0.000009 Tokens per Sec: 10317575, Lr: 0.001000
2020-02-26 18:54:32,661 Epoch 946: total training loss 0.00104
2020-02-26 18:54:32,662 EPOCH 947
2020-02-26 18:54:42,681 Epoch 947: total training loss 0.00103
2020-02-26 18:54:42,682 EPOCH 948
2020-02-26 18:54:53,188 Epoch 948: total training loss 0.00101
2020-02-26 18:54:53,189 EPOCH 949
2020-02-26 18:54:55,352 Epoch 949 Step:   105250 Batch Loss:     0.000012 Tokens per Sec:  9425732, Lr: 0.001000
2020-02-26 18:55:03,997 Epoch 949: total training loss 0.00103
2020-02-26 18:55:03,998 EPOCH 950
2020-02-26 18:55:14,229 Epoch 950: total training loss 0.00103
2020-02-26 18:55:14,230 EPOCH 951
2020-02-26 18:55:19,020 Epoch 951 Step:   105500 Batch Loss:     0.000009 Tokens per Sec:  9999745, Lr: 0.001000
2020-02-26 18:55:24,512 Epoch 951: total training loss 0.00101
2020-02-26 18:55:24,512 EPOCH 952
2020-02-26 18:55:34,807 Epoch 952: total training loss 0.00105
2020-02-26 18:55:34,808 EPOCH 953
2020-02-26 18:55:41,913 Epoch 953 Step:   105750 Batch Loss:     0.000009 Tokens per Sec: 10262265, Lr: 0.001000
2020-02-26 18:55:44,914 Epoch 953: total training loss 0.00103
2020-02-26 18:55:44,914 EPOCH 954
2020-02-26 18:55:55,018 Epoch 954: total training loss 0.00102
2020-02-26 18:55:55,019 EPOCH 955
2020-02-26 18:56:04,874 Epoch 955 Step:   106000 Batch Loss:     0.000012 Tokens per Sec: 10146342, Lr: 0.001000
2020-02-26 18:56:05,262 Epoch 955: total training loss 0.00100
2020-02-26 18:56:05,263 EPOCH 956
2020-02-26 18:56:15,243 Epoch 956: total training loss 0.00101
2020-02-26 18:56:15,243 EPOCH 957
2020-02-26 18:56:25,161 Epoch 957: total training loss 0.00101
2020-02-26 18:56:25,162 EPOCH 958
2020-02-26 18:56:27,427 Epoch 958 Step:   106250 Batch Loss:     0.000010 Tokens per Sec:  9915864, Lr: 0.001000
2020-02-26 18:56:35,603 Epoch 958: total training loss 0.00103
2020-02-26 18:56:35,604 EPOCH 959
2020-02-26 18:56:45,812 Epoch 959: total training loss 0.00108
2020-02-26 18:56:45,813 EPOCH 960
2020-02-26 18:56:50,583 Epoch 960 Step:   106500 Batch Loss:     0.000011 Tokens per Sec: 10008009, Lr: 0.001000
2020-02-26 18:56:56,002 Epoch 960: total training loss 0.00106
2020-02-26 18:56:56,003 EPOCH 961
2020-02-26 18:57:06,371 Epoch 961: total training loss 0.00103
2020-02-26 18:57:06,372 EPOCH 962
2020-02-26 18:57:13,723 Epoch 962 Step:   106750 Batch Loss:     0.000007 Tokens per Sec:  9854854, Lr: 0.001000
2020-02-26 18:57:16,704 Epoch 962: total training loss 0.00109
2020-02-26 18:57:16,705 EPOCH 963
2020-02-26 18:57:26,655 Epoch 963: total training loss 0.00109
2020-02-26 18:57:26,655 EPOCH 964
2020-02-26 18:57:36,355 Epoch 964 Step:   107000 Batch Loss:     0.000014 Tokens per Sec: 10292305, Lr: 0.001000
2020-02-26 18:57:36,712 Epoch 964: total training loss 0.00102
2020-02-26 18:57:36,712 EPOCH 965
2020-02-26 18:57:46,642 Epoch 965: total training loss 0.00101
2020-02-26 18:57:46,643 EPOCH 966
2020-02-26 18:57:56,724 Epoch 966: total training loss 0.00100
2020-02-26 18:57:56,726 EPOCH 967
2020-02-26 18:57:58,952 Epoch 967 Step:   107250 Batch Loss:     0.000012 Tokens per Sec:  9768589, Lr: 0.001000
2020-02-26 18:58:07,023 Epoch 967: total training loss 0.00099
2020-02-26 18:58:07,024 EPOCH 968
2020-02-26 18:58:17,316 Epoch 968: total training loss 0.00099
2020-02-26 18:58:17,316 EPOCH 969
2020-02-26 18:58:22,233 Epoch 969 Step:   107500 Batch Loss:     0.000008 Tokens per Sec: 10249502, Lr: 0.001000
2020-02-26 18:58:27,508 Epoch 969: total training loss 0.00100
2020-02-26 18:58:27,509 EPOCH 970
2020-02-26 18:58:37,564 Epoch 970: total training loss 0.00102
2020-02-26 18:58:37,565 EPOCH 971
2020-02-26 18:58:44,546 Epoch 971 Step:   107750 Batch Loss:     0.000007 Tokens per Sec: 10485737, Lr: 0.001000
2020-02-26 18:58:47,248 Epoch 971: total training loss 0.00104
2020-02-26 18:58:47,248 EPOCH 972
2020-02-26 18:58:57,117 Epoch 972: total training loss 0.00098
2020-02-26 18:58:57,117 EPOCH 973
2020-02-26 18:59:06,738 Epoch 973 Step:   108000 Batch Loss:     0.000009 Tokens per Sec: 10439692, Lr: 0.001000
2020-02-26 18:59:06,991 Epoch 973: total training loss 0.00099
2020-02-26 18:59:06,992 EPOCH 974
2020-02-26 18:59:16,958 Epoch 974: total training loss 0.00112
2020-02-26 18:59:16,959 EPOCH 975
2020-02-26 18:59:26,937 Epoch 975: total training loss 0.00103
2020-02-26 18:59:26,958 EPOCH 976
2020-02-26 18:59:29,371 Epoch 976 Step:   108250 Batch Loss:     0.000007 Tokens per Sec: 10036888, Lr: 0.001000
2020-02-26 18:59:37,065 Epoch 976: total training loss 0.00101
2020-02-26 18:59:37,066 EPOCH 977
2020-02-26 18:59:47,407 Epoch 977: total training loss 0.00100
2020-02-26 18:59:47,408 EPOCH 978
2020-02-26 18:59:52,510 Epoch 978 Step:   108500 Batch Loss:     0.000011 Tokens per Sec:  9717268, Lr: 0.001000
2020-02-26 18:59:57,916 Epoch 978: total training loss 0.00100
2020-02-26 18:59:57,917 EPOCH 979
2020-02-26 19:00:08,200 Epoch 979: total training loss 0.00100
2020-02-26 19:00:08,200 EPOCH 980
2020-02-26 19:00:15,762 Epoch 980 Step:   108750 Batch Loss:     0.000012 Tokens per Sec:  9887500, Lr: 0.001000
2020-02-26 19:00:18,655 Epoch 980: total training loss 0.00111
2020-02-26 19:00:18,656 EPOCH 981
2020-02-26 19:00:28,696 Epoch 981: total training loss 0.00108
2020-02-26 19:00:28,696 EPOCH 982
2020-02-26 19:00:38,443 Epoch 982 Step:   109000 Batch Loss:     0.000011 Tokens per Sec: 10367400, Lr: 0.001000
2020-02-26 19:00:38,620 Epoch 982: total training loss 0.00101
2020-02-26 19:00:38,620 EPOCH 983
2020-02-26 19:00:48,651 Epoch 983: total training loss 0.00100
2020-02-26 19:00:48,651 EPOCH 984
2020-02-26 19:00:58,746 Epoch 984: total training loss 0.00099
2020-02-26 19:00:58,747 EPOCH 985
2020-02-26 19:01:00,961 Epoch 985 Step:   109250 Batch Loss:     0.000005 Tokens per Sec:  9797022, Lr: 0.001000
2020-02-26 19:01:08,818 Epoch 985: total training loss 0.00101
2020-02-26 19:01:08,818 EPOCH 986
2020-02-26 19:01:18,904 Epoch 986: total training loss 0.00100
2020-02-26 19:01:18,905 EPOCH 987
2020-02-26 19:01:23,599 Epoch 987 Step:   109500 Batch Loss:     0.000007 Tokens per Sec: 10117968, Lr: 0.001000
2020-02-26 19:01:28,955 Epoch 987: total training loss 0.00098
2020-02-26 19:01:28,956 EPOCH 988
2020-02-26 19:01:38,998 Epoch 988: total training loss 0.00100
2020-02-26 19:01:38,999 EPOCH 989
2020-02-26 19:01:46,403 Epoch 989 Step:   109750 Batch Loss:     0.000007 Tokens per Sec: 10406210, Lr: 0.001000
2020-02-26 19:01:48,912 Epoch 989: total training loss 0.00098
2020-02-26 19:01:48,912 EPOCH 990
2020-02-26 19:01:58,850 Epoch 990: total training loss 0.00113
2020-02-26 19:01:58,851 EPOCH 991
2020-02-26 19:02:08,824 Epoch 991 Step:   110000 Batch Loss:     0.000007 Tokens per Sec: 10243148, Lr: 0.001000
2020-02-26 19:02:08,826 Model noise rate: 5
2020-02-26 19:03:12,009 Validation result at epoch 991, step   110000: Val DTW Score:  10.66, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0358, GT DTW Score:      nan, duration: 63.1830s
2020-02-26 19:03:12,102 Epoch 991: total training loss 0.00102
2020-02-26 19:03:12,103 EPOCH 992
2020-02-26 19:03:22,193 Epoch 992: total training loss 0.00100
2020-02-26 19:03:22,194 EPOCH 993
2020-02-26 19:03:32,203 Epoch 993: total training loss 0.00101
2020-02-26 19:03:32,203 EPOCH 994
2020-02-26 19:03:34,452 Epoch 994 Step:   110250 Batch Loss:     0.000009 Tokens per Sec: 10103820, Lr: 0.001000
2020-02-26 19:03:42,117 Epoch 994: total training loss 0.00098
2020-02-26 19:03:42,119 EPOCH 995
2020-02-26 19:03:52,275 Epoch 995: total training loss 0.00096
2020-02-26 19:03:52,276 EPOCH 996
2020-02-26 19:03:57,528 Epoch 996 Step:   110500 Batch Loss:     0.000009 Tokens per Sec: 10036127, Lr: 0.001000
2020-02-26 19:04:02,482 Epoch 996: total training loss 0.00096
2020-02-26 19:04:02,482 EPOCH 997
2020-02-26 19:04:12,715 Epoch 997: total training loss 0.00098
2020-02-26 19:04:12,715 EPOCH 998
2020-02-26 19:04:20,204 Epoch 998 Step:   110750 Batch Loss:     0.000006 Tokens per Sec: 10006833, Lr: 0.001000
2020-02-26 19:04:23,080 Epoch 998: total training loss 0.00097
2020-02-26 19:04:23,081 EPOCH 999
2020-02-26 19:04:33,512 Epoch 999: total training loss 0.00098
2020-02-26 19:04:33,512 EPOCH 1000
2020-02-26 19:04:43,688 Epoch 1000 Step:   111000 Batch Loss:     0.000007 Tokens per Sec: 10110112, Lr: 0.001000
2020-02-26 19:04:43,689 Epoch 1000: total training loss 0.00100
2020-02-26 19:04:43,689 EPOCH 1001
2020-02-26 19:04:53,684 Epoch 1001: total training loss 0.00102
2020-02-26 19:04:53,685 EPOCH 1002
2020-02-26 19:05:03,721 Epoch 1002: total training loss 0.00099
2020-02-26 19:05:03,722 EPOCH 1003
2020-02-26 19:05:06,355 Epoch 1003 Step:   111250 Batch Loss:     0.000005 Tokens per Sec: 10337037, Lr: 0.001000
2020-02-26 19:05:13,583 Epoch 1003: total training loss 0.00097
2020-02-26 19:05:13,583 EPOCH 1004
2020-02-26 19:05:23,591 Epoch 1004: total training loss 0.00099
2020-02-26 19:05:23,592 EPOCH 1005
2020-02-26 19:05:28,823 Epoch 1005 Step:   111500 Batch Loss:     0.000005 Tokens per Sec: 10250441, Lr: 0.001000
2020-02-26 19:05:33,584 Epoch 1005: total training loss 0.00100
2020-02-26 19:05:33,585 EPOCH 1006
2020-02-26 19:05:43,961 Epoch 1006: total training loss 0.00099
2020-02-26 19:05:43,961 EPOCH 1007
2020-02-26 19:05:51,696 Epoch 1007 Step:   111750 Batch Loss:     0.000013 Tokens per Sec: 10015932, Lr: 0.001000
2020-02-26 19:05:54,214 Epoch 1007: total training loss 0.00101
2020-02-26 19:05:54,214 EPOCH 1008
2020-02-26 19:06:04,515 Epoch 1008: total training loss 0.00099
2020-02-26 19:06:04,516 EPOCH 1009
2020-02-26 19:06:14,759 Epoch 1009: total training loss 0.00101
2020-02-26 19:06:14,760 EPOCH 1010
2020-02-26 19:06:14,866 Epoch 1010 Step:   112000 Batch Loss:     0.000008 Tokens per Sec:  8001680, Lr: 0.001000
2020-02-26 19:06:24,790 Epoch 1010: total training loss 0.00103
2020-02-26 19:06:24,791 EPOCH 1011
2020-02-26 19:06:34,904 Epoch 1011: total training loss 0.00104
2020-02-26 19:06:34,904 EPOCH 1012
2020-02-26 19:06:37,682 Epoch 1012 Step:   112250 Batch Loss:     0.000007 Tokens per Sec: 10432900, Lr: 0.001000
2020-02-26 19:06:44,953 Epoch 1012: total training loss 0.00098
2020-02-26 19:06:44,954 EPOCH 1013
2020-02-26 19:06:55,098 Epoch 1013: total training loss 0.00097
2020-02-26 19:06:55,098 EPOCH 1014
2020-02-26 19:07:00,102 Epoch 1014 Step:   112500 Batch Loss:     0.000006 Tokens per Sec: 10164753, Lr: 0.001000
2020-02-26 19:07:05,100 Epoch 1014: total training loss 0.00098
2020-02-26 19:07:05,101 EPOCH 1015
2020-02-26 19:07:15,076 Epoch 1015: total training loss 0.00097
2020-02-26 19:07:15,077 EPOCH 1016
2020-02-26 19:07:22,765 Epoch 1016 Step:   112750 Batch Loss:     0.000009 Tokens per Sec: 10009642, Lr: 0.001000
2020-02-26 19:07:25,293 Epoch 1016: total training loss 0.00104
2020-02-26 19:07:25,293 EPOCH 1017
2020-02-26 19:07:35,692 Epoch 1017: total training loss 0.00106
2020-02-26 19:07:35,694 EPOCH 1018
2020-02-26 19:07:46,210 Epoch 1018: total training loss 0.00097
2020-02-26 19:07:46,211 EPOCH 1019
2020-02-26 19:07:46,399 Epoch 1019 Step:   113000 Batch Loss:     0.000008 Tokens per Sec:  7827057, Lr: 0.001000
2020-02-26 19:07:56,843 Epoch 1019: total training loss 0.00098
2020-02-26 19:07:56,843 EPOCH 1020
2020-02-26 19:08:07,212 Epoch 1020: total training loss 0.00096
2020-02-26 19:08:07,213 EPOCH 1021
2020-02-26 19:08:10,055 Epoch 1021 Step:   113250 Batch Loss:     0.000007 Tokens per Sec: 10318570, Lr: 0.001000
2020-02-26 19:08:17,255 Epoch 1021: total training loss 0.00097
2020-02-26 19:08:17,256 EPOCH 1022
2020-02-26 19:08:27,222 Epoch 1022: total training loss 0.00099
2020-02-26 19:08:27,222 EPOCH 1023
2020-02-26 19:08:32,425 Epoch 1023 Step:   113500 Batch Loss:     0.000010 Tokens per Sec: 10545720, Lr: 0.001000
2020-02-26 19:08:37,084 Epoch 1023: total training loss 0.00104
2020-02-26 19:08:37,085 EPOCH 1024
2020-02-26 19:08:46,790 Epoch 1024: total training loss 0.00101
2020-02-26 19:08:46,791 EPOCH 1025
2020-02-26 19:08:54,500 Epoch 1025 Step:   113750 Batch Loss:     0.000004 Tokens per Sec: 10319300, Lr: 0.001000
2020-02-26 19:08:56,746 Epoch 1025: total training loss 0.00098
2020-02-26 19:08:56,746 EPOCH 1026
2020-02-26 19:09:06,916 Epoch 1026: total training loss 0.00105
2020-02-26 19:09:06,917 EPOCH 1027
2020-02-26 19:09:17,055 Epoch 1027: total training loss 0.00098
2020-02-26 19:09:17,055 EPOCH 1028
2020-02-26 19:09:17,340 Epoch 1028 Step:   114000 Batch Loss:     0.000006 Tokens per Sec:  8274690, Lr: 0.001000
2020-02-26 19:09:27,289 Epoch 1028: total training loss 0.00098
2020-02-26 19:09:27,292 EPOCH 1029
2020-02-26 19:09:37,639 Epoch 1029: total training loss 0.00097
2020-02-26 19:09:37,641 EPOCH 1030
2020-02-26 19:09:40,564 Epoch 1030 Step:   114250 Batch Loss:     0.000009 Tokens per Sec: 10410067, Lr: 0.001000
2020-02-26 19:09:47,890 Epoch 1030: total training loss 0.00096
2020-02-26 19:09:47,890 EPOCH 1031
2020-02-26 19:09:57,913 Epoch 1031: total training loss 0.00098
2020-02-26 19:09:57,915 EPOCH 1032
2020-02-26 19:10:03,298 Epoch 1032 Step:   114500 Batch Loss:     0.000013 Tokens per Sec: 10043105, Lr: 0.001000
2020-02-26 19:10:08,061 Epoch 1032: total training loss 0.00098
2020-02-26 19:10:08,063 EPOCH 1033
2020-02-26 19:10:18,201 Epoch 1033: total training loss 0.00102
2020-02-26 19:10:18,201 EPOCH 1034
2020-02-26 19:10:26,204 Epoch 1034 Step:   114750 Batch Loss:     0.000011 Tokens per Sec: 10259008, Lr: 0.001000
2020-02-26 19:10:28,257 Epoch 1034: total training loss 0.00103
2020-02-26 19:10:28,258 EPOCH 1035
2020-02-26 19:10:38,616 Epoch 1035: total training loss 0.00097
2020-02-26 19:10:38,617 EPOCH 1036
2020-02-26 19:10:49,108 Epoch 1036: total training loss 0.00098
2020-02-26 19:10:49,108 EPOCH 1037
2020-02-26 19:10:49,471 Epoch 1037 Step:   115000 Batch Loss:     0.000008 Tokens per Sec:  9284586, Lr: 0.001000
2020-02-26 19:10:59,290 Epoch 1037: total training loss 0.00099
2020-02-26 19:10:59,291 EPOCH 1038
2020-02-26 19:11:09,475 Epoch 1038: total training loss 0.00104
2020-02-26 19:11:09,476 EPOCH 1039
2020-02-26 19:11:12,551 Epoch 1039 Step:   115250 Batch Loss:     0.000015 Tokens per Sec:  9802553, Lr: 0.001000
2020-02-26 19:11:19,788 Epoch 1039: total training loss 0.00098
2020-02-26 19:11:19,788 EPOCH 1040
2020-02-26 19:11:29,897 Epoch 1040: total training loss 0.00095
2020-02-26 19:11:29,897 EPOCH 1041
2020-02-26 19:11:35,097 Epoch 1041 Step:   115500 Batch Loss:     0.000009 Tokens per Sec: 10227031, Lr: 0.001000
2020-02-26 19:11:39,903 Epoch 1041: total training loss 0.00096
2020-02-26 19:11:39,904 EPOCH 1042
2020-02-26 19:11:49,775 Epoch 1042: total training loss 0.00103
2020-02-26 19:11:49,775 EPOCH 1043
2020-02-26 19:11:57,601 Epoch 1043 Step:   115750 Batch Loss:     0.000006 Tokens per Sec: 10398458, Lr: 0.001000
2020-02-26 19:11:59,654 Epoch 1043: total training loss 0.00100
2020-02-26 19:11:59,655 EPOCH 1044
2020-02-26 19:12:09,857 Epoch 1044: total training loss 0.00096
2020-02-26 19:12:09,857 EPOCH 1045
2020-02-26 19:12:20,093 Epoch 1045: total training loss 0.00097
2020-02-26 19:12:20,093 EPOCH 1046
2020-02-26 19:12:20,714 Epoch 1046 Step:   116000 Batch Loss:     0.000014 Tokens per Sec: 10128951, Lr: 0.001000
2020-02-26 19:12:30,263 Epoch 1046: total training loss 0.00096
2020-02-26 19:12:30,264 EPOCH 1047
2020-02-26 19:12:40,851 Epoch 1047: total training loss 0.00100
2020-02-26 19:12:40,852 EPOCH 1048
2020-02-26 19:12:44,000 Epoch 1048 Step:   116250 Batch Loss:     0.000012 Tokens per Sec:  9824911, Lr: 0.001000
2020-02-26 19:12:51,238 Epoch 1048: total training loss 0.00098
2020-02-26 19:12:51,239 EPOCH 1049
2020-02-26 19:13:01,381 Epoch 1049: total training loss 0.00095
2020-02-26 19:13:01,381 EPOCH 1050
2020-02-26 19:13:06,875 Epoch 1050 Step:   116500 Batch Loss:     0.000007 Tokens per Sec: 10257937, Lr: 0.001000
2020-02-26 19:13:11,472 Epoch 1050: total training loss 0.00096
2020-02-26 19:13:11,473 EPOCH 1051
2020-02-26 19:13:21,569 Epoch 1051: total training loss 0.00100
2020-02-26 19:13:21,569 EPOCH 1052
2020-02-26 19:13:29,697 Epoch 1052 Step:   116750 Batch Loss:     0.000010 Tokens per Sec: 10223178, Lr: 0.001000
2020-02-26 19:13:31,616 Epoch 1052: total training loss 0.00098
2020-02-26 19:13:31,617 EPOCH 1053
2020-02-26 19:13:41,554 Epoch 1053: total training loss 0.00098
2020-02-26 19:13:41,555 EPOCH 1054
2020-02-26 19:13:51,612 Epoch 1054: total training loss 0.00095
2020-02-26 19:13:51,613 EPOCH 1055
2020-02-26 19:13:52,125 Epoch 1055 Step:   117000 Batch Loss:     0.000006 Tokens per Sec:  9698445, Lr: 0.001000
2020-02-26 19:14:01,899 Epoch 1055: total training loss 0.00103
2020-02-26 19:14:01,900 EPOCH 1056
2020-02-26 19:14:12,175 Epoch 1056: total training loss 0.00099
2020-02-26 19:14:12,176 EPOCH 1057
2020-02-26 19:14:15,287 Epoch 1057 Step:   117250 Batch Loss:     0.000008 Tokens per Sec: 10163617, Lr: 0.001000
2020-02-26 19:14:22,334 Epoch 1057: total training loss 0.00097
2020-02-26 19:14:22,334 EPOCH 1058
2020-02-26 19:14:32,557 Epoch 1058: total training loss 0.00105
2020-02-26 19:14:32,557 EPOCH 1059
2020-02-26 19:14:37,854 Epoch 1059 Step:   117500 Batch Loss:     0.000013 Tokens per Sec: 10150686, Lr: 0.001000
2020-02-26 19:14:42,513 Epoch 1059: total training loss 0.00099
2020-02-26 19:14:42,513 EPOCH 1060
2020-02-26 19:14:52,620 Epoch 1060: total training loss 0.00093
2020-02-26 19:14:52,620 EPOCH 1061
2020-02-26 19:15:00,748 Epoch 1061 Step:   117750 Batch Loss:     0.000005 Tokens per Sec: 10367772, Lr: 0.001000
2020-02-26 19:15:02,573 Epoch 1061: total training loss 0.00096
2020-02-26 19:15:02,573 EPOCH 1062
2020-02-26 19:15:12,657 Epoch 1062: total training loss 0.00095
2020-02-26 19:15:12,659 EPOCH 1063
2020-02-26 19:15:22,647 Epoch 1063: total training loss 0.00096
2020-02-26 19:15:22,648 EPOCH 1064
2020-02-26 19:15:23,269 Epoch 1064 Step:   118000 Batch Loss:     0.000010 Tokens per Sec:  9850180, Lr: 0.001000
2020-02-26 19:15:32,726 Epoch 1064: total training loss 0.00098
2020-02-26 19:15:32,728 EPOCH 1065
2020-02-26 19:15:43,107 Epoch 1065: total training loss 0.00097
2020-02-26 19:15:43,112 EPOCH 1066
2020-02-26 19:15:46,377 Epoch 1066 Step:   118250 Batch Loss:     0.000007 Tokens per Sec:  9708147, Lr: 0.001000
2020-02-26 19:15:53,506 Epoch 1066: total training loss 0.00096
2020-02-26 19:15:53,507 EPOCH 1067
2020-02-26 19:16:03,810 Epoch 1067: total training loss 0.00097
2020-02-26 19:16:03,810 EPOCH 1068
2020-02-26 19:16:09,426 Epoch 1068 Step:   118500 Batch Loss:     0.000013 Tokens per Sec:  9880086, Lr: 0.001000
2020-02-26 19:16:14,022 Epoch 1068: total training loss 0.00094
2020-02-26 19:16:14,023 EPOCH 1069
2020-02-26 19:16:24,177 Epoch 1069: total training loss 0.00096
2020-02-26 19:16:24,178 EPOCH 1070
2020-02-26 19:16:32,517 Epoch 1070 Step:   118750 Batch Loss:     0.000006 Tokens per Sec: 10308481, Lr: 0.001000
2020-02-26 19:16:34,202 Epoch 1070: total training loss 0.00100
2020-02-26 19:16:34,202 EPOCH 1071
2020-02-26 19:16:44,208 Epoch 1071: total training loss 0.00097
2020-02-26 19:16:44,208 EPOCH 1072
2020-02-26 19:16:54,394 Epoch 1072: total training loss 0.00098
2020-02-26 19:16:54,394 EPOCH 1073
2020-02-26 19:16:55,107 Epoch 1073 Step:   119000 Batch Loss:     0.000010 Tokens per Sec:  9739478, Lr: 0.001000
2020-02-26 19:17:04,517 Epoch 1073: total training loss 0.00097
2020-02-26 19:17:04,518 EPOCH 1074
2020-02-26 19:17:14,512 Epoch 1074: total training loss 0.00095
2020-02-26 19:17:14,513 EPOCH 1075
2020-02-26 19:17:17,669 Epoch 1075 Step:   119250 Batch Loss:     0.000012 Tokens per Sec: 10229504, Lr: 0.001000
2020-02-26 19:17:24,617 Epoch 1075: total training loss 0.00094
2020-02-26 19:17:24,820 EPOCH 1076
2020-02-26 19:17:35,078 Epoch 1076: total training loss 0.00094
2020-02-26 19:17:35,153 EPOCH 1077
2020-02-26 19:17:41,112 Epoch 1077 Step:   119500 Batch Loss:     0.000008 Tokens per Sec:  9681232, Lr: 0.001000
2020-02-26 19:17:45,613 Epoch 1077: total training loss 0.00095
2020-02-26 19:17:45,614 EPOCH 1078
2020-02-26 19:17:55,963 Epoch 1078: total training loss 0.00101
2020-02-26 19:17:55,963 EPOCH 1079
2020-02-26 19:18:04,566 Epoch 1079 Step:   119750 Batch Loss:     0.000012 Tokens per Sec:  9654830, Lr: 0.001000
2020-02-26 19:18:06,510 Epoch 1079: total training loss 0.00105
2020-02-26 19:18:06,510 EPOCH 1080
2020-02-26 19:18:16,868 Epoch 1080: total training loss 0.00096
2020-02-26 19:18:16,868 EPOCH 1081
2020-02-26 19:18:26,911 Epoch 1081: total training loss 0.00095
2020-02-26 19:18:26,911 EPOCH 1082
2020-02-26 19:18:27,916 Epoch 1082 Step:   120000 Batch Loss:     0.000009 Tokens per Sec:  9862932, Lr: 0.001000
2020-02-26 19:18:27,917 Model noise rate: 5
2020-02-26 19:19:29,181 Validation result at epoch 1082, step   120000: Val DTW Score:  10.67, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0369, GT DTW Score:      nan, duration: 61.2638s
2020-02-26 19:19:38,463 Epoch 1082: total training loss 0.00093
2020-02-26 19:19:38,464 EPOCH 1083
2020-02-26 19:19:48,547 Epoch 1083: total training loss 0.00093
2020-02-26 19:19:48,547 EPOCH 1084
2020-02-26 19:19:51,832 Epoch 1084 Step:   120250 Batch Loss:     0.000006 Tokens per Sec:  9917645, Lr: 0.001000
2020-02-26 19:19:58,570 Epoch 1084: total training loss 0.00095
2020-02-26 19:19:58,571 EPOCH 1085
2020-02-26 19:20:08,725 Epoch 1085: total training loss 0.00094
2020-02-26 19:20:08,725 EPOCH 1086
2020-02-26 19:20:14,869 Epoch 1086 Step:   120500 Batch Loss:     0.000013 Tokens per Sec: 10281687, Lr: 0.001000
2020-02-26 19:20:18,818 Epoch 1086: total training loss 0.00095
2020-02-26 19:20:18,818 EPOCH 1087
2020-02-26 19:20:28,879 Epoch 1087: total training loss 0.00096
2020-02-26 19:20:29,032 EPOCH 1088
2020-02-26 19:20:37,338 Epoch 1088 Step:   120750 Batch Loss:     0.000007 Tokens per Sec: 10457961, Lr: 0.001000
2020-02-26 19:20:38,934 Epoch 1088: total training loss 0.00095
2020-02-26 19:20:38,934 EPOCH 1089
2020-02-26 19:20:48,974 Epoch 1089: total training loss 0.00096
2020-02-26 19:20:48,975 EPOCH 1090
2020-02-26 19:20:59,074 Epoch 1090: total training loss 0.00093
2020-02-26 19:20:59,076 EPOCH 1091
2020-02-26 19:20:59,935 Epoch 1091 Step:   121000 Batch Loss:     0.000005 Tokens per Sec:  9847225, Lr: 0.001000
2020-02-26 19:21:09,388 Epoch 1091: total training loss 0.00094
2020-02-26 19:21:09,389 EPOCH 1092
2020-02-26 19:21:19,610 Epoch 1092: total training loss 0.00101
2020-02-26 19:21:19,610 EPOCH 1093
2020-02-26 19:21:23,156 Epoch 1093 Step:   121250 Batch Loss:     0.000005 Tokens per Sec: 10271796, Lr: 0.001000
2020-02-26 19:21:29,687 Epoch 1093: total training loss 0.00101
2020-02-26 19:21:29,687 EPOCH 1094
2020-02-26 19:21:39,961 Epoch 1094: total training loss 0.00096
2020-02-26 19:21:39,961 EPOCH 1095
2020-02-26 19:21:46,074 Epoch 1095 Step:   121500 Batch Loss:     0.000007 Tokens per Sec:  9895866, Lr: 0.001000
2020-02-26 19:21:50,191 Epoch 1095: total training loss 0.00093
2020-02-26 19:21:50,191 EPOCH 1096
2020-02-26 19:22:00,103 Epoch 1096: total training loss 0.00095
2020-02-26 19:22:00,104 EPOCH 1097
2020-02-26 19:22:08,743 Epoch 1097 Step:   121750 Batch Loss:     0.000005 Tokens per Sec: 10183995, Lr: 0.001000
2020-02-26 19:22:10,190 Epoch 1097: total training loss 0.00092
2020-02-26 19:22:10,190 EPOCH 1098
2020-02-26 19:22:20,157 Epoch 1098: total training loss 0.00093
2020-02-26 19:22:20,157 EPOCH 1099
2020-02-26 19:22:30,150 Epoch 1099: total training loss 0.00092
2020-02-26 19:22:30,150 EPOCH 1100
2020-02-26 19:22:31,273 Epoch 1100 Step:   122000 Batch Loss:     0.000005 Tokens per Sec: 10193534, Lr: 0.001000
2020-02-26 19:22:40,208 Epoch 1100: total training loss 0.00095
2020-02-26 19:22:40,210 EPOCH 1101
2020-02-26 19:22:50,170 Epoch 1101: total training loss 0.00095
2020-02-26 19:22:50,172 EPOCH 1102
2020-02-26 19:22:53,396 Epoch 1102 Step:   122250 Batch Loss:     0.000006 Tokens per Sec: 10018378, Lr: 0.001000
2020-02-26 19:23:00,136 Epoch 1102: total training loss 0.00093
2020-02-26 19:23:00,136 EPOCH 1103
2020-02-26 19:23:10,140 Epoch 1103: total training loss 0.00092
2020-02-26 19:23:10,141 EPOCH 1104
2020-02-26 19:23:16,085 Epoch 1104 Step:   122500 Batch Loss:     0.000007 Tokens per Sec: 10342463, Lr: 0.001000
2020-02-26 19:23:20,130 Epoch 1104: total training loss 0.00099
2020-02-26 19:23:20,130 EPOCH 1105
2020-02-26 19:23:30,167 Epoch 1105: total training loss 0.00100
2020-02-26 19:23:30,168 EPOCH 1106
2020-02-26 19:23:38,813 Epoch 1106 Step:   122750 Batch Loss:     0.000006 Tokens per Sec: 10175400, Lr: 0.001000
2020-02-26 19:23:40,216 Epoch 1106: total training loss 0.00097
2020-02-26 19:23:40,216 EPOCH 1107
2020-02-26 19:23:50,483 Epoch 1107: total training loss 0.00092
2020-02-26 19:23:50,483 EPOCH 1108
2020-02-26 19:24:00,944 Epoch 1108: total training loss 0.00092
2020-02-26 19:24:01,105 EPOCH 1109
2020-02-26 19:24:02,262 Epoch 1109 Step:   123000 Batch Loss:     0.000008 Tokens per Sec:  9719699, Lr: 0.001000
2020-02-26 19:24:11,471 Epoch 1109: total training loss 0.00098
2020-02-26 19:24:11,511 EPOCH 1110
2020-02-26 19:24:21,589 Epoch 1110: total training loss 0.00096
2020-02-26 19:24:21,590 EPOCH 1111
2020-02-26 19:24:25,273 Epoch 1111 Step:   123250 Batch Loss:     0.000012 Tokens per Sec: 10180139, Lr: 0.001000
2020-02-26 19:24:31,604 Epoch 1111: total training loss 0.00100
2020-02-26 19:24:31,604 EPOCH 1112
2020-02-26 19:24:41,831 Epoch 1112: total training loss 0.00094
2020-02-26 19:24:41,831 EPOCH 1113
2020-02-26 19:24:47,971 Epoch 1113 Step:   123500 Batch Loss:     0.000007 Tokens per Sec: 10149123, Lr: 0.001000
2020-02-26 19:24:51,910 Epoch 1113: total training loss 0.00093
2020-02-26 19:24:51,922 EPOCH 1114
2020-02-26 19:25:01,898 Epoch 1114: total training loss 0.00097
2020-02-26 19:25:01,898 EPOCH 1115
2020-02-26 19:25:10,423 Epoch 1115 Step:   123750 Batch Loss:     0.000004 Tokens per Sec: 10470139, Lr: 0.001000
2020-02-26 19:25:11,772 Epoch 1115: total training loss 0.00095
2020-02-26 19:25:11,772 EPOCH 1116
2020-02-26 19:25:21,678 Epoch 1116: total training loss 0.00095
2020-02-26 19:25:21,680 EPOCH 1117
2020-02-26 19:25:32,092 Epoch 1117: total training loss 0.00095
2020-02-26 19:25:32,092 EPOCH 1118
2020-02-26 19:25:33,412 Epoch 1118 Step:   124000 Batch Loss:     0.000008 Tokens per Sec:  9508253, Lr: 0.001000
2020-02-26 19:25:42,810 Epoch 1118: total training loss 0.00092
2020-02-26 19:25:42,811 EPOCH 1119
2020-02-26 19:25:53,489 Epoch 1119: total training loss 0.00094
2020-02-26 19:25:53,490 EPOCH 1120
2020-02-26 19:25:57,198 Epoch 1120 Step:   124250 Batch Loss:     0.000013 Tokens per Sec:  9393693, Lr: 0.001000
2020-02-26 19:26:04,074 Epoch 1120: total training loss 0.00093
2020-02-26 19:26:04,074 EPOCH 1121
2020-02-26 19:26:14,441 Epoch 1121: total training loss 0.00093
2020-02-26 19:26:14,441 EPOCH 1122
2020-02-26 19:26:20,847 Epoch 1122 Step:   124500 Batch Loss:     0.000009 Tokens per Sec: 10334016, Lr: 0.001000
2020-02-26 19:26:24,468 Epoch 1122: total training loss 0.00092
2020-02-26 19:26:24,469 EPOCH 1123
2020-02-26 19:26:34,629 Epoch 1123: total training loss 0.00092
2020-02-26 19:26:34,629 EPOCH 1124
2020-02-26 19:26:43,672 Epoch 1124 Step:   124750 Batch Loss:     0.000008 Tokens per Sec: 10106419, Lr: 0.001000
2020-02-26 19:26:44,858 Epoch 1124: total training loss 0.00090
2020-02-26 19:26:44,858 EPOCH 1125
2020-02-26 19:26:54,917 Epoch 1125: total training loss 0.00091
2020-02-26 19:26:54,917 EPOCH 1126
2020-02-26 19:27:04,839 Epoch 1126: total training loss 0.00111
2020-02-26 19:27:04,839 EPOCH 1127
2020-02-26 19:27:06,032 Epoch 1127 Step:   125000 Batch Loss:     0.000008 Tokens per Sec:  9942553, Lr: 0.001000
2020-02-26 19:27:14,873 Epoch 1127: total training loss 0.00096
2020-02-26 19:27:14,875 EPOCH 1128
2020-02-26 19:27:25,149 Epoch 1128: total training loss 0.00093
2020-02-26 19:27:25,149 EPOCH 1129
2020-02-26 19:27:29,067 Epoch 1129 Step:   125250 Batch Loss:     0.000006 Tokens per Sec:  9849959, Lr: 0.001000
2020-02-26 19:27:35,537 Epoch 1129: total training loss 0.00095
2020-02-26 19:27:35,537 EPOCH 1130
2020-02-26 19:27:45,913 Epoch 1130: total training loss 0.00093
2020-02-26 19:27:45,916 EPOCH 1131
2020-02-26 19:27:52,421 Epoch 1131 Step:   125500 Batch Loss:     0.000007 Tokens per Sec: 10101599, Lr: 0.001000
2020-02-26 19:27:56,267 Epoch 1131: total training loss 0.00097
2020-02-26 19:27:56,267 EPOCH 1132
2020-02-26 19:28:06,435 Epoch 1132: total training loss 0.00099
2020-02-26 19:28:06,436 EPOCH 1133
2020-02-26 19:28:15,366 Epoch 1133 Step:   125750 Batch Loss:     0.000006 Tokens per Sec: 10157785, Lr: 0.001000
2020-02-26 19:28:16,515 Epoch 1133: total training loss 0.00094
2020-02-26 19:28:16,515 EPOCH 1134
2020-02-26 19:28:26,579 Epoch 1134: total training loss 0.00092
2020-02-26 19:28:26,579 EPOCH 1135
2020-02-26 19:28:36,735 Epoch 1135: total training loss 0.00113
2020-02-26 19:28:36,736 EPOCH 1136
2020-02-26 19:28:38,034 Epoch 1136 Step:   126000 Batch Loss:     0.000006 Tokens per Sec: 10077628, Lr: 0.001000
2020-02-26 19:28:46,663 Epoch 1136: total training loss 0.00095
2020-02-26 19:28:46,663 EPOCH 1137
2020-02-26 19:28:56,593 Epoch 1137: total training loss 0.00091
2020-02-26 19:28:56,593 EPOCH 1138
2020-02-26 19:29:00,462 Epoch 1138 Step:   126250 Batch Loss:     0.000011 Tokens per Sec: 10234896, Lr: 0.001000
2020-02-26 19:29:06,687 Epoch 1138: total training loss 0.00096
2020-02-26 19:29:06,688 EPOCH 1139
2020-02-26 19:29:16,904 Epoch 1139: total training loss 0.00094
2020-02-26 19:29:16,905 EPOCH 1140
2020-02-26 19:29:23,467 Epoch 1140 Step:   126500 Batch Loss:     0.000006 Tokens per Sec: 10267684, Lr: 0.001000
2020-02-26 19:29:26,997 Epoch 1140: total training loss 0.00091
2020-02-26 19:29:26,998 EPOCH 1141
2020-02-26 19:29:37,071 Epoch 1141: total training loss 0.00090
2020-02-26 19:29:37,072 EPOCH 1142
2020-02-26 19:29:46,026 Epoch 1142 Step:   126750 Batch Loss:     0.000011 Tokens per Sec: 10100886, Lr: 0.001000
2020-02-26 19:29:47,235 Epoch 1142: total training loss 0.00092
2020-02-26 19:29:47,235 EPOCH 1143
2020-02-26 19:29:57,191 Epoch 1143: total training loss 0.00099
2020-02-26 19:29:57,191 EPOCH 1144
2020-02-26 19:30:07,175 Epoch 1144: total training loss 0.00098
2020-02-26 19:30:07,324 EPOCH 1145
2020-02-26 19:30:08,760 Epoch 1145 Step:   127000 Batch Loss:     0.000015 Tokens per Sec: 10408818, Lr: 0.001000
2020-02-26 19:30:17,233 Epoch 1145: total training loss 0.00108
2020-02-26 19:30:17,234 EPOCH 1146
2020-02-26 19:30:27,471 Epoch 1146: total training loss 0.00094
2020-02-26 19:30:27,472 EPOCH 1147
2020-02-26 19:30:31,755 Epoch 1147 Step:   127250 Batch Loss:     0.000008 Tokens per Sec: 10386295, Lr: 0.001000
2020-02-26 19:30:37,532 Epoch 1147: total training loss 0.00090
2020-02-26 19:30:37,533 EPOCH 1148
2020-02-26 19:30:47,693 Epoch 1148: total training loss 0.00094
2020-02-26 19:30:47,694 EPOCH 1149
2020-02-26 19:30:54,579 Epoch 1149 Step:   127500 Batch Loss:     0.000007 Tokens per Sec:  9633583, Lr: 0.001000
2020-02-26 19:30:58,351 Epoch 1149: total training loss 0.00093
2020-02-26 19:30:58,351 EPOCH 1150
2020-02-26 19:31:08,807 Epoch 1150: total training loss 0.00091
2020-02-26 19:31:08,807 EPOCH 1151
2020-02-26 19:31:18,076 Epoch 1151 Step:   127750 Batch Loss:     0.000004 Tokens per Sec: 10047351, Lr: 0.001000
2020-02-26 19:31:19,101 Epoch 1151: total training loss 0.00096
2020-02-26 19:31:19,101 EPOCH 1152
2020-02-26 19:31:29,369 Epoch 1152: total training loss 0.00096
2020-02-26 19:31:29,369 EPOCH 1153
2020-02-26 19:31:39,525 Epoch 1153: total training loss 0.00092
2020-02-26 19:31:39,525 EPOCH 1154
2020-02-26 19:31:41,065 Epoch 1154 Step:   128000 Batch Loss:     0.000008 Tokens per Sec:  9826331, Lr: 0.001000
2020-02-26 19:31:49,464 Epoch 1154: total training loss 0.00090
2020-02-26 19:31:49,464 EPOCH 1155
2020-02-26 19:31:59,300 Epoch 1155: total training loss 0.00092
2020-02-26 19:31:59,301 EPOCH 1156
2020-02-26 19:32:03,422 Epoch 1156 Step:   128250 Batch Loss:     0.000006 Tokens per Sec: 10264358, Lr: 0.001000
2020-02-26 19:32:09,275 Epoch 1156: total training loss 0.00094
2020-02-26 19:32:09,276 EPOCH 1157
2020-02-26 19:32:19,415 Epoch 1157: total training loss 0.00093
2020-02-26 19:32:19,417 EPOCH 1158
2020-02-26 19:32:26,025 Epoch 1158 Step:   128500 Batch Loss:     0.000009 Tokens per Sec: 10074098, Lr: 0.001000
2020-02-26 19:32:29,576 Epoch 1158: total training loss 0.00091
2020-02-26 19:32:29,577 EPOCH 1159
2020-02-26 19:32:39,757 Epoch 1159: total training loss 0.00090
2020-02-26 19:32:39,757 EPOCH 1160
2020-02-26 19:32:49,142 Epoch 1160 Step:   128750 Batch Loss:     0.000009 Tokens per Sec:  9978002, Lr: 0.001000
2020-02-26 19:32:50,078 Epoch 1160: total training loss 0.00092
2020-02-26 19:32:50,079 EPOCH 1161
2020-02-26 19:33:00,780 Epoch 1161: total training loss 0.00096
2020-02-26 19:33:00,780 EPOCH 1162
2020-02-26 19:33:10,931 Epoch 1162: total training loss 0.00091
2020-02-26 19:33:10,932 EPOCH 1163
2020-02-26 19:33:12,620 Epoch 1163 Step:   129000 Batch Loss:     0.000009 Tokens per Sec: 10099210, Lr: 0.001000
2020-02-26 19:33:21,006 Epoch 1163: total training loss 0.00092
2020-02-26 19:33:21,006 EPOCH 1164
2020-02-26 19:33:31,172 Epoch 1164: total training loss 0.00091
2020-02-26 19:33:31,172 EPOCH 1165
2020-02-26 19:33:35,417 Epoch 1165 Step:   129250 Batch Loss:     0.000006 Tokens per Sec: 10336241, Lr: 0.001000
2020-02-26 19:33:41,305 Epoch 1165: total training loss 0.00092
2020-02-26 19:33:41,305 EPOCH 1166
2020-02-26 19:33:51,289 Epoch 1166: total training loss 0.00090
2020-02-26 19:33:51,289 EPOCH 1167
2020-02-26 19:33:58,011 Epoch 1167 Step:   129500 Batch Loss:     0.000012 Tokens per Sec: 10251111, Lr: 0.001000
2020-02-26 19:34:01,417 Epoch 1167: total training loss 0.00090
2020-02-26 19:34:01,418 EPOCH 1168
2020-02-26 19:34:11,876 Epoch 1168: total training loss 0.00092
2020-02-26 19:34:11,877 EPOCH 1169
2020-02-26 19:34:21,368 Epoch 1169 Step:   129750 Batch Loss:     0.000011 Tokens per Sec:  9968234, Lr: 0.001000
2020-02-26 19:34:22,166 Epoch 1169: total training loss 0.00091
2020-02-26 19:34:22,166 EPOCH 1170
2020-02-26 19:34:32,431 Epoch 1170: total training loss 0.00091
2020-02-26 19:34:32,431 EPOCH 1171
2020-02-26 19:34:42,758 Epoch 1171: total training loss 0.00094
2020-02-26 19:34:42,759 EPOCH 1172
2020-02-26 19:34:44,708 Epoch 1172 Step:   130000 Batch Loss:     0.000005 Tokens per Sec: 10217842, Lr: 0.001000
2020-02-26 19:34:44,708 Model noise rate: 5
2020-02-26 19:35:46,035 Validation result at epoch 1172, step   130000: Val DTW Score:  10.67, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0374, GT DTW Score:      nan, duration: 61.3266s
2020-02-26 19:35:54,381 Epoch 1172: total training loss 0.00094
2020-02-26 19:35:54,381 EPOCH 1173
2020-02-26 19:36:04,689 Epoch 1173: total training loss 0.00093
2020-02-26 19:36:04,690 EPOCH 1174
2020-02-26 19:36:08,944 Epoch 1174 Step:   130250 Batch Loss:     0.000014 Tokens per Sec:  9926919, Lr: 0.001000
2020-02-26 19:36:15,079 Epoch 1174: total training loss 0.00095
2020-02-26 19:36:15,080 EPOCH 1175
2020-02-26 19:36:25,302 Epoch 1175: total training loss 0.00093
2020-02-26 19:36:25,302 EPOCH 1176
2020-02-26 19:36:32,139 Epoch 1176 Step:   130500 Batch Loss:     0.000007 Tokens per Sec:  9604361, Lr: 0.001000
2020-02-26 19:36:35,899 Epoch 1176: total training loss 0.00094
2020-02-26 19:36:35,899 EPOCH 1177
2020-02-26 19:36:46,027 Epoch 1177: total training loss 0.00096
2020-02-26 19:36:46,028 EPOCH 1178
2020-02-26 19:36:55,270 Epoch 1178 Step:   130750 Batch Loss:     0.000009 Tokens per Sec: 10183633, Lr: 0.001000
2020-02-26 19:36:56,023 Epoch 1178: total training loss 0.00094
2020-02-26 19:36:56,024 EPOCH 1179
2020-02-26 19:37:06,065 Epoch 1179: total training loss 0.00091
2020-02-26 19:37:06,066 EPOCH 1180
2020-02-26 19:37:16,144 Epoch 1180: total training loss 0.00096
2020-02-26 19:37:16,144 EPOCH 1181
2020-02-26 19:37:17,916 Epoch 1181 Step:   131000 Batch Loss:     0.000007 Tokens per Sec: 10070083, Lr: 0.001000
2020-02-26 19:37:26,242 Epoch 1181: total training loss 0.00099
2020-02-26 19:37:26,242 EPOCH 1182
2020-02-26 19:37:36,353 Epoch 1182: total training loss 0.00096
2020-02-26 19:37:36,354 EPOCH 1183
2020-02-26 19:37:40,686 Epoch 1183 Step:   131250 Batch Loss:     0.000012 Tokens per Sec: 10360290, Lr: 0.001000
2020-02-26 19:37:46,369 Epoch 1183: total training loss 0.00091
2020-02-26 19:37:46,369 EPOCH 1184
2020-02-26 19:37:56,404 Epoch 1184: total training loss 0.00089
2020-02-26 19:37:56,405 EPOCH 1185
2020-02-26 19:38:03,696 Epoch 1185 Step:   131500 Batch Loss:     0.000009 Tokens per Sec: 10002928, Lr: 0.001000
2020-02-26 19:38:06,654 Epoch 1185: total training loss 0.00090
2020-02-26 19:38:06,654 EPOCH 1186
2020-02-26 19:38:16,859 Epoch 1186: total training loss 0.00090
2020-02-26 19:38:16,860 EPOCH 1187
2020-02-26 19:38:26,373 Epoch 1187 Step:   131750 Batch Loss:     0.000010 Tokens per Sec: 10123193, Lr: 0.001000
2020-02-26 19:38:27,062 Epoch 1187: total training loss 0.00090
2020-02-26 19:38:27,062 EPOCH 1188
2020-02-26 19:38:37,165 Epoch 1188: total training loss 0.00091
2020-02-26 19:38:37,165 EPOCH 1189
2020-02-26 19:38:47,124 Epoch 1189: total training loss 0.00090
2020-02-26 19:38:47,126 EPOCH 1190
2020-02-26 19:38:48,959 Epoch 1190 Step:   132000 Batch Loss:     0.000009 Tokens per Sec: 10232573, Lr: 0.001000
2020-02-26 19:38:57,188 Epoch 1190: total training loss 0.00091
2020-02-26 19:38:57,189 EPOCH 1191
2020-02-26 19:39:07,150 Epoch 1191: total training loss 0.00100
2020-02-26 19:39:07,150 EPOCH 1192
2020-02-26 19:39:11,760 Epoch 1192 Step:   132250 Batch Loss:     0.000009 Tokens per Sec: 10212484, Lr: 0.001000
2020-02-26 19:39:17,113 Epoch 1192: total training loss 0.00096
2020-02-26 19:39:17,113 EPOCH 1193
2020-02-26 19:39:27,250 Epoch 1193: total training loss 0.00098
2020-02-26 19:39:27,252 EPOCH 1194
2020-02-26 19:39:34,354 Epoch 1194 Step:   132500 Batch Loss:     0.000006 Tokens per Sec:  9928442, Lr: 0.001000
2020-02-26 19:39:37,477 Epoch 1194: total training loss 0.00092
2020-02-26 19:39:37,477 EPOCH 1195
2020-02-26 19:39:47,610 Epoch 1195: total training loss 0.00090
2020-02-26 19:39:47,610 EPOCH 1196
2020-02-26 19:39:57,299 Epoch 1196 Step:   132750 Batch Loss:     0.000005 Tokens per Sec:  9962515, Lr: 0.001000
2020-02-26 19:39:57,897 Epoch 1196: total training loss 0.00092
2020-02-26 19:39:57,897 EPOCH 1197
2020-02-26 19:40:08,131 Epoch 1197: total training loss 0.00090
2020-02-26 19:40:08,131 EPOCH 1198
2020-02-26 19:40:18,102 Epoch 1198: total training loss 0.00089
2020-02-26 19:40:18,102 EPOCH 1199
2020-02-26 19:40:19,987 Epoch 1199 Step:   133000 Batch Loss:     0.000008 Tokens per Sec:  9884392, Lr: 0.001000
2020-02-26 19:40:28,168 Epoch 1199: total training loss 0.00089
2020-02-26 19:40:28,195 EPOCH 1200
2020-02-26 19:40:38,197 Epoch 1200: total training loss 0.00091
2020-02-26 19:40:38,197 EPOCH 1201
2020-02-26 19:40:42,770 Epoch 1201 Step:   133250 Batch Loss:     0.000014 Tokens per Sec: 10189926, Lr: 0.001000
2020-02-26 19:40:48,253 Epoch 1201: total training loss 0.00091
2020-02-26 19:40:48,253 EPOCH 1202
2020-02-26 19:40:58,049 Epoch 1202: total training loss 0.00093
2020-02-26 19:40:58,050 EPOCH 1203
2020-02-26 19:41:04,979 Epoch 1203 Step:   133500 Batch Loss:     0.000008 Tokens per Sec: 10300996, Lr: 0.001000
2020-02-26 19:41:08,015 Epoch 1203: total training loss 0.00094
2020-02-26 19:41:08,016 EPOCH 1204
2020-02-26 19:41:18,445 Epoch 1204: total training loss 0.00095
2020-02-26 19:41:18,448 EPOCH 1205
2020-02-26 19:41:28,435 Epoch 1205 Step:   133750 Batch Loss:     0.000009 Tokens per Sec:  9726535, Lr: 0.001000
2020-02-26 19:41:28,959 Epoch 1205: total training loss 0.00092
2020-02-26 19:41:28,959 EPOCH 1206
2020-02-26 19:41:39,666 Epoch 1206: total training loss 0.00095
2020-02-26 19:41:39,666 EPOCH 1207
2020-02-26 19:41:49,970 Epoch 1207: total training loss 0.00090
2020-02-26 19:41:49,971 EPOCH 1208
2020-02-26 19:41:52,213 Epoch 1208 Step:   134000 Batch Loss:     0.000013 Tokens per Sec:  9806300, Lr: 0.001000
2020-02-26 19:42:00,329 Epoch 1208: total training loss 0.00088
2020-02-26 19:42:00,329 EPOCH 1209
2020-02-26 19:42:10,310 Epoch 1209: total training loss 0.00091
2020-02-26 19:42:10,311 EPOCH 1210
2020-02-26 19:42:14,877 Epoch 1210 Step:   134250 Batch Loss:     0.000012 Tokens per Sec: 10411656, Lr: 0.001000
2020-02-26 19:42:20,297 Epoch 1210: total training loss 0.00090
2020-02-26 19:42:20,297 EPOCH 1211
2020-02-26 19:42:30,262 Epoch 1211: total training loss 0.00089
2020-02-26 19:42:30,263 EPOCH 1212
2020-02-26 19:42:37,212 Epoch 1212 Step:   134500 Batch Loss:     0.000010 Tokens per Sec: 10124941, Lr: 0.001000
2020-02-26 19:42:40,256 Epoch 1212: total training loss 0.00088
2020-02-26 19:42:40,256 EPOCH 1213
2020-02-26 19:42:50,380 Epoch 1213: total training loss 0.00105
2020-02-26 19:42:50,382 EPOCH 1214
2020-02-26 19:43:00,297 Epoch 1214 Step:   134750 Batch Loss:     0.000009 Tokens per Sec:  9994520, Lr: 0.001000
2020-02-26 19:43:00,661 Epoch 1214: total training loss 0.00092
2020-02-26 19:43:00,661 EPOCH 1215
2020-02-26 19:43:10,940 Epoch 1215: total training loss 0.00090
2020-02-26 19:43:10,940 EPOCH 1216
2020-02-26 19:43:21,140 Epoch 1216: total training loss 0.00091
2020-02-26 19:43:21,140 EPOCH 1217
2020-02-26 19:43:23,253 Epoch 1217 Step:   135000 Batch Loss:     0.000005 Tokens per Sec:  9617960, Lr: 0.001000
2020-02-26 19:43:31,387 Epoch 1217: total training loss 0.00088
2020-02-26 19:43:31,387 EPOCH 1218
2020-02-26 19:43:41,400 Epoch 1218: total training loss 0.00091
2020-02-26 19:43:41,400 EPOCH 1219
2020-02-26 19:43:46,069 Epoch 1219 Step:   135250 Batch Loss:     0.000011 Tokens per Sec: 10420351, Lr: 0.001000
2020-02-26 19:43:51,327 Epoch 1219: total training loss 0.00087
2020-02-26 19:43:51,327 EPOCH 1220
2020-02-26 19:44:01,277 Epoch 1220: total training loss 0.00087
2020-02-26 19:44:01,277 EPOCH 1221
2020-02-26 19:44:08,540 Epoch 1221 Step:   135500 Batch Loss:     0.000005 Tokens per Sec: 10310057, Lr: 0.001000
2020-02-26 19:44:11,245 Epoch 1221: total training loss 0.00088
2020-02-26 19:44:11,246 EPOCH 1222
2020-02-26 19:44:21,072 Epoch 1222: total training loss 0.00100
2020-02-26 19:44:21,072 EPOCH 1223
2020-02-26 19:44:30,709 Epoch 1223 Step:   135750 Batch Loss:     0.000006 Tokens per Sec: 10370830, Lr: 0.001000
2020-02-26 19:44:30,957 Epoch 1223: total training loss 0.00093
2020-02-26 19:44:30,957 EPOCH 1224
2020-02-26 19:44:40,871 Epoch 1224: total training loss 0.00088
2020-02-26 19:44:40,871 EPOCH 1225
2020-02-26 19:44:50,724 Epoch 1225: total training loss 0.00087
2020-02-26 19:44:50,724 EPOCH 1226
2020-02-26 19:44:53,034 Epoch 1226 Step:   136000 Batch Loss:     0.000010 Tokens per Sec: 10208444, Lr: 0.001000
2020-02-26 19:45:00,700 Epoch 1226: total training loss 0.00094
2020-02-26 19:45:00,700 EPOCH 1227
2020-02-26 19:45:10,774 Epoch 1227: total training loss 0.00091
2020-02-26 19:45:10,776 EPOCH 1228
2020-02-26 19:45:15,691 Epoch 1228 Step:   136250 Batch Loss:     0.000008 Tokens per Sec:  9981540, Lr: 0.001000
2020-02-26 19:45:20,996 Epoch 1228: total training loss 0.00093
2020-02-26 19:45:20,996 EPOCH 1229
2020-02-26 19:45:31,246 Epoch 1229: total training loss 0.00090
2020-02-26 19:45:31,246 EPOCH 1230
2020-02-26 19:45:38,700 Epoch 1230 Step:   136500 Batch Loss:     0.000007 Tokens per Sec: 10053042, Lr: 0.001000
2020-02-26 19:45:41,443 Epoch 1230: total training loss 0.00090
2020-02-26 19:45:41,443 EPOCH 1231
2020-02-26 19:45:51,801 Epoch 1231: total training loss 0.00091
2020-02-26 19:45:51,801 EPOCH 1232
2020-02-26 19:46:01,865 Epoch 1232 Step:   136750 Batch Loss:     0.000006 Tokens per Sec: 10068640, Lr: 0.001000
2020-02-26 19:46:02,037 Epoch 1232: total training loss 0.00087
2020-02-26 19:46:02,037 EPOCH 1233
2020-02-26 19:46:12,174 Epoch 1233: total training loss 0.00087
2020-02-26 19:46:12,174 EPOCH 1234
2020-02-26 19:46:22,267 Epoch 1234: total training loss 0.00090
2020-02-26 19:46:22,267 EPOCH 1235
2020-02-26 19:46:24,507 Epoch 1235 Step:   137000 Batch Loss:     0.000006 Tokens per Sec: 10056866, Lr: 0.001000
2020-02-26 19:46:32,524 Epoch 1235: total training loss 0.00108
2020-02-26 19:46:32,526 EPOCH 1236
2020-02-26 19:46:42,571 Epoch 1236: total training loss 0.00103
2020-02-26 19:46:42,571 EPOCH 1237
2020-02-26 19:46:47,281 Epoch 1237 Step:   137250 Batch Loss:     0.000006 Tokens per Sec: 10330345, Lr: 0.001000
2020-02-26 19:46:52,536 Epoch 1237: total training loss 0.00094
2020-02-26 19:46:52,536 EPOCH 1238
2020-02-26 19:47:02,503 Epoch 1238: total training loss 0.00089
2020-02-26 19:47:02,504 EPOCH 1239
2020-02-26 19:47:10,143 Epoch 1239 Step:   137500 Batch Loss:     0.000005 Tokens per Sec:  9944058, Lr: 0.001000
2020-02-26 19:47:12,782 Epoch 1239: total training loss 0.00088
2020-02-26 19:47:12,783 EPOCH 1240
2020-02-26 19:47:23,092 Epoch 1240: total training loss 0.00086
2020-02-26 19:47:23,092 EPOCH 1241
2020-02-26 19:47:33,350 Epoch 1241 Step:   137750 Batch Loss:     0.000005 Tokens per Sec:  9900822, Lr: 0.001000
2020-02-26 19:47:33,453 Epoch 1241: total training loss 0.00087
2020-02-26 19:47:33,453 EPOCH 1242
2020-02-26 19:47:43,924 Epoch 1242: total training loss 0.00092
2020-02-26 19:47:43,925 EPOCH 1243
2020-02-26 19:47:54,256 Epoch 1243: total training loss 0.00090
2020-02-26 19:47:54,256 EPOCH 1244
2020-02-26 19:47:56,543 Epoch 1244 Step:   138000 Batch Loss:     0.000005 Tokens per Sec: 10133529, Lr: 0.001000
2020-02-26 19:48:04,362 Epoch 1244: total training loss 0.00088
2020-02-26 19:48:04,362 EPOCH 1245
2020-02-26 19:48:14,515 Epoch 1245: total training loss 0.00090
2020-02-26 19:48:14,516 EPOCH 1246
2020-02-26 19:48:19,513 Epoch 1246 Step:   138250 Batch Loss:     0.000018 Tokens per Sec: 10169946, Lr: 0.001000
2020-02-26 19:48:24,620 Epoch 1246: total training loss 0.00097
2020-02-26 19:48:24,620 EPOCH 1247
2020-02-26 19:48:34,387 Epoch 1247: total training loss 0.00099
2020-02-26 19:48:34,387 EPOCH 1248
2020-02-26 19:48:41,842 Epoch 1248 Step:   138500 Batch Loss:     0.000013 Tokens per Sec: 10404978, Lr: 0.001000
2020-02-26 19:48:44,352 Epoch 1248: total training loss 0.00098
2020-02-26 19:48:44,352 EPOCH 1249
2020-02-26 19:48:54,262 Epoch 1249: total training loss 0.00089
2020-02-26 19:48:54,263 EPOCH 1250
2020-02-26 19:49:04,120 Epoch 1250 Step:   138750 Batch Loss:     0.000008 Tokens per Sec: 10362127, Lr: 0.001000
2020-02-26 19:49:04,120 Epoch 1250: total training loss 0.00088
2020-02-26 19:49:04,121 EPOCH 1251
2020-02-26 19:49:14,051 Epoch 1251: total training loss 0.00087
2020-02-26 19:49:14,052 EPOCH 1252
2020-02-26 19:49:24,379 Epoch 1252: total training loss 0.00086
2020-02-26 19:49:24,380 EPOCH 1253
2020-02-26 19:49:27,071 Epoch 1253 Step:   139000 Batch Loss:     0.000008 Tokens per Sec: 10043791, Lr: 0.001000
2020-02-26 19:49:34,784 Epoch 1253: total training loss 0.00092
2020-02-26 19:49:34,784 EPOCH 1254
2020-02-26 19:49:45,076 Epoch 1254: total training loss 0.00093
2020-02-26 19:49:45,077 EPOCH 1255
2020-02-26 19:49:50,330 Epoch 1255 Step:   139250 Batch Loss:     0.000010 Tokens per Sec:  9731419, Lr: 0.001000
2020-02-26 19:49:55,464 Epoch 1255: total training loss 0.00088
2020-02-26 19:49:55,464 EPOCH 1256
2020-02-26 19:50:05,427 Epoch 1256: total training loss 0.00088
2020-02-26 19:50:05,427 EPOCH 1257
2020-02-26 19:50:12,934 Epoch 1257 Step:   139500 Batch Loss:     0.000007 Tokens per Sec: 10353722, Lr: 0.001000
2020-02-26 19:50:15,342 Epoch 1257: total training loss 0.00087
2020-02-26 19:50:15,342 EPOCH 1258
2020-02-26 19:50:25,247 Epoch 1258: total training loss 0.00086
2020-02-26 19:50:25,247 EPOCH 1259
2020-02-26 19:50:35,260 Epoch 1259: total training loss 0.00089
2020-02-26 19:50:35,260 EPOCH 1260
2020-02-26 19:50:35,386 Epoch 1260 Step:   139750 Batch Loss:     0.000012 Tokens per Sec:  8753219, Lr: 0.001000
2020-02-26 19:50:45,135 Epoch 1260: total training loss 0.00088
2020-02-26 19:50:45,136 EPOCH 1261
2020-02-26 19:50:55,102 Epoch 1261: total training loss 0.00091
2020-02-26 19:50:55,103 EPOCH 1262
2020-02-26 19:50:57,677 Epoch 1262 Step:   140000 Batch Loss:     0.000010 Tokens per Sec: 10252872, Lr: 0.001000
2020-02-26 19:50:57,678 Model noise rate: 5
2020-02-26 19:51:57,629 Validation result at epoch 1262, step   140000: Val DTW Score:  10.62, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0373, GT DTW Score:      nan, duration: 59.9515s
2020-02-26 19:52:05,222 Epoch 1262: total training loss 0.00089
2020-02-26 19:52:05,223 EPOCH 1263
2020-02-26 19:52:15,662 Epoch 1263: total training loss 0.00087
2020-02-26 19:52:15,662 EPOCH 1264
2020-02-26 19:52:20,712 Epoch 1264 Step:   140250 Batch Loss:     0.000012 Tokens per Sec: 10142149, Lr: 0.001000
2020-02-26 19:52:25,821 Epoch 1264: total training loss 0.00089
2020-02-26 19:52:25,821 EPOCH 1265
2020-02-26 19:52:35,753 Epoch 1265: total training loss 0.00088
2020-02-26 19:52:35,754 EPOCH 1266
2020-02-26 19:52:43,369 Epoch 1266 Step:   140500 Batch Loss:     0.000005 Tokens per Sec: 10351135, Lr: 0.001000
2020-02-26 19:52:45,633 Epoch 1266: total training loss 0.00090
2020-02-26 19:52:45,633 EPOCH 1267
2020-02-26 19:52:55,624 Epoch 1267: total training loss 0.00089
2020-02-26 19:52:55,624 EPOCH 1268
2020-02-26 19:53:05,441 Epoch 1268: total training loss 0.00088
2020-02-26 19:53:05,442 EPOCH 1269
2020-02-26 19:53:05,614 Epoch 1269 Step:   140750 Batch Loss:     0.000008 Tokens per Sec:  8869246, Lr: 0.001000
2020-02-26 19:53:15,088 Epoch 1269: total training loss 0.00090
2020-02-26 19:53:15,088 EPOCH 1270
2020-02-26 19:53:24,838 Epoch 1270: total training loss 0.00087
2020-02-26 19:53:24,839 EPOCH 1271
2020-02-26 19:53:27,456 Epoch 1271 Step:   141000 Batch Loss:     0.000011 Tokens per Sec: 10516659, Lr: 0.001000
2020-02-26 19:53:34,665 Epoch 1271: total training loss 0.00090
2020-02-26 19:53:34,665 EPOCH 1272
2020-02-26 19:53:44,390 Epoch 1272: total training loss 0.00091
2020-02-26 19:53:44,390 EPOCH 1273
2020-02-26 19:53:49,553 Epoch 1273 Step:   141250 Batch Loss:     0.000015 Tokens per Sec: 10675725, Lr: 0.001000
2020-02-26 19:53:54,093 Epoch 1273: total training loss 0.00088
2020-02-26 19:53:54,093 EPOCH 1274
2020-02-26 19:54:03,844 Epoch 1274: total training loss 0.00087
2020-02-26 19:54:03,845 EPOCH 1275
2020-02-26 19:54:11,360 Epoch 1275 Step:   141500 Batch Loss:     0.000010 Tokens per Sec: 10680287, Lr: 0.001000
2020-02-26 19:54:13,525 Epoch 1275: total training loss 0.00087
2020-02-26 19:54:13,526 EPOCH 1276
2020-02-26 19:54:23,278 Epoch 1276: total training loss 0.00094
2020-02-26 19:54:23,279 EPOCH 1277
2020-02-26 19:54:33,108 Epoch 1277: total training loss 0.00096
2020-02-26 19:54:33,109 EPOCH 1278
2020-02-26 19:54:33,375 Epoch 1278 Step:   141750 Batch Loss:     0.000005 Tokens per Sec:  9115428, Lr: 0.001000
2020-02-26 19:54:42,932 Epoch 1278: total training loss 0.00086
2020-02-26 19:54:42,932 EPOCH 1279
2020-02-26 19:54:52,756 Epoch 1279: total training loss 0.00087
2020-02-26 19:54:52,757 EPOCH 1280
2020-02-26 19:54:55,679 Epoch 1280 Step:   142000 Batch Loss:     0.000010 Tokens per Sec: 10477011, Lr: 0.001000
2020-02-26 19:55:02,570 Epoch 1280: total training loss 0.00088
2020-02-26 19:55:02,570 EPOCH 1281
2020-02-26 19:55:12,338 Epoch 1281: total training loss 0.00092
2020-02-26 19:55:12,339 EPOCH 1282
2020-02-26 19:55:17,491 Epoch 1282 Step:   142250 Batch Loss:     0.000008 Tokens per Sec: 10789816, Lr: 0.001000
2020-02-26 19:55:21,975 Epoch 1282: total training loss 0.00089
2020-02-26 19:55:21,975 EPOCH 1283
2020-02-26 19:55:31,728 Epoch 1283: total training loss 0.00086
2020-02-26 19:55:31,728 EPOCH 1284
2020-02-26 19:55:39,315 Epoch 1284 Step:   142500 Batch Loss:     0.000006 Tokens per Sec: 10618467, Lr: 0.001000
2020-02-26 19:55:41,333 Epoch 1284: total training loss 0.00086
2020-02-26 19:55:41,334 EPOCH 1285
2020-02-26 19:55:51,050 Epoch 1285: total training loss 0.00085
2020-02-26 19:55:51,051 EPOCH 1286
2020-02-26 19:56:00,783 Epoch 1286: total training loss 0.00087
2020-02-26 19:56:00,783 EPOCH 1287
2020-02-26 19:56:01,178 Epoch 1287 Step:   142750 Batch Loss:     0.000008 Tokens per Sec:  9838802, Lr: 0.001000
2020-02-26 19:56:10,494 Epoch 1287: total training loss 0.00087
2020-02-26 19:56:10,494 EPOCH 1288
2020-02-26 19:56:20,162 Epoch 1288: total training loss 0.00085
2020-02-26 19:56:20,163 EPOCH 1289
2020-02-26 19:56:22,940 Epoch 1289 Step:   143000 Batch Loss:     0.000009 Tokens per Sec: 10529593, Lr: 0.001000
2020-02-26 19:56:29,859 Epoch 1289: total training loss 0.00094
2020-02-26 19:56:29,860 EPOCH 1290
2020-02-26 19:56:39,688 Epoch 1290: total training loss 0.00102
2020-02-26 19:56:39,688 EPOCH 1291
2020-02-26 19:56:44,963 Epoch 1291 Step:   143250 Batch Loss:     0.000007 Tokens per Sec: 10207666, Lr: 0.001000
2020-02-26 19:56:49,663 Epoch 1291: total training loss 0.00089
2020-02-26 19:56:49,663 EPOCH 1292
2020-02-26 19:56:59,660 Epoch 1292: total training loss 0.00087
2020-02-26 19:56:59,661 EPOCH 1293
2020-02-26 19:57:07,959 Epoch 1293 Step:   143500 Batch Loss:     0.000006 Tokens per Sec:  9941370, Lr: 0.001000
2020-02-26 19:57:10,134 Epoch 1293: total training loss 0.00088
2020-02-26 19:57:10,134 EPOCH 1294
2020-02-26 19:57:20,614 Epoch 1294: total training loss 0.00087
2020-02-26 19:57:20,614 EPOCH 1295
2020-02-26 19:57:31,082 Epoch 1295: total training loss 0.00087
2020-02-26 19:57:31,083 EPOCH 1296
2020-02-26 19:57:31,517 Epoch 1296 Step:   143750 Batch Loss:     0.000005 Tokens per Sec:  9591782, Lr: 0.001000
2020-02-26 19:57:41,163 Epoch 1296: total training loss 0.00086
2020-02-26 19:57:41,163 EPOCH 1297
2020-02-26 19:57:51,272 Epoch 1297: total training loss 0.00090
2020-02-26 19:57:51,272 EPOCH 1298
2020-02-26 19:57:53,956 Epoch 1298 Step:   144000 Batch Loss:     0.000006 Tokens per Sec: 10311112, Lr: 0.001000
2020-02-26 19:58:01,106 Epoch 1298: total training loss 0.00086
2020-02-26 19:58:01,106 EPOCH 1299
2020-02-26 19:58:10,858 Epoch 1299: total training loss 0.00086
2020-02-26 19:58:10,858 EPOCH 1300
2020-02-26 19:58:16,309 Epoch 1300 Step:   144250 Batch Loss:     0.000008 Tokens per Sec: 10450883, Lr: 0.001000
2020-02-26 19:58:20,689 Epoch 1300: total training loss 0.00093
2020-02-26 19:58:20,689 EPOCH 1301
2020-02-26 19:58:30,366 Epoch 1301: total training loss 0.00092
2020-02-26 19:58:30,366 EPOCH 1302
2020-02-26 19:58:38,268 Epoch 1302 Step:   144500 Batch Loss:     0.000011 Tokens per Sec: 10575605, Lr: 0.001000
2020-02-26 19:58:40,131 Epoch 1302: total training loss 0.00090
2020-02-26 19:58:40,132 EPOCH 1303
2020-02-26 19:58:50,059 Epoch 1303: total training loss 0.00091
2020-02-26 19:58:50,059 EPOCH 1304
2020-02-26 19:59:00,244 Epoch 1304: total training loss 0.00090
2020-02-26 19:59:00,244 EPOCH 1305
2020-02-26 19:59:00,804 Epoch 1305 Step:   144750 Batch Loss:     0.000007 Tokens per Sec: 10308453, Lr: 0.001000
2020-02-26 19:59:10,244 Epoch 1305: total training loss 0.00090
2020-02-26 19:59:10,244 EPOCH 1306
2020-02-26 19:59:20,501 Epoch 1306: total training loss 0.00088
2020-02-26 19:59:20,502 EPOCH 1307
2020-02-26 19:59:23,548 Epoch 1307 Step:   145000 Batch Loss:     0.000009 Tokens per Sec:  9992426, Lr: 0.001000
2020-02-26 19:59:30,651 Epoch 1307: total training loss 0.00088
2020-02-26 19:59:30,651 EPOCH 1308
2020-02-26 19:59:40,584 Epoch 1308: total training loss 0.00090
2020-02-26 19:59:40,584 EPOCH 1309
2020-02-26 19:59:46,002 Epoch 1309 Step:   145250 Batch Loss:     0.000007 Tokens per Sec: 10259133, Lr: 0.001000
2020-02-26 19:59:50,497 Epoch 1309: total training loss 0.00087
2020-02-26 19:59:50,498 EPOCH 1310
2020-02-26 20:00:00,357 Epoch 1310: total training loss 0.00087
2020-02-26 20:00:00,358 EPOCH 1311
2020-02-26 20:00:08,198 Epoch 1311 Step:   145500 Batch Loss:     0.000007 Tokens per Sec: 10627491, Lr: 0.001000
2020-02-26 20:00:09,993 Epoch 1311: total training loss 0.00088
2020-02-26 20:00:09,993 EPOCH 1312
2020-02-26 20:00:19,624 Epoch 1312: total training loss 0.00090
2020-02-26 20:00:19,624 EPOCH 1313
2020-02-26 20:00:29,288 Epoch 1313: total training loss 0.00091
2020-02-26 20:00:29,289 EPOCH 1314
2020-02-26 20:00:29,835 Epoch 1314 Step:   145750 Batch Loss:     0.000006 Tokens per Sec:  9509329, Lr: 0.001000
2020-02-26 20:00:39,199 Epoch 1314: total training loss 0.00088
2020-02-26 20:00:39,246 EPOCH 1315
2020-02-26 20:00:49,170 Epoch 1315: total training loss 0.00085
2020-02-26 20:00:49,171 EPOCH 1316
2020-02-26 20:00:52,354 Epoch 1316 Step:   146000 Batch Loss:     0.000006 Tokens per Sec: 10225594, Lr: 0.001000
2020-02-26 20:00:59,265 Epoch 1316: total training loss 0.00085
2020-02-26 20:00:59,265 EPOCH 1317
2020-02-26 20:01:09,233 Epoch 1317: total training loss 0.00086
2020-02-26 20:01:09,234 EPOCH 1318
2020-02-26 20:01:14,842 Epoch 1318 Step:   146250 Batch Loss:     0.000015 Tokens per Sec: 10371051, Lr: 0.001000
2020-02-26 20:01:19,129 Epoch 1318: total training loss 0.00089
2020-02-26 20:01:19,130 EPOCH 1319
2020-02-26 20:01:28,817 Epoch 1319: total training loss 0.00085
2020-02-26 20:01:28,817 EPOCH 1320
2020-02-26 20:01:36,808 Epoch 1320 Step:   146500 Batch Loss:     0.000013 Tokens per Sec: 10595189, Lr: 0.001000
2020-02-26 20:01:38,505 Epoch 1320: total training loss 0.00087
2020-02-26 20:01:38,505 EPOCH 1321
2020-02-26 20:01:48,353 Epoch 1321: total training loss 0.00086
2020-02-26 20:01:48,353 EPOCH 1322
2020-02-26 20:01:58,323 Epoch 1322: total training loss 0.00084
2020-02-26 20:01:58,324 EPOCH 1323
2020-02-26 20:01:59,014 Epoch 1323 Step:   146750 Batch Loss:     0.000008 Tokens per Sec: 10000497, Lr: 0.001000
2020-02-26 20:02:08,163 Epoch 1323: total training loss 0.00089
2020-02-26 20:02:08,163 EPOCH 1324
2020-02-26 20:02:18,035 Epoch 1324: total training loss 0.00091
2020-02-26 20:02:18,036 EPOCH 1325
2020-02-26 20:02:21,350 Epoch 1325 Step:   147000 Batch Loss:     0.000005 Tokens per Sec: 10302263, Lr: 0.001000
2020-02-26 20:02:28,058 Epoch 1325: total training loss 0.00086
2020-02-26 20:02:28,059 EPOCH 1326
2020-02-26 20:02:38,143 Epoch 1326: total training loss 0.00085
2020-02-26 20:02:38,144 EPOCH 1327
2020-02-26 20:02:43,748 Epoch 1327 Step:   147250 Batch Loss:     0.000008 Tokens per Sec: 10328446, Lr: 0.001000
2020-02-26 20:02:48,157 Epoch 1327: total training loss 0.00084
2020-02-26 20:02:48,158 EPOCH 1328
2020-02-26 20:02:58,254 Epoch 1328: total training loss 0.00086
2020-02-26 20:02:58,254 EPOCH 1329
2020-02-26 20:03:06,158 Epoch 1329 Step:   147500 Batch Loss:     0.000004 Tokens per Sec: 10397725, Lr: 0.001000
2020-02-26 20:03:08,060 Epoch 1329: total training loss 0.00086
2020-02-26 20:03:08,061 EPOCH 1330
2020-02-26 20:03:17,781 Epoch 1330: total training loss 0.00088
2020-02-26 20:03:17,781 EPOCH 1331
2020-02-26 20:03:27,513 Epoch 1331: total training loss 0.00088
2020-02-26 20:03:27,513 EPOCH 1332
2020-02-26 20:03:28,373 Epoch 1332 Step:   147750 Batch Loss:     0.000007 Tokens per Sec: 10327492, Lr: 0.001000
2020-02-26 20:03:37,293 Epoch 1332: total training loss 0.00098
2020-02-26 20:03:37,294 EPOCH 1333
2020-02-26 20:03:47,043 Epoch 1333: total training loss 0.00086
2020-02-26 20:03:47,043 EPOCH 1334
2020-02-26 20:03:50,544 Epoch 1334 Step:   148000 Batch Loss:     0.000008 Tokens per Sec: 10589401, Lr: 0.001000
2020-02-26 20:03:56,938 Epoch 1334: total training loss 0.00086
2020-02-26 20:03:56,938 EPOCH 1335
2020-02-26 20:04:06,810 Epoch 1335: total training loss 0.00088
2020-02-26 20:04:06,810 EPOCH 1336
2020-02-26 20:04:12,678 Epoch 1336 Step:   148250 Batch Loss:     0.000012 Tokens per Sec: 10398210, Lr: 0.001000
2020-02-26 20:04:16,777 Epoch 1336: total training loss 0.00085
2020-02-26 20:04:16,777 EPOCH 1337
2020-02-26 20:04:26,865 Epoch 1337: total training loss 0.00086
2020-02-26 20:04:26,865 EPOCH 1338
2020-02-26 20:04:35,481 Epoch 1338 Step:   148500 Batch Loss:     0.000008 Tokens per Sec:  9991619, Lr: 0.001000
2020-02-26 20:04:37,184 Epoch 1338: total training loss 0.00085
2020-02-26 20:04:37,185 EPOCH 1339
2020-02-26 20:04:47,445 Epoch 1339: total training loss 0.00088
2020-02-26 20:04:47,445 EPOCH 1340
2020-02-26 20:04:57,353 Epoch 1340: total training loss 0.00086
2020-02-26 20:04:57,353 EPOCH 1341
2020-02-26 20:04:58,339 Epoch 1341 Step:   148750 Batch Loss:     0.000008 Tokens per Sec: 10327811, Lr: 0.001000
2020-02-26 20:05:07,280 Epoch 1341: total training loss 0.00085
2020-02-26 20:05:07,280 EPOCH 1342
2020-02-26 20:05:17,023 Epoch 1342: total training loss 0.00086
2020-02-26 20:05:17,023 EPOCH 1343
2020-02-26 20:05:20,221 Epoch 1343 Step:   149000 Batch Loss:     0.000008 Tokens per Sec: 10493891, Lr: 0.001000
2020-02-26 20:05:26,781 Epoch 1343: total training loss 0.00088
2020-02-26 20:05:26,781 EPOCH 1344
2020-02-26 20:05:36,469 Epoch 1344: total training loss 0.00091
2020-02-26 20:05:36,470 EPOCH 1345
2020-02-26 20:05:42,414 Epoch 1345 Step:   149250 Batch Loss:     0.000007 Tokens per Sec: 10348283, Lr: 0.001000
2020-02-26 20:05:46,436 Epoch 1345: total training loss 0.00087
2020-02-26 20:05:46,437 EPOCH 1346
2020-02-26 20:05:56,302 Epoch 1346: total training loss 0.00088
2020-02-26 20:05:56,303 EPOCH 1347
2020-02-26 20:06:04,600 Epoch 1347 Step:   149500 Batch Loss:     0.000009 Tokens per Sec: 10438006, Lr: 0.001000
2020-02-26 20:06:06,156 Epoch 1347: total training loss 0.00087
2020-02-26 20:06:06,156 EPOCH 1348
2020-02-26 20:06:16,038 Epoch 1348: total training loss 0.00089
2020-02-26 20:06:16,038 EPOCH 1349
2020-02-26 20:06:25,991 Epoch 1349: total training loss 0.00087
2020-02-26 20:06:25,992 EPOCH 1350
2020-02-26 20:06:26,963 Epoch 1350 Step:   149750 Batch Loss:     0.000006 Tokens per Sec: 10278897, Lr: 0.001000
2020-02-26 20:06:35,656 Epoch 1350: total training loss 0.00094
2020-02-26 20:06:35,656 EPOCH 1351
2020-02-26 20:06:45,532 Epoch 1351: total training loss 0.00086
2020-02-26 20:06:45,532 EPOCH 1352
2020-02-26 20:06:48,845 Epoch 1352 Step:   150000 Batch Loss:     0.000006 Tokens per Sec: 10491649, Lr: 0.001000
2020-02-26 20:06:48,846 Model noise rate: 5
2020-02-26 20:07:49,450 Validation result at epoch 1352, step   150000: Val DTW Score:  10.67, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0377, GT DTW Score:      nan, duration: 60.6042s
2020-02-26 20:07:56,229 Epoch 1352: total training loss 0.00084
2020-02-26 20:07:56,230 EPOCH 1353
2020-02-26 20:08:06,553 Epoch 1353: total training loss 0.00083
2020-02-26 20:08:06,554 EPOCH 1354
2020-02-26 20:08:12,572 Epoch 1354 Step:   150250 Batch Loss:     0.000005 Tokens per Sec: 10050785, Lr: 0.001000
2020-02-26 20:08:16,686 Epoch 1354: total training loss 0.00084
2020-02-26 20:08:16,686 EPOCH 1355
2020-02-26 20:08:26,529 Epoch 1355: total training loss 0.00084
2020-02-26 20:08:26,530 EPOCH 1356
2020-02-26 20:08:34,829 Epoch 1356 Step:   150500 Batch Loss:     0.000005 Tokens per Sec: 10587460, Lr: 0.001000
2020-02-26 20:08:36,375 Epoch 1356: total training loss 0.00082
2020-02-26 20:08:36,375 EPOCH 1357
2020-02-26 20:08:46,211 Epoch 1357: total training loss 0.00084
2020-02-26 20:08:46,211 EPOCH 1358
2020-02-26 20:08:55,979 Epoch 1358: total training loss 0.00088
2020-02-26 20:08:55,980 EPOCH 1359
2020-02-26 20:08:56,999 Epoch 1359 Step:   150750 Batch Loss:     0.000008 Tokens per Sec: 10374042, Lr: 0.001000
2020-02-26 20:09:05,742 Epoch 1359: total training loss 0.00087
2020-02-26 20:09:05,743 EPOCH 1360
2020-02-26 20:09:15,631 Epoch 1360: total training loss 0.00085
2020-02-26 20:09:15,631 EPOCH 1361
2020-02-26 20:09:19,103 Epoch 1361 Step:   151000 Batch Loss:     0.000008 Tokens per Sec: 10196840, Lr: 0.001000
2020-02-26 20:09:25,519 Epoch 1361: total training loss 0.00093
2020-02-26 20:09:25,520 EPOCH 1362
2020-02-26 20:09:35,552 Epoch 1362: total training loss 0.00087
2020-02-26 20:09:35,552 EPOCH 1363
2020-02-26 20:09:41,733 Epoch 1363 Step:   151250 Batch Loss:     0.000007 Tokens per Sec: 10235980, Lr: 0.001000
2020-02-26 20:09:45,562 Epoch 1363: total training loss 0.00084
2020-02-26 20:09:45,562 EPOCH 1364
2020-02-26 20:09:55,295 Epoch 1364: total training loss 0.00083
2020-02-26 20:09:55,296 EPOCH 1365
2020-02-26 20:10:03,584 Epoch 1365 Step:   151500 Batch Loss:     0.000011 Tokens per Sec: 10690099, Lr: 0.001000
2020-02-26 20:10:04,913 Epoch 1365: total training loss 0.00084
2020-02-26 20:10:04,913 EPOCH 1366
2020-02-26 20:10:14,616 Epoch 1366: total training loss 0.00090
2020-02-26 20:10:14,616 EPOCH 1367
2020-02-26 20:10:24,333 Epoch 1367: total training loss 0.00086
2020-02-26 20:10:24,334 EPOCH 1368
2020-02-26 20:10:25,588 Epoch 1368 Step:   151750 Batch Loss:     0.000005 Tokens per Sec: 10484701, Lr: 0.001000
2020-02-26 20:10:34,139 Epoch 1368: total training loss 0.00086
2020-02-26 20:10:34,140 EPOCH 1369
2020-02-26 20:10:44,008 Epoch 1369: total training loss 0.00091
2020-02-26 20:10:44,008 EPOCH 1370
2020-02-26 20:10:47,510 Epoch 1370 Step:   152000 Batch Loss:     0.000009 Tokens per Sec: 10455969, Lr: 0.001000
2020-02-26 20:10:53,907 Epoch 1370: total training loss 0.00088
2020-02-26 20:10:53,907 EPOCH 1371
2020-02-26 20:11:03,900 Epoch 1371: total training loss 0.00084
2020-02-26 20:11:03,902 EPOCH 1372
2020-02-26 20:11:09,947 Epoch 1372 Step:   152250 Batch Loss:     0.000010 Tokens per Sec: 10258657, Lr: 0.001000
2020-02-26 20:11:13,899 Epoch 1372: total training loss 0.00082
2020-02-26 20:11:13,900 EPOCH 1373
2020-02-26 20:11:23,980 Epoch 1373: total training loss 0.00082
2020-02-26 20:11:23,980 EPOCH 1374
2020-02-26 20:11:32,804 Epoch 1374 Step:   152500 Batch Loss:     0.000012 Tokens per Sec: 10116355, Lr: 0.001000
2020-02-26 20:11:34,150 Epoch 1374: total training loss 0.00099
2020-02-26 20:11:34,150 EPOCH 1375
2020-02-26 20:11:44,215 Epoch 1375: total training loss 0.00088
2020-02-26 20:11:44,215 EPOCH 1376
2020-02-26 20:11:54,003 Epoch 1376: total training loss 0.00084
2020-02-26 20:11:54,003 EPOCH 1377
2020-02-26 20:11:55,228 Epoch 1377 Step:   152750 Batch Loss:     0.000007 Tokens per Sec: 10478731, Lr: 0.001000
2020-02-26 20:12:03,810 Epoch 1377: total training loss 0.00083
2020-02-26 20:12:03,810 EPOCH 1378
2020-02-26 20:12:13,524 Epoch 1378: total training loss 0.00093
2020-02-26 20:12:13,524 EPOCH 1379
2020-02-26 20:12:17,117 Epoch 1379 Step:   153000 Batch Loss:     0.000006 Tokens per Sec: 10448802, Lr: 0.001000
2020-02-26 20:12:23,175 Epoch 1379: total training loss 0.00092
2020-02-26 20:12:23,175 EPOCH 1380
2020-02-26 20:12:32,894 Epoch 1380: total training loss 0.00084
2020-02-26 20:12:32,895 EPOCH 1381
2020-02-26 20:12:39,038 Epoch 1381 Step:   153250 Batch Loss:     0.000006 Tokens per Sec: 10203748, Lr: 0.001000
2020-02-26 20:12:42,949 Epoch 1381: total training loss 0.00084
2020-02-26 20:12:42,949 EPOCH 1382
2020-02-26 20:12:52,857 Epoch 1382: total training loss 0.00083
2020-02-26 20:12:52,858 EPOCH 1383
2020-02-26 20:13:01,567 Epoch 1383 Step:   153500 Batch Loss:     0.000008 Tokens per Sec: 10367076, Lr: 0.001000
2020-02-26 20:13:02,678 Epoch 1383: total training loss 0.00085
2020-02-26 20:13:02,678 EPOCH 1384
2020-02-26 20:13:12,940 Epoch 1384: total training loss 0.00083
2020-02-26 20:13:12,940 EPOCH 1385
2020-02-26 20:13:23,035 Epoch 1385: total training loss 0.00083
2020-02-26 20:13:23,036 EPOCH 1386
2020-02-26 20:13:24,507 Epoch 1386 Step:   153750 Batch Loss:     0.000007 Tokens per Sec: 10300565, Lr: 0.001000
2020-02-26 20:13:33,133 Epoch 1386: total training loss 0.00084
2020-02-26 20:13:33,133 EPOCH 1387
2020-02-26 20:13:43,201 Epoch 1387: total training loss 0.00091
2020-02-26 20:13:43,201 EPOCH 1388
2020-02-26 20:13:47,111 Epoch 1388 Step:   154000 Batch Loss:     0.000008 Tokens per Sec: 10265734, Lr: 0.001000
2020-02-26 20:13:53,042 Epoch 1388: total training loss 0.00087
2020-02-26 20:13:53,042 EPOCH 1389
2020-02-26 20:14:02,882 Epoch 1389: total training loss 0.00085
2020-02-26 20:14:02,882 EPOCH 1390
2020-02-26 20:14:09,124 Epoch 1390 Step:   154250 Batch Loss:     0.000009 Tokens per Sec: 10384192, Lr: 0.001000
2020-02-26 20:14:12,773 Epoch 1390: total training loss 0.00084
2020-02-26 20:14:12,774 EPOCH 1391
2020-02-26 20:14:22,762 Epoch 1391: total training loss 0.00081
2020-02-26 20:14:22,763 EPOCH 1392
2020-02-26 20:14:31,744 Epoch 1392 Step:   154500 Batch Loss:     0.000005 Tokens per Sec: 10238826, Lr: 0.001000
2020-02-26 20:14:32,838 Epoch 1392: total training loss 0.00086
2020-02-26 20:14:32,838 EPOCH 1393
2020-02-26 20:14:42,835 Epoch 1393: total training loss 0.00085
2020-02-26 20:14:42,835 EPOCH 1394
2020-02-26 20:14:52,707 Epoch 1394: total training loss 0.00085
2020-02-26 20:14:52,707 EPOCH 1395
2020-02-26 20:14:54,056 Epoch 1395 Step:   154750 Batch Loss:     0.000005 Tokens per Sec:  9970807, Lr: 0.001000
2020-02-26 20:15:02,446 Epoch 1395: total training loss 0.00085
2020-02-26 20:15:02,446 EPOCH 1396
2020-02-26 20:15:12,147 Epoch 1396: total training loss 0.00083
2020-02-26 20:15:12,148 EPOCH 1397
2020-02-26 20:15:15,974 Epoch 1397 Step:   155000 Batch Loss:     0.000004 Tokens per Sec: 10507566, Lr: 0.001000
2020-02-26 20:15:21,895 Epoch 1397: total training loss 0.00084
2020-02-26 20:15:21,896 EPOCH 1398
2020-02-26 20:15:31,723 Epoch 1398: total training loss 0.00095
2020-02-26 20:15:31,724 EPOCH 1399
2020-02-26 20:15:38,224 Epoch 1399 Step:   155250 Batch Loss:     0.000009 Tokens per Sec: 10456023, Lr: 0.001000
2020-02-26 20:15:41,634 Epoch 1399: total training loss 0.00086
2020-02-26 20:15:41,634 EPOCH 1400
2020-02-26 20:15:51,529 Epoch 1400: total training loss 0.00086
2020-02-26 20:15:51,529 EPOCH 1401
2020-02-26 20:16:00,293 Epoch 1401 Step:   155500 Batch Loss:     0.000004 Tokens per Sec: 10554988, Lr: 0.001000
2020-02-26 20:16:01,266 Epoch 1401: total training loss 0.00082
2020-02-26 20:16:01,266 EPOCH 1402
2020-02-26 20:16:11,009 Epoch 1402: total training loss 0.00083
2020-02-26 20:16:11,009 EPOCH 1403
2020-02-26 20:16:20,754 Epoch 1403: total training loss 0.00082
2020-02-26 20:16:20,754 EPOCH 1404
2020-02-26 20:16:22,281 Epoch 1404 Step:   155750 Batch Loss:     0.000008 Tokens per Sec: 10219171, Lr: 0.001000
2020-02-26 20:16:30,569 Epoch 1404: total training loss 0.00082
2020-02-26 20:16:30,570 EPOCH 1405
2020-02-26 20:16:40,761 Epoch 1405: total training loss 0.00081
2020-02-26 20:16:40,762 EPOCH 1406
2020-02-26 20:16:44,804 Epoch 1406 Step:   156000 Batch Loss:     0.000009 Tokens per Sec: 10333019, Lr: 0.001000
2020-02-26 20:16:50,728 Epoch 1406: total training loss 0.00086
2020-02-26 20:16:50,728 EPOCH 1407
2020-02-26 20:17:00,678 Epoch 1407: total training loss 0.00089
2020-02-26 20:17:00,679 EPOCH 1408
2020-02-26 20:17:07,116 Epoch 1408 Step:   156250 Batch Loss:     0.000005 Tokens per Sec: 10309464, Lr: 0.001000
2020-02-26 20:17:10,521 Epoch 1408: total training loss 0.00083
2020-02-26 20:17:10,521 EPOCH 1409
2020-02-26 20:17:20,333 Epoch 1409: total training loss 0.00083
2020-02-26 20:17:20,334 EPOCH 1410
2020-02-26 20:17:29,292 Epoch 1410 Step:   156500 Batch Loss:     0.000008 Tokens per Sec: 10470717, Lr: 0.001000
2020-02-26 20:17:30,213 Epoch 1410: total training loss 0.00087
2020-02-26 20:17:30,213 EPOCH 1411
2020-02-26 20:17:39,955 Epoch 1411: total training loss 0.00086
2020-02-26 20:17:39,955 EPOCH 1412
2020-02-26 20:17:49,732 Epoch 1412: total training loss 0.00086
2020-02-26 20:17:49,732 EPOCH 1413
2020-02-26 20:17:51,261 Epoch 1413 Step:   156750 Batch Loss:     0.000010 Tokens per Sec: 10356569, Lr: 0.001000
2020-02-26 20:17:59,422 Epoch 1413: total training loss 0.00084
2020-02-26 20:17:59,422 EPOCH 1414
2020-02-26 20:18:09,229 Epoch 1414: total training loss 0.00082
2020-02-26 20:18:09,230 EPOCH 1415
2020-02-26 20:18:13,306 Epoch 1415 Step:   157000 Batch Loss:     0.000006 Tokens per Sec: 10189350, Lr: 0.001000
2020-02-26 20:18:19,266 Epoch 1415: total training loss 0.00091
2020-02-26 20:18:19,267 EPOCH 1416
2020-02-26 20:18:29,630 Epoch 1416: total training loss 0.00085
2020-02-26 20:18:29,630 EPOCH 1417
2020-02-26 20:18:36,444 Epoch 1417 Step:   157250 Batch Loss:     0.000011 Tokens per Sec:  9967287, Lr: 0.001000
2020-02-26 20:18:39,907 Epoch 1417: total training loss 0.00082
2020-02-26 20:18:39,907 EPOCH 1418
2020-02-26 20:18:50,299 Epoch 1418: total training loss 0.00087
2020-02-26 20:18:50,300 EPOCH 1419
2020-02-26 20:18:59,698 Epoch 1419 Step:   157500 Batch Loss:     0.000007 Tokens per Sec: 10068879, Lr: 0.001000
2020-02-26 20:19:00,567 Epoch 1419: total training loss 0.00087
2020-02-26 20:19:00,567 EPOCH 1420
2020-02-26 20:19:10,492 Epoch 1420: total training loss 0.00086
2020-02-26 20:19:10,493 EPOCH 1421
2020-02-26 20:19:20,382 Epoch 1421: total training loss 0.00085
2020-02-26 20:19:20,382 EPOCH 1422
2020-02-26 20:19:21,977 Epoch 1422 Step:   157750 Batch Loss:     0.000007 Tokens per Sec: 10351626, Lr: 0.001000
2020-02-26 20:19:30,240 Epoch 1422: total training loss 0.00087
2020-02-26 20:19:30,241 EPOCH 1423
2020-02-26 20:19:40,084 Epoch 1423: total training loss 0.00082
2020-02-26 20:19:40,084 EPOCH 1424
2020-02-26 20:19:44,181 Epoch 1424 Step:   158000 Batch Loss:     0.000007 Tokens per Sec: 10598679, Lr: 0.001000
2020-02-26 20:19:49,737 Epoch 1424: total training loss 0.00085
2020-02-26 20:19:49,737 EPOCH 1425
2020-02-26 20:19:59,361 Epoch 1425: total training loss 0.00082
2020-02-26 20:19:59,361 EPOCH 1426
2020-02-26 20:20:05,845 Epoch 1426 Step:   158250 Batch Loss:     0.000004 Tokens per Sec: 10692541, Lr: 0.001000
2020-02-26 20:20:09,011 Epoch 1426: total training loss 0.00082
2020-02-26 20:20:09,012 EPOCH 1427
2020-02-26 20:20:18,783 Epoch 1427: total training loss 0.00082
2020-02-26 20:20:18,784 EPOCH 1428
2020-02-26 20:20:27,771 Epoch 1428 Step:   158500 Batch Loss:     0.000005 Tokens per Sec: 10540156, Lr: 0.001000
2020-02-26 20:20:28,515 Epoch 1428: total training loss 0.00081
2020-02-26 20:20:28,515 EPOCH 1429
2020-02-26 20:20:38,340 Epoch 1429: total training loss 0.00085
2020-02-26 20:20:38,341 EPOCH 1430
2020-02-26 20:20:48,423 Epoch 1430: total training loss 0.00085
2020-02-26 20:20:48,426 EPOCH 1431
2020-02-26 20:20:50,213 Epoch 1431 Step:   158750 Batch Loss:     0.000014 Tokens per Sec:  9767051, Lr: 0.001000
2020-02-26 20:20:58,447 Epoch 1431: total training loss 0.00085
2020-02-26 20:20:58,447 EPOCH 1432
2020-02-26 20:21:08,364 Epoch 1432: total training loss 0.00082
2020-02-26 20:21:08,365 EPOCH 1433
2020-02-26 20:21:12,679 Epoch 1433 Step:   159000 Batch Loss:     0.000008 Tokens per Sec: 10303305, Lr: 0.001000
2020-02-26 20:21:18,389 Epoch 1433: total training loss 0.00086
2020-02-26 20:21:18,390 EPOCH 1434
2020-02-26 20:21:28,367 Epoch 1434: total training loss 0.00089
2020-02-26 20:21:28,367 EPOCH 1435
2020-02-26 20:21:35,047 Epoch 1435 Step:   159250 Batch Loss:     0.000004 Tokens per Sec: 10576188, Lr: 0.001000
2020-02-26 20:21:38,111 Epoch 1435: total training loss 0.00083
2020-02-26 20:21:38,112 EPOCH 1436
2020-02-26 20:21:47,824 Epoch 1436: total training loss 0.00084
2020-02-26 20:21:47,825 EPOCH 1437
2020-02-26 20:21:57,129 Epoch 1437 Step:   159500 Batch Loss:     0.000009 Tokens per Sec: 10341050, Lr: 0.001000
2020-02-26 20:21:57,755 Epoch 1437: total training loss 0.00087
2020-02-26 20:21:57,755 EPOCH 1438
2020-02-26 20:22:07,730 Epoch 1438: total training loss 0.00090
2020-02-26 20:22:07,730 EPOCH 1439
2020-02-26 20:22:17,586 Epoch 1439: total training loss 0.00086
2020-02-26 20:22:17,586 EPOCH 1440
2020-02-26 20:22:19,403 Epoch 1440 Step:   159750 Batch Loss:     0.000007 Tokens per Sec: 10231251, Lr: 0.001000
2020-02-26 20:22:27,593 Epoch 1440: total training loss 0.00085
2020-02-26 20:22:27,593 EPOCH 1441
2020-02-26 20:22:37,375 Epoch 1441: total training loss 0.00083
2020-02-26 20:22:37,375 EPOCH 1442
2020-02-26 20:22:41,708 Epoch 1442 Step:   160000 Batch Loss:     0.000007 Tokens per Sec: 10549468, Lr: 0.001000
2020-02-26 20:22:41,709 Model noise rate: 5
2020-02-26 20:23:43,717 Validation result at epoch 1442, step   160000: Val DTW Score:  10.65, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0386, GT DTW Score:      nan, duration: 62.0086s
2020-02-26 20:23:49,471 Epoch 1442: total training loss 0.00082
2020-02-26 20:23:49,471 EPOCH 1443
2020-02-26 20:23:59,788 Epoch 1443: total training loss 0.00084
2020-02-26 20:23:59,789 EPOCH 1444
2020-02-26 20:24:06,748 Epoch 1444 Step:   160250 Batch Loss:     0.000005 Tokens per Sec: 10200468, Lr: 0.001000
2020-02-26 20:24:09,759 Epoch 1444: total training loss 0.00084
2020-02-26 20:24:09,759 EPOCH 1445
2020-02-26 20:24:19,556 Epoch 1445: total training loss 0.00086
2020-02-26 20:24:19,556 EPOCH 1446
2020-02-26 20:24:28,834 Epoch 1446 Step:   160500 Batch Loss:     0.000010 Tokens per Sec: 10475901, Lr: 0.001000
2020-02-26 20:24:29,318 Epoch 1446: total training loss 0.00090
2020-02-26 20:24:29,318 EPOCH 1447
2020-02-26 20:24:39,140 Epoch 1447: total training loss 0.00087
2020-02-26 20:24:39,141 EPOCH 1448
2020-02-26 20:24:48,891 Epoch 1448: total training loss 0.00085
2020-02-26 20:24:48,891 EPOCH 1449
2020-02-26 20:24:50,838 Epoch 1449 Step:   160750 Batch Loss:     0.000006 Tokens per Sec: 10650207, Lr: 0.001000
2020-02-26 20:24:58,543 Epoch 1449: total training loss 0.00082
2020-02-26 20:24:58,543 EPOCH 1450
2020-02-26 20:25:08,294 Epoch 1450: total training loss 0.00082
2020-02-26 20:25:08,350 EPOCH 1451
2020-02-26 20:25:12,806 Epoch 1451 Step:   161000 Batch Loss:     0.000004 Tokens per Sec: 10186446, Lr: 0.001000
2020-02-26 20:25:18,441 Epoch 1451: total training loss 0.00082
2020-02-26 20:25:18,441 EPOCH 1452
2020-02-26 20:25:28,373 Epoch 1452: total training loss 0.00082
2020-02-26 20:25:28,373 EPOCH 1453
2020-02-26 20:25:35,436 Epoch 1453 Step:   161250 Batch Loss:     0.000009 Tokens per Sec: 10239192, Lr: 0.001000
2020-02-26 20:25:38,362 Epoch 1453: total training loss 0.00088
2020-02-26 20:25:38,362 EPOCH 1454
2020-02-26 20:25:48,619 Epoch 1454: total training loss 0.00082
2020-02-26 20:25:48,619 EPOCH 1455
2020-02-26 20:25:58,271 Epoch 1455 Step:   161500 Batch Loss:     0.000008 Tokens per Sec: 10238195, Lr: 0.001000
2020-02-26 20:25:58,744 Epoch 1455: total training loss 0.00081
2020-02-26 20:25:58,744 EPOCH 1456
2020-02-26 20:26:08,651 Epoch 1456: total training loss 0.00080
2020-02-26 20:26:08,651 EPOCH 1457
2020-02-26 20:26:18,636 Epoch 1457: total training loss 0.00080
2020-02-26 20:26:18,637 EPOCH 1458
2020-02-26 20:26:20,677 Epoch 1458 Step:   161750 Batch Loss:     0.000010 Tokens per Sec: 10157285, Lr: 0.001000
2020-02-26 20:26:28,515 Epoch 1458: total training loss 0.00083
2020-02-26 20:26:28,515 EPOCH 1459
2020-02-26 20:26:38,296 Epoch 1459: total training loss 0.00081
2020-02-26 20:26:38,296 EPOCH 1460
2020-02-26 20:26:42,747 Epoch 1460 Step:   162000 Batch Loss:     0.000009 Tokens per Sec: 10520745, Lr: 0.001000
2020-02-26 20:26:48,075 Epoch 1460: total training loss 0.00084
2020-02-26 20:26:48,108 EPOCH 1461
2020-02-26 20:26:58,048 Epoch 1461: total training loss 0.00089
2020-02-26 20:26:58,048 EPOCH 1462
2020-02-26 20:27:05,137 Epoch 1462 Step:   162250 Batch Loss:     0.000008 Tokens per Sec: 10415006, Lr: 0.001000
2020-02-26 20:27:07,941 Epoch 1462: total training loss 0.00082
2020-02-26 20:27:07,942 EPOCH 1463
2020-02-26 20:27:17,822 Epoch 1463: total training loss 0.00081
2020-02-26 20:27:17,823 EPOCH 1464
2020-02-26 20:27:27,477 Epoch 1464 Step:   162500 Batch Loss:     0.000006 Tokens per Sec: 10206315, Lr: 0.001000
2020-02-26 20:27:27,904 Epoch 1464: total training loss 0.00083
2020-02-26 20:27:27,904 EPOCH 1465
2020-02-26 20:27:37,834 Epoch 1465: total training loss 0.00084
2020-02-26 20:27:37,835 EPOCH 1466
2020-02-26 20:27:47,501 Epoch 1466: total training loss 0.00087
2020-02-26 20:27:47,501 EPOCH 1467
2020-02-26 20:27:49,597 Epoch 1467 Step:   162750 Batch Loss:     0.000005 Tokens per Sec: 10594725, Lr: 0.001000
2020-02-26 20:27:57,173 Epoch 1467: total training loss 0.00086
2020-02-26 20:27:57,174 EPOCH 1468
2020-02-26 20:28:06,956 Epoch 1468: total training loss 0.00083
2020-02-26 20:28:06,957 EPOCH 1469
2020-02-26 20:28:11,460 Epoch 1469 Step:   163000 Batch Loss:     0.000008 Tokens per Sec: 10525249, Lr: 0.001000
2020-02-26 20:28:16,757 Epoch 1469: total training loss 0.00081
2020-02-26 20:28:16,757 EPOCH 1470
2020-02-26 20:28:26,842 Epoch 1470: total training loss 0.00082
2020-02-26 20:28:26,843 EPOCH 1471
2020-02-26 20:28:34,324 Epoch 1471 Step:   163250 Batch Loss:     0.000004 Tokens per Sec:  9970733, Lr: 0.001000
2020-02-26 20:28:37,127 Epoch 1471: total training loss 0.00086
2020-02-26 20:28:37,128 EPOCH 1472
2020-02-26 20:28:47,397 Epoch 1472: total training loss 0.00082
2020-02-26 20:28:47,398 EPOCH 1473
2020-02-26 20:28:57,342 Epoch 1473 Step:   163500 Batch Loss:     0.000008 Tokens per Sec: 10134191, Lr: 0.001000
2020-02-26 20:28:57,541 Epoch 1473: total training loss 0.00095
2020-02-26 20:28:57,541 EPOCH 1474
2020-02-26 20:29:07,921 Epoch 1474: total training loss 0.00083
2020-02-26 20:29:07,921 EPOCH 1475
2020-02-26 20:29:18,069 Epoch 1475: total training loss 0.00081
2020-02-26 20:29:18,070 EPOCH 1476
2020-02-26 20:29:20,415 Epoch 1476 Step:   163750 Batch Loss:     0.000012 Tokens per Sec: 10649815, Lr: 0.001000
2020-02-26 20:29:27,934 Epoch 1476: total training loss 0.00084
2020-02-26 20:29:27,934 EPOCH 1477
2020-02-26 20:29:37,732 Epoch 1477: total training loss 0.00086
2020-02-26 20:29:37,733 EPOCH 1478
2020-02-26 20:29:42,264 Epoch 1478 Step:   164000 Batch Loss:     0.000007 Tokens per Sec: 10499977, Lr: 0.001000
2020-02-26 20:29:47,440 Epoch 1478: total training loss 0.00083
2020-02-26 20:29:47,440 EPOCH 1479
2020-02-26 20:29:57,019 Epoch 1479: total training loss 0.00081
2020-02-26 20:29:57,019 EPOCH 1480
2020-02-26 20:30:04,034 Epoch 1480 Step:   164250 Batch Loss:     0.000013 Tokens per Sec: 10667081, Lr: 0.001000
2020-02-26 20:30:06,660 Epoch 1480: total training loss 0.00089
2020-02-26 20:30:06,661 EPOCH 1481
2020-02-26 20:30:16,445 Epoch 1481: total training loss 0.00101
2020-02-26 20:30:16,446 EPOCH 1482
2020-02-26 20:30:26,105 Epoch 1482 Step:   164500 Batch Loss:     0.000009 Tokens per Sec: 10390289, Lr: 0.001000
2020-02-26 20:30:26,312 Epoch 1482: total training loss 0.00083
2020-02-26 20:30:26,313 EPOCH 1483
2020-02-26 20:30:36,253 Epoch 1483: total training loss 0.00082
2020-02-26 20:30:36,253 EPOCH 1484
2020-02-26 20:30:46,248 Epoch 1484: total training loss 0.00096
2020-02-26 20:30:46,250 EPOCH 1485
2020-02-26 20:30:48,639 Epoch 1485 Step:   164750 Batch Loss:     0.000006 Tokens per Sec: 10480731, Lr: 0.001000
2020-02-26 20:30:56,051 Epoch 1485: total training loss 0.00083
2020-02-26 20:30:56,052 EPOCH 1486
2020-02-26 20:31:06,068 Epoch 1486: total training loss 0.00080
2020-02-26 20:31:06,069 EPOCH 1487
2020-02-26 20:31:10,900 Epoch 1487 Step:   165000 Batch Loss:     0.000011 Tokens per Sec: 10330025, Lr: 0.001000
2020-02-26 20:31:16,049 Epoch 1487: total training loss 0.00080
2020-02-26 20:31:16,049 EPOCH 1488
2020-02-26 20:31:26,054 Epoch 1488: total training loss 0.00083
2020-02-26 20:31:26,054 EPOCH 1489
2020-02-26 20:31:33,359 Epoch 1489 Step:   165250 Batch Loss:     0.000007 Tokens per Sec: 10272526, Lr: 0.001000
2020-02-26 20:31:35,955 Epoch 1489: total training loss 0.00084
2020-02-26 20:31:35,955 EPOCH 1490
2020-02-26 20:31:45,667 Epoch 1490: total training loss 0.00081
2020-02-26 20:31:45,668 EPOCH 1491
2020-02-26 20:31:55,329 Epoch 1491 Step:   165500 Batch Loss:     0.000006 Tokens per Sec: 10502208, Lr: 0.001000
2020-02-26 20:31:55,447 Epoch 1491: total training loss 0.00080
2020-02-26 20:31:55,447 EPOCH 1492
2020-02-26 20:32:05,465 Epoch 1492: total training loss 0.00080
2020-02-26 20:32:05,465 EPOCH 1493
2020-02-26 20:32:15,576 Epoch 1493: total training loss 0.00081
2020-02-26 20:32:15,576 EPOCH 1494
2020-02-26 20:32:18,111 Epoch 1494 Step:   165750 Batch Loss:     0.000007 Tokens per Sec: 10226342, Lr: 0.001000
2020-02-26 20:32:25,529 Epoch 1494: total training loss 0.00080
2020-02-26 20:32:25,529 EPOCH 1495
2020-02-26 20:32:35,575 Epoch 1495: total training loss 0.00078
2020-02-26 20:32:35,576 EPOCH 1496
2020-02-26 20:32:40,458 Epoch 1496 Step:   166000 Batch Loss:     0.000006 Tokens per Sec: 10490358, Lr: 0.001000
2020-02-26 20:32:45,288 Epoch 1496: total training loss 0.00083
2020-02-26 20:32:45,289 EPOCH 1497
2020-02-26 20:32:55,024 Epoch 1497: total training loss 0.00084
2020-02-26 20:32:55,024 EPOCH 1498
2020-02-26 20:33:02,475 Epoch 1498 Step:   166250 Batch Loss:     0.000011 Tokens per Sec: 10565552, Lr: 0.001000
2020-02-26 20:33:04,828 Epoch 1498: total training loss 0.00080
2020-02-26 20:33:04,828 EPOCH 1499
2020-02-26 20:33:14,618 Epoch 1499: total training loss 0.00082
2020-02-26 20:33:14,619 EPOCH 1500
2020-02-26 20:33:24,451 Epoch 1500 Step:   166500 Batch Loss:     0.000008 Tokens per Sec: 10448065, Lr: 0.001000
2020-02-26 20:33:24,453 Epoch 1500: total training loss 0.00087
2020-02-26 20:33:24,453 EPOCH 1501
2020-02-26 20:33:34,416 Epoch 1501: total training loss 0.00107
2020-02-26 20:33:34,418 EPOCH 1502
2020-02-26 20:33:44,652 Epoch 1502: total training loss 0.00084
2020-02-26 20:33:44,653 EPOCH 1503
2020-02-26 20:33:47,461 Epoch 1503 Step:   166750 Batch Loss:     0.000008 Tokens per Sec:  9766338, Lr: 0.001000
2020-02-26 20:33:54,940 Epoch 1503: total training loss 0.00082
2020-02-26 20:33:54,941 EPOCH 1504
2020-02-26 20:34:05,354 Epoch 1504: total training loss 0.00079
2020-02-26 20:34:05,354 EPOCH 1505
2020-02-26 20:34:10,701 Epoch 1505 Step:   167000 Batch Loss:     0.000007 Tokens per Sec:  9829273, Lr: 0.001000
2020-02-26 20:34:15,562 Epoch 1505: total training loss 0.00079
2020-02-26 20:34:15,562 EPOCH 1506
2020-02-26 20:34:25,429 Epoch 1506: total training loss 0.00079
2020-02-26 20:34:25,429 EPOCH 1507
2020-02-26 20:34:32,777 Epoch 1507 Step:   167250 Batch Loss:     0.000007 Tokens per Sec: 10478264, Lr: 0.001000
2020-02-26 20:34:35,148 Epoch 1507: total training loss 0.00082
2020-02-26 20:34:35,148 EPOCH 1508
2020-02-26 20:34:44,821 Epoch 1508: total training loss 0.00081
2020-02-26 20:34:44,821 EPOCH 1509
2020-02-26 20:34:54,527 Epoch 1509: total training loss 0.00080
2020-02-26 20:34:54,527 EPOCH 1510
2020-02-26 20:34:54,656 Epoch 1510 Step:   167500 Batch Loss:     0.000010 Tokens per Sec:  9460837, Lr: 0.001000
2020-02-26 20:35:04,295 Epoch 1510: total training loss 0.00078
2020-02-26 20:35:04,296 EPOCH 1511
2020-02-26 20:35:14,127 Epoch 1511: total training loss 0.00081
2020-02-26 20:35:14,128 EPOCH 1512
2020-02-26 20:35:16,745 Epoch 1512 Step:   167750 Batch Loss:     0.000006 Tokens per Sec: 10352334, Lr: 0.001000
2020-02-26 20:35:24,131 Epoch 1512: total training loss 0.00080
2020-02-26 20:35:24,132 EPOCH 1513
2020-02-26 20:35:34,035 Epoch 1513: total training loss 0.00081
2020-02-26 20:35:34,035 EPOCH 1514
2020-02-26 20:35:39,360 Epoch 1514 Step:   168000 Batch Loss:     0.000006 Tokens per Sec: 10435359, Lr: 0.001000
2020-02-26 20:35:43,853 Epoch 1514: total training loss 0.00082
2020-02-26 20:35:43,853 EPOCH 1515
2020-02-26 20:35:53,960 Epoch 1515: total training loss 0.00080
2020-02-26 20:35:53,962 EPOCH 1516
2020-02-26 20:36:01,689 Epoch 1516 Step:   168250 Batch Loss:     0.000007 Tokens per Sec: 10168592, Lr: 0.001000
2020-02-26 20:36:04,058 Epoch 1516: total training loss 0.00088
2020-02-26 20:36:04,059 EPOCH 1517
2020-02-26 20:36:14,186 Epoch 1517: total training loss 0.00084
2020-02-26 20:36:14,187 EPOCH 1518
2020-02-26 20:36:24,192 Epoch 1518: total training loss 0.00080
2020-02-26 20:36:24,192 EPOCH 1519
2020-02-26 20:36:24,365 Epoch 1519 Step:   168500 Batch Loss:     0.000005 Tokens per Sec:  8012960, Lr: 0.001000
2020-02-26 20:36:33,975 Epoch 1519: total training loss 0.00080
2020-02-26 20:36:33,975 EPOCH 1520
2020-02-26 20:36:43,700 Epoch 1520: total training loss 0.00084
2020-02-26 20:36:43,701 EPOCH 1521
2020-02-26 20:36:46,278 Epoch 1521 Step:   168750 Batch Loss:     0.000009 Tokens per Sec: 10558916, Lr: 0.001000
2020-02-26 20:36:53,365 Epoch 1521: total training loss 0.00080
2020-02-26 20:36:53,366 EPOCH 1522
2020-02-26 20:37:03,187 Epoch 1522: total training loss 0.00079
2020-02-26 20:37:03,187 EPOCH 1523
2020-02-26 20:37:08,271 Epoch 1523 Step:   169000 Batch Loss:     0.000005 Tokens per Sec: 10654981, Lr: 0.001000
2020-02-26 20:37:12,974 Epoch 1523: total training loss 0.00079
2020-02-26 20:37:12,974 EPOCH 1524
2020-02-26 20:37:22,739 Epoch 1524: total training loss 0.00081
2020-02-26 20:37:22,740 EPOCH 1525
2020-02-26 20:37:30,380 Epoch 1525 Step:   169250 Batch Loss:     0.000007 Tokens per Sec: 10602604, Lr: 0.001000
2020-02-26 20:37:32,492 Epoch 1525: total training loss 0.00082
2020-02-26 20:37:32,492 EPOCH 1526
2020-02-26 20:37:42,193 Epoch 1526: total training loss 0.00085
2020-02-26 20:37:42,194 EPOCH 1527
2020-02-26 20:37:52,058 Epoch 1527: total training loss 0.00081
2020-02-26 20:37:52,059 EPOCH 1528
2020-02-26 20:37:52,342 Epoch 1528 Step:   169500 Batch Loss:     0.000009 Tokens per Sec:  9208078, Lr: 0.001000
2020-02-26 20:38:01,962 Epoch 1528: total training loss 0.00079
2020-02-26 20:38:01,962 EPOCH 1529
2020-02-26 20:38:11,889 Epoch 1529: total training loss 0.00082
2020-02-26 20:38:11,890 EPOCH 1530
2020-02-26 20:38:14,730 Epoch 1530 Step:   169750 Batch Loss:     0.000008 Tokens per Sec: 10301559, Lr: 0.001000
2020-02-26 20:38:21,893 Epoch 1530: total training loss 0.00084
2020-02-26 20:38:21,894 EPOCH 1531
2020-02-26 20:38:31,807 Epoch 1531: total training loss 0.00082
2020-02-26 20:38:31,807 EPOCH 1532
2020-02-26 20:38:37,027 Epoch 1532 Step:   170000 Batch Loss:     0.000007 Tokens per Sec: 10555167, Lr: 0.001000
2020-02-26 20:38:37,027 Model noise rate: 5
2020-02-26 20:39:38,005 Validation result at epoch 1532, step   170000: Val DTW Score:  10.65, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0385, GT DTW Score:      nan, duration: 60.9775s
2020-02-26 20:39:42,884 Epoch 1532: total training loss 0.00082
2020-02-26 20:39:42,885 EPOCH 1533
2020-02-26 20:39:52,952 Epoch 1533: total training loss 0.00081
2020-02-26 20:39:52,953 EPOCH 1534
2020-02-26 20:40:00,949 Epoch 1534 Step:   170250 Batch Loss:     0.000007 Tokens per Sec: 10190983, Lr: 0.001000
2020-02-26 20:40:03,008 Epoch 1534: total training loss 0.00083
2020-02-26 20:40:03,008 EPOCH 1535
2020-02-26 20:40:13,031 Epoch 1535: total training loss 0.00085
2020-02-26 20:40:13,032 EPOCH 1536
2020-02-26 20:40:22,779 Epoch 1536: total training loss 0.00086
2020-02-26 20:40:22,780 EPOCH 1537
2020-02-26 20:40:23,187 Epoch 1537 Step:   170500 Batch Loss:     0.000009 Tokens per Sec: 10215250, Lr: 0.001000
2020-02-26 20:40:32,725 Epoch 1537: total training loss 0.00083
2020-02-26 20:40:32,725 EPOCH 1538
2020-02-26 20:40:42,552 Epoch 1538: total training loss 0.00080
2020-02-26 20:40:42,553 EPOCH 1539
2020-02-26 20:40:45,563 Epoch 1539 Step:   170750 Batch Loss:     0.000011 Tokens per Sec: 10413052, Lr: 0.001000
2020-02-26 20:40:52,437 Epoch 1539: total training loss 0.00081
2020-02-26 20:40:52,438 EPOCH 1540
2020-02-26 20:41:02,245 Epoch 1540: total training loss 0.00080
2020-02-26 20:41:02,245 EPOCH 1541
2020-02-26 20:41:07,626 Epoch 1541 Step:   171000 Batch Loss:     0.000005 Tokens per Sec: 10448502, Lr: 0.001000
2020-02-26 20:41:12,159 Epoch 1541: total training loss 0.00078
2020-02-26 20:41:12,159 EPOCH 1542
2020-02-26 20:41:22,177 Epoch 1542: total training loss 0.00077
2020-02-26 20:41:22,177 EPOCH 1543
2020-02-26 20:41:30,001 Epoch 1543 Step:   171250 Batch Loss:     0.000008 Tokens per Sec: 10163682, Lr: 0.001000
2020-02-26 20:41:32,131 Epoch 1543: total training loss 0.00084
2020-02-26 20:41:32,131 EPOCH 1544
2020-02-26 20:41:42,291 Epoch 1544: total training loss 0.00083
2020-02-26 20:41:42,293 EPOCH 1545
2020-02-26 20:41:52,595 Epoch 1545: total training loss 0.00090
2020-02-26 20:41:52,596 EPOCH 1546
2020-02-26 20:41:53,145 Epoch 1546 Step:   171500 Batch Loss:     0.000012 Tokens per Sec:  9489635, Lr: 0.001000
2020-02-26 20:42:02,621 Epoch 1546: total training loss 0.00083
2020-02-26 20:42:02,621 EPOCH 1547
2020-02-26 20:42:12,562 Epoch 1547: total training loss 0.00082
2020-02-26 20:42:12,563 EPOCH 1548
2020-02-26 20:42:15,379 Epoch 1548 Step:   171750 Batch Loss:     0.000006 Tokens per Sec: 10290425, Lr: 0.001000
2020-02-26 20:42:22,378 Epoch 1548: total training loss 0.00085
2020-02-26 20:42:22,379 EPOCH 1549
2020-02-26 20:42:32,094 Epoch 1549: total training loss 0.00084
2020-02-26 20:42:32,095 EPOCH 1550
2020-02-26 20:42:37,308 Epoch 1550 Step:   172000 Batch Loss:     0.000006 Tokens per Sec: 10370868, Lr: 0.001000
2020-02-26 20:42:42,041 Epoch 1550: total training loss 0.00079
2020-02-26 20:42:42,041 EPOCH 1551
2020-02-26 20:42:52,044 Epoch 1551: total training loss 0.00078
2020-02-26 20:42:52,044 EPOCH 1552
2020-02-26 20:42:59,940 Epoch 1552 Step:   172250 Batch Loss:     0.000006 Tokens per Sec: 10346009, Lr: 0.001000
2020-02-26 20:43:01,931 Epoch 1552: total training loss 0.00078
2020-02-26 20:43:01,932 EPOCH 1553
2020-02-26 20:43:11,671 Epoch 1553: total training loss 0.00079
2020-02-26 20:43:11,672 EPOCH 1554
2020-02-26 20:43:21,536 Epoch 1554: total training loss 0.00078
2020-02-26 20:43:21,537 EPOCH 1555
2020-02-26 20:43:22,128 Epoch 1555 Step:   172500 Batch Loss:     0.000007 Tokens per Sec: 10244702, Lr: 0.001000
2020-02-26 20:43:31,272 Epoch 1555: total training loss 0.00082
2020-02-26 20:43:31,272 EPOCH 1556
2020-02-26 20:43:41,197 Epoch 1556: total training loss 0.00078
2020-02-26 20:43:41,197 EPOCH 1557
2020-02-26 20:43:44,375 Epoch 1557 Step:   172750 Batch Loss:     0.000004 Tokens per Sec: 10302996, Lr: 0.001000
2020-02-26 20:43:51,199 Epoch 1557: total training loss 0.00078
2020-02-26 20:43:51,200 EPOCH 1558
2020-02-26 20:44:01,061 Epoch 1558: total training loss 0.00079
2020-02-26 20:44:01,062 EPOCH 1559
2020-02-26 20:44:06,579 Epoch 1559 Step:   173000 Batch Loss:     0.000007 Tokens per Sec: 10395022, Lr: 0.001000
2020-02-26 20:44:10,909 Epoch 1559: total training loss 0.00080
2020-02-26 20:44:10,909 EPOCH 1560
2020-02-26 20:44:21,010 Epoch 1560: total training loss 0.00081
2020-02-26 20:44:21,011 EPOCH 1561
2020-02-26 20:44:29,325 Epoch 1561 Step:   173250 Batch Loss:     0.000008 Tokens per Sec:  9996229, Lr: 0.001000
2020-02-26 20:44:31,247 Epoch 1561: total training loss 0.00079
2020-02-26 20:44:31,247 EPOCH 1562
2020-02-26 20:44:41,665 Epoch 1562: total training loss 0.00077
2020-02-26 20:44:41,665 EPOCH 1563
2020-02-26 20:44:52,234 Epoch 1563: total training loss 0.00078
2020-02-26 20:44:52,234 EPOCH 1564
2020-02-26 20:44:52,934 Epoch 1564 Step:   173500 Batch Loss:     0.000010 Tokens per Sec:  9627996, Lr: 0.001000
2020-02-26 20:45:02,486 Epoch 1564: total training loss 0.00085
2020-02-26 20:45:02,487 EPOCH 1565
2020-02-26 20:45:12,756 Epoch 1565: total training loss 0.00086
2020-02-26 20:45:12,756 EPOCH 1566
2020-02-26 20:45:15,942 Epoch 1566 Step:   173750 Batch Loss:     0.000005 Tokens per Sec: 10160308, Lr: 0.001000
2020-02-26 20:45:22,684 Epoch 1566: total training loss 0.00084
2020-02-26 20:45:22,685 EPOCH 1567
2020-02-26 20:45:32,512 Epoch 1567: total training loss 0.00086
2020-02-26 20:45:32,513 EPOCH 1568
2020-02-26 20:45:38,240 Epoch 1568 Step:   174000 Batch Loss:     0.000006 Tokens per Sec: 10394212, Lr: 0.001000
2020-02-26 20:45:42,470 Epoch 1568: total training loss 0.00080
2020-02-26 20:45:42,471 EPOCH 1569
2020-02-26 20:45:52,312 Epoch 1569: total training loss 0.00080
2020-02-26 20:45:52,313 EPOCH 1570
2020-02-26 20:46:00,341 Epoch 1570 Step:   174250 Batch Loss:     0.000006 Tokens per Sec: 10536603, Lr: 0.001000
2020-02-26 20:46:02,086 Epoch 1570: total training loss 0.00082
2020-02-26 20:46:02,086 EPOCH 1571
2020-02-26 20:46:12,044 Epoch 1571: total training loss 0.00080
2020-02-26 20:46:12,045 EPOCH 1572
2020-02-26 20:46:22,008 Epoch 1572: total training loss 0.00083
2020-02-26 20:46:22,009 EPOCH 1573
2020-02-26 20:46:22,879 Epoch 1573 Step:   174500 Batch Loss:     0.000008 Tokens per Sec: 10084016, Lr: 0.001000
2020-02-26 20:46:31,895 Epoch 1573: total training loss 0.00091
2020-02-26 20:46:31,896 EPOCH 1574
2020-02-26 20:46:41,974 Epoch 1574: total training loss 0.00080
2020-02-26 20:46:41,975 EPOCH 1575
2020-02-26 20:46:45,260 Epoch 1575 Step:   174750 Batch Loss:     0.000007 Tokens per Sec: 10291173, Lr: 0.001000
2020-02-26 20:46:52,014 Epoch 1575: total training loss 0.00088
2020-02-26 20:46:52,015 EPOCH 1576
2020-02-26 20:47:02,016 Epoch 1576: total training loss 0.00084
2020-02-26 20:47:02,017 EPOCH 1577
2020-02-26 20:47:07,601 Epoch 1577 Step:   175000 Batch Loss:     0.000008 Tokens per Sec: 10324121, Lr: 0.001000
2020-02-26 20:47:11,961 Epoch 1577: total training loss 0.00082
2020-02-26 20:47:11,961 EPOCH 1578
2020-02-26 20:47:22,052 Epoch 1578: total training loss 0.00077
2020-02-26 20:47:22,052 EPOCH 1579
2020-02-26 20:47:30,435 Epoch 1579 Step:   175250 Batch Loss:     0.000009 Tokens per Sec: 10096452, Lr: 0.001000
2020-02-26 20:47:32,210 Epoch 1579: total training loss 0.00077
2020-02-26 20:47:32,211 EPOCH 1580
2020-02-26 20:47:42,301 Epoch 1580: total training loss 0.00076
2020-02-26 20:47:42,302 EPOCH 1581
2020-02-26 20:47:52,202 Epoch 1581: total training loss 0.00077
2020-02-26 20:47:52,205 EPOCH 1582
2020-02-26 20:47:53,068 Epoch 1582 Step:   175500 Batch Loss:     0.000007 Tokens per Sec: 10495120, Lr: 0.001000
2020-02-26 20:48:02,172 Epoch 1582: total training loss 0.00078
2020-02-26 20:48:02,172 EPOCH 1583
2020-02-26 20:48:12,162 Epoch 1583: total training loss 0.00077
2020-02-26 20:48:12,163 EPOCH 1584
2020-02-26 20:48:15,696 Epoch 1584 Step:   175750 Batch Loss:     0.000007 Tokens per Sec: 10335777, Lr: 0.001000
2020-02-26 20:48:22,156 Epoch 1584: total training loss 0.00078
2020-02-26 20:48:22,157 EPOCH 1585
2020-02-26 20:48:32,192 Epoch 1585: total training loss 0.00077
2020-02-26 20:48:32,192 EPOCH 1586
2020-02-26 20:48:38,047 Epoch 1586 Step:   176000 Batch Loss:     0.000006 Tokens per Sec: 10516423, Lr: 0.001000
2020-02-26 20:48:42,047 Epoch 1586: total training loss 0.00088
2020-02-26 20:48:42,047 EPOCH 1587
2020-02-26 20:48:51,848 Epoch 1587: total training loss 0.00088
2020-02-26 20:48:51,848 EPOCH 1588
2020-02-26 20:49:00,036 Epoch 1588 Step:   176250 Batch Loss:     0.000009 Tokens per Sec: 10599069, Lr: 0.001000
2020-02-26 20:49:01,541 Epoch 1588: total training loss 0.00088
2020-02-26 20:49:01,541 EPOCH 1589
2020-02-26 20:49:11,252 Epoch 1589: total training loss 0.00079
2020-02-26 20:49:11,253 EPOCH 1590
2020-02-26 20:49:20,991 Epoch 1590: total training loss 0.00077
2020-02-26 20:49:20,992 EPOCH 1591
2020-02-26 20:49:21,961 Epoch 1591 Step:   176500 Batch Loss:     0.000009 Tokens per Sec: 10908670, Lr: 0.001000
2020-02-26 20:49:30,832 Epoch 1591: total training loss 0.00077
2020-02-26 20:49:30,834 EPOCH 1592
2020-02-26 20:49:40,821 Epoch 1592: total training loss 0.00080
2020-02-26 20:49:40,823 EPOCH 1593
2020-02-26 20:49:44,219 Epoch 1593 Step:   176750 Batch Loss:     0.000006 Tokens per Sec:  9777012, Lr: 0.001000
2020-02-26 20:49:51,058 Epoch 1593: total training loss 0.00079
2020-02-26 20:49:51,059 EPOCH 1594
2020-02-26 20:50:01,070 Epoch 1594: total training loss 0.00078
2020-02-26 20:50:01,071 EPOCH 1595
2020-02-26 20:50:07,056 Epoch 1595 Step:   177000 Batch Loss:     0.000007 Tokens per Sec: 10283286, Lr: 0.001000
2020-02-26 20:50:11,112 Epoch 1595: total training loss 0.00078
2020-02-26 20:50:11,113 EPOCH 1596
2020-02-26 20:50:20,968 Epoch 1596: total training loss 0.00078
2020-02-26 20:50:20,969 EPOCH 1597
2020-02-26 20:50:29,260 Epoch 1597 Step:   177250 Batch Loss:     0.000014 Tokens per Sec: 10591097, Lr: 0.001000
2020-02-26 20:50:30,672 Epoch 1597: total training loss 0.00082
2020-02-26 20:50:30,673 EPOCH 1598
2020-02-26 20:50:40,387 Epoch 1598: total training loss 0.00080
2020-02-26 20:50:40,388 EPOCH 1599
2020-02-26 20:50:50,176 Epoch 1599: total training loss 0.00080
2020-02-26 20:50:50,177 EPOCH 1600
2020-02-26 20:50:51,259 Epoch 1600 Step:   177500 Batch Loss:     0.000005 Tokens per Sec: 10736656, Lr: 0.001000
2020-02-26 20:50:59,768 Epoch 1600: total training loss 0.00084
2020-02-26 20:50:59,769 EPOCH 1601
2020-02-26 20:51:09,497 Epoch 1601: total training loss 0.00082
2020-02-26 20:51:09,498 EPOCH 1602
2020-02-26 20:51:13,105 Epoch 1602 Step:   177750 Batch Loss:     0.000006 Tokens per Sec: 10737589, Lr: 0.001000
2020-02-26 20:51:19,165 Epoch 1602: total training loss 0.00081
2020-02-26 20:51:19,166 EPOCH 1603
2020-02-26 20:51:29,113 Epoch 1603: total training loss 0.00081
2020-02-26 20:51:29,113 EPOCH 1604
2020-02-26 20:51:35,082 Epoch 1604 Step:   178000 Batch Loss:     0.000006 Tokens per Sec: 10397527, Lr: 0.001000
2020-02-26 20:51:39,040 Epoch 1604: total training loss 0.00078
2020-02-26 20:51:39,041 EPOCH 1605
2020-02-26 20:51:48,938 Epoch 1605: total training loss 0.00077
2020-02-26 20:51:48,938 EPOCH 1606
2020-02-26 20:51:57,353 Epoch 1606 Step:   178250 Batch Loss:     0.000007 Tokens per Sec: 10500263, Lr: 0.001000
2020-02-26 20:51:58,739 Epoch 1606: total training loss 0.00076
2020-02-26 20:51:58,739 EPOCH 1607
2020-02-26 20:52:08,626 Epoch 1607: total training loss 0.00076
2020-02-26 20:52:08,627 EPOCH 1608
2020-02-26 20:52:18,467 Epoch 1608: total training loss 0.00079
2020-02-26 20:52:18,468 EPOCH 1609
2020-02-26 20:52:19,544 Epoch 1609 Step:   178500 Batch Loss:     0.000008 Tokens per Sec: 10337741, Lr: 0.001000
2020-02-26 20:52:28,174 Epoch 1609: total training loss 0.00079
2020-02-26 20:52:28,174 EPOCH 1610
2020-02-26 20:52:37,884 Epoch 1610: total training loss 0.00080
2020-02-26 20:52:37,884 EPOCH 1611
2020-02-26 20:52:41,448 Epoch 1611 Step:   178750 Batch Loss:     0.000007 Tokens per Sec:  9992454, Lr: 0.001000
2020-02-26 20:52:47,938 Epoch 1611: total training loss 0.00081
2020-02-26 20:52:47,939 EPOCH 1612
2020-02-26 20:52:58,032 Epoch 1612: total training loss 0.00082
2020-02-26 20:52:58,032 EPOCH 1613
2020-02-26 20:53:04,154 Epoch 1613 Step:   179000 Batch Loss:     0.000005 Tokens per Sec: 10102018, Lr: 0.001000
2020-02-26 20:53:08,188 Epoch 1613: total training loss 0.00079
2020-02-26 20:53:08,188 EPOCH 1614
2020-02-26 20:53:18,395 Epoch 1614: total training loss 0.00080
2020-02-26 20:53:18,395 EPOCH 1615
2020-02-26 20:53:27,024 Epoch 1615 Step:   179250 Batch Loss:     0.000010 Tokens per Sec: 10163168, Lr: 0.001000
2020-02-26 20:53:28,454 Epoch 1615: total training loss 0.00077
2020-02-26 20:53:28,455 EPOCH 1616
2020-02-26 20:53:38,177 Epoch 1616: total training loss 0.00076
2020-02-26 20:53:38,177 EPOCH 1617
2020-02-26 20:53:47,976 Epoch 1617: total training loss 0.00076
2020-02-26 20:53:47,976 EPOCH 1618
2020-02-26 20:53:49,107 Epoch 1618 Step:   179500 Batch Loss:     0.000013 Tokens per Sec: 10342991, Lr: 0.001000
2020-02-26 20:53:57,687 Epoch 1618: total training loss 0.00076
2020-02-26 20:53:57,688 EPOCH 1619
2020-02-26 20:54:07,366 Epoch 1619: total training loss 0.00077
2020-02-26 20:54:07,367 EPOCH 1620
2020-02-26 20:54:10,914 Epoch 1620 Step:   179750 Batch Loss:     0.000006 Tokens per Sec: 10539371, Lr: 0.001000
2020-02-26 20:54:17,031 Epoch 1620: total training loss 0.00077
2020-02-26 20:54:17,031 EPOCH 1621
2020-02-26 20:54:26,800 Epoch 1621: total training loss 0.00086
2020-02-26 20:54:26,801 EPOCH 1622
2020-02-26 20:54:33,081 Epoch 1622 Step:   180000 Batch Loss:     0.000007 Tokens per Sec: 10351936, Lr: 0.001000
2020-02-26 20:54:33,081 Model noise rate: 5
2020-02-26 20:55:34,630 Validation result at epoch 1622, step   180000: Val DTW Score:  10.65, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0373, GT DTW Score:      nan, duration: 61.5482s
2020-02-26 20:55:38,389 Epoch 1622: total training loss 0.00085
2020-02-26 20:55:38,390 EPOCH 1623
2020-02-26 20:55:48,196 Epoch 1623: total training loss 0.00075
2020-02-26 20:55:48,197 EPOCH 1624
2020-02-26 20:55:56,687 Epoch 1624 Step:   180250 Batch Loss:     0.000006 Tokens per Sec: 10598075, Lr: 0.000700
2020-02-26 20:55:57,876 Epoch 1624: total training loss 0.00073
2020-02-26 20:55:57,877 EPOCH 1625
2020-02-26 20:56:07,591 Epoch 1625: total training loss 0.00072
2020-02-26 20:56:07,592 EPOCH 1626
2020-02-26 20:56:17,445 Epoch 1626: total training loss 0.00072
2020-02-26 20:56:17,447 EPOCH 1627
2020-02-26 20:56:18,673 Epoch 1627 Step:   180500 Batch Loss:     0.000005 Tokens per Sec: 10357827, Lr: 0.000700
2020-02-26 20:56:27,277 Epoch 1627: total training loss 0.00076
2020-02-26 20:56:27,278 EPOCH 1628
2020-02-26 20:56:37,235 Epoch 1628: total training loss 0.00074
2020-02-26 20:56:37,236 EPOCH 1629
2020-02-26 20:56:40,790 Epoch 1629 Step:   180750 Batch Loss:     0.000007 Tokens per Sec: 10287352, Lr: 0.000700
2020-02-26 20:56:47,139 Epoch 1629: total training loss 0.00073
2020-02-26 20:56:47,139 EPOCH 1630
2020-02-26 20:56:56,942 Epoch 1630: total training loss 0.00072
2020-02-26 20:56:56,943 EPOCH 1631
2020-02-26 20:57:03,167 Epoch 1631 Step:   181000 Batch Loss:     0.000004 Tokens per Sec: 10548817, Lr: 0.000700
2020-02-26 20:57:06,737 Epoch 1631: total training loss 0.00072
2020-02-26 20:57:06,738 EPOCH 1632
2020-02-26 20:57:16,565 Epoch 1632: total training loss 0.00071
2020-02-26 20:57:16,565 EPOCH 1633
2020-02-26 20:57:25,083 Epoch 1633 Step:   181250 Batch Loss:     0.000005 Tokens per Sec: 10489554, Lr: 0.000700
2020-02-26 20:57:26,307 Epoch 1633: total training loss 0.00073
2020-02-26 20:57:26,307 EPOCH 1634
2020-02-26 20:57:36,143 Epoch 1634: total training loss 0.00072
2020-02-26 20:57:36,143 EPOCH 1635
2020-02-26 20:57:45,917 Epoch 1635: total training loss 0.00072
2020-02-26 20:57:45,917 EPOCH 1636
2020-02-26 20:57:47,262 Epoch 1636 Step:   181500 Batch Loss:     0.000006 Tokens per Sec: 10373105, Lr: 0.000700
2020-02-26 20:57:55,650 Epoch 1636: total training loss 0.00074
2020-02-26 20:57:55,651 EPOCH 1637
2020-02-26 20:58:05,481 Epoch 1637: total training loss 0.00074
2020-02-26 20:58:05,482 EPOCH 1638
2020-02-26 20:58:09,347 Epoch 1638 Step:   181750 Batch Loss:     0.000007 Tokens per Sec: 10524017, Lr: 0.000700
2020-02-26 20:58:15,301 Epoch 1638: total training loss 0.00072
2020-02-26 20:58:15,302 EPOCH 1639
2020-02-26 20:58:25,013 Epoch 1639: total training loss 0.00072
2020-02-26 20:58:25,014 EPOCH 1640
2020-02-26 20:58:31,002 Epoch 1640 Step:   182000 Batch Loss:     0.000005 Tokens per Sec: 10424149, Lr: 0.000700
2020-02-26 20:58:34,741 Epoch 1640: total training loss 0.00073
2020-02-26 20:58:34,742 EPOCH 1641
2020-02-26 20:58:44,696 Epoch 1641: total training loss 0.00072
2020-02-26 20:58:44,696 EPOCH 1642
2020-02-26 20:58:53,516 Epoch 1642 Step:   182250 Batch Loss:     0.000004 Tokens per Sec: 10363391, Lr: 0.000700
2020-02-26 20:58:54,574 Epoch 1642: total training loss 0.00072
2020-02-26 20:58:54,574 EPOCH 1643
2020-02-26 20:59:04,614 Epoch 1643: total training loss 0.00073
2020-02-26 20:59:04,614 EPOCH 1644
2020-02-26 20:59:14,564 Epoch 1644: total training loss 0.00072
2020-02-26 20:59:14,564 EPOCH 1645
2020-02-26 20:59:15,890 Epoch 1645 Step:   182500 Batch Loss:     0.000007 Tokens per Sec: 10150421, Lr: 0.000700
2020-02-26 20:59:24,341 Epoch 1645: total training loss 0.00072
2020-02-26 20:59:24,341 EPOCH 1646
2020-02-26 20:59:34,119 Epoch 1646: total training loss 0.00072
2020-02-26 20:59:34,119 EPOCH 1647
2020-02-26 20:59:38,176 Epoch 1647 Step:   182750 Batch Loss:     0.000007 Tokens per Sec: 10704376, Lr: 0.000700
2020-02-26 20:59:43,963 Epoch 1647: total training loss 0.00072
2020-02-26 20:59:43,963 EPOCH 1648
2020-02-26 20:59:53,638 Epoch 1648: total training loss 0.00071
2020-02-26 20:59:53,639 EPOCH 1649
2020-02-26 20:59:59,732 Epoch 1649 Step:   183000 Batch Loss:     0.000005 Tokens per Sec: 10559547, Lr: 0.000700
2020-02-26 21:00:03,293 Epoch 1649: total training loss 0.00072
2020-02-26 21:00:03,293 EPOCH 1650
2020-02-26 21:00:13,148 Epoch 1650: total training loss 0.00076
2020-02-26 21:00:13,149 EPOCH 1651
2020-02-26 21:00:22,027 Epoch 1651 Step:   183250 Batch Loss:     0.000008 Tokens per Sec: 10491053, Lr: 0.000700
2020-02-26 21:00:22,997 Epoch 1651: total training loss 0.00073
2020-02-26 21:00:22,997 EPOCH 1652
2020-02-26 21:00:33,205 Epoch 1652: total training loss 0.00072
2020-02-26 21:00:33,205 EPOCH 1653
2020-02-26 21:00:43,346 Epoch 1653: total training loss 0.00072
2020-02-26 21:00:43,359 EPOCH 1654
2020-02-26 21:00:44,994 Epoch 1654 Step:   183500 Batch Loss:     0.000006 Tokens per Sec: 10239205, Lr: 0.000700
2020-02-26 21:00:53,470 Epoch 1654: total training loss 0.00073
2020-02-26 21:00:53,470 EPOCH 1655
2020-02-26 21:01:03,521 Epoch 1655: total training loss 0.00073
2020-02-26 21:01:03,521 EPOCH 1656
2020-02-26 21:01:07,484 Epoch 1656 Step:   183750 Batch Loss:     0.000012 Tokens per Sec: 10253594, Lr: 0.000700
2020-02-26 21:01:13,377 Epoch 1656: total training loss 0.00072
2020-02-26 21:01:13,377 EPOCH 1657
2020-02-26 21:01:23,168 Epoch 1657: total training loss 0.00071
2020-02-26 21:01:23,168 EPOCH 1658
2020-02-26 21:01:29,377 Epoch 1658 Step:   184000 Batch Loss:     0.000005 Tokens per Sec: 10414422, Lr: 0.000700
2020-02-26 21:01:32,935 Epoch 1658: total training loss 0.00072
2020-02-26 21:01:32,935 EPOCH 1659
2020-02-26 21:01:42,623 Epoch 1659: total training loss 0.00072
2020-02-26 21:01:42,624 EPOCH 1660
2020-02-26 21:01:51,374 Epoch 1660 Step:   184250 Batch Loss:     0.000005 Tokens per Sec: 10668120, Lr: 0.000700
2020-02-26 21:01:52,264 Epoch 1660: total training loss 0.00072
2020-02-26 21:01:52,264 EPOCH 1661
2020-02-26 21:02:02,122 Epoch 1661: total training loss 0.00071
2020-02-26 21:02:02,123 EPOCH 1662
2020-02-26 21:02:12,006 Epoch 1662: total training loss 0.00072
2020-02-26 21:02:12,006 EPOCH 1663
2020-02-26 21:02:13,678 Epoch 1663 Step:   184500 Batch Loss:     0.000007 Tokens per Sec: 10370502, Lr: 0.000700
2020-02-26 21:02:21,930 Epoch 1663: total training loss 0.00072
2020-02-26 21:02:21,931 EPOCH 1664
2020-02-26 21:02:31,861 Epoch 1664: total training loss 0.00072
2020-02-26 21:02:31,861 EPOCH 1665
2020-02-26 21:02:36,003 Epoch 1665 Step:   184750 Batch Loss:     0.000006 Tokens per Sec: 10352738, Lr: 0.000700
2020-02-26 21:02:41,819 Epoch 1665: total training loss 0.00073
2020-02-26 21:02:41,820 EPOCH 1666
2020-02-26 21:02:52,173 Epoch 1666: total training loss 0.00076
2020-02-26 21:02:52,174 EPOCH 1667
2020-02-26 21:02:59,025 Epoch 1667 Step:   185000 Batch Loss:     0.000004 Tokens per Sec: 10067698, Lr: 0.000700
2020-02-26 21:03:02,331 Epoch 1667: total training loss 0.00074
2020-02-26 21:03:02,332 EPOCH 1668
2020-02-26 21:03:12,580 Epoch 1668: total training loss 0.00074
2020-02-26 21:03:12,580 EPOCH 1669
2020-02-26 21:03:21,854 Epoch 1669 Step:   185250 Batch Loss:     0.000006 Tokens per Sec: 10212761, Lr: 0.000700
2020-02-26 21:03:22,623 Epoch 1669: total training loss 0.00072
2020-02-26 21:03:22,623 EPOCH 1670
2020-02-26 21:03:32,357 Epoch 1670: total training loss 0.00071
2020-02-26 21:03:32,357 EPOCH 1671
2020-02-26 21:03:42,167 Epoch 1671: total training loss 0.00072
2020-02-26 21:03:42,167 EPOCH 1672
2020-02-26 21:03:43,760 Epoch 1672 Step:   185500 Batch Loss:     0.000006 Tokens per Sec: 10454669, Lr: 0.000700
2020-02-26 21:03:52,045 Epoch 1672: total training loss 0.00070
2020-02-26 21:03:52,046 EPOCH 1673
2020-02-26 21:04:02,024 Epoch 1673: total training loss 0.00074
2020-02-26 21:04:02,024 EPOCH 1674
2020-02-26 21:04:06,321 Epoch 1674 Step:   185750 Batch Loss:     0.000007 Tokens per Sec: 10374859, Lr: 0.000700
2020-02-26 21:04:11,819 Epoch 1674: total training loss 0.00072
2020-02-26 21:04:11,819 EPOCH 1675
2020-02-26 21:04:21,596 Epoch 1675: total training loss 0.00072
2020-02-26 21:04:21,597 EPOCH 1676
2020-02-26 21:04:28,357 Epoch 1676 Step:   186000 Batch Loss:     0.000009 Tokens per Sec: 10461071, Lr: 0.000700
2020-02-26 21:04:31,488 Epoch 1676: total training loss 0.00072
2020-02-26 21:04:31,488 EPOCH 1677
2020-02-26 21:04:41,384 Epoch 1677: total training loss 0.00072
2020-02-26 21:04:41,385 EPOCH 1678
2020-02-26 21:04:50,318 Epoch 1678 Step:   186250 Batch Loss:     0.000007 Tokens per Sec: 10497865, Lr: 0.000700
2020-02-26 21:04:51,114 Epoch 1678: total training loss 0.00073
2020-02-26 21:04:51,115 EPOCH 1679
2020-02-26 21:05:00,885 Epoch 1679: total training loss 0.00071
2020-02-26 21:05:00,886 EPOCH 1680
2020-02-26 21:05:10,712 Epoch 1680: total training loss 0.00070
2020-02-26 21:05:10,713 EPOCH 1681
2020-02-26 21:05:12,578 Epoch 1681 Step:   186500 Batch Loss:     0.000006 Tokens per Sec: 10556886, Lr: 0.000700
2020-02-26 21:05:20,592 Epoch 1681: total training loss 0.00071
2020-02-26 21:05:20,593 EPOCH 1682
2020-02-26 21:05:30,611 Epoch 1682: total training loss 0.00071
2020-02-26 21:05:30,612 EPOCH 1683
2020-02-26 21:05:35,088 Epoch 1683 Step:   186750 Batch Loss:     0.000006 Tokens per Sec: 10021412, Lr: 0.000700
2020-02-26 21:05:40,988 Epoch 1683: total training loss 0.00079
2020-02-26 21:05:40,989 EPOCH 1684
2020-02-26 21:05:51,378 Epoch 1684: total training loss 0.00073
2020-02-26 21:05:51,379 EPOCH 1685
2020-02-26 21:05:58,510 Epoch 1685 Step:   187000 Batch Loss:     0.000005 Tokens per Sec:  9985035, Lr: 0.000700
2020-02-26 21:06:01,708 Epoch 1685: total training loss 0.00071
2020-02-26 21:06:01,708 EPOCH 1686
2020-02-26 21:06:11,731 Epoch 1686: total training loss 0.00072
2020-02-26 21:06:11,731 EPOCH 1687
2020-02-26 21:06:20,907 Epoch 1687 Step:   187250 Batch Loss:     0.000006 Tokens per Sec: 10407718, Lr: 0.000700
2020-02-26 21:06:21,585 Epoch 1687: total training loss 0.00076
2020-02-26 21:06:21,586 EPOCH 1688
2020-02-26 21:06:31,352 Epoch 1688: total training loss 0.00074
2020-02-26 21:06:31,353 EPOCH 1689
2020-02-26 21:06:41,156 Epoch 1689: total training loss 0.00073
2020-02-26 21:06:41,157 EPOCH 1690
2020-02-26 21:06:42,971 Epoch 1690 Step:   187500 Batch Loss:     0.000007 Tokens per Sec: 10385836, Lr: 0.000700
2020-02-26 21:06:51,055 Epoch 1690: total training loss 0.00072
2020-02-26 21:06:51,056 EPOCH 1691
2020-02-26 21:07:00,923 Epoch 1691: total training loss 0.00071
2020-02-26 21:07:00,923 EPOCH 1692
2020-02-26 21:07:05,189 Epoch 1692 Step:   187750 Batch Loss:     0.000006 Tokens per Sec: 10454878, Lr: 0.000700
2020-02-26 21:07:10,776 Epoch 1692: total training loss 0.00073
2020-02-26 21:07:10,776 EPOCH 1693
2020-02-26 21:07:20,647 Epoch 1693: total training loss 0.00072
2020-02-26 21:07:20,647 EPOCH 1694
2020-02-26 21:07:27,513 Epoch 1694 Step:   188000 Batch Loss:     0.000004 Tokens per Sec: 10435161, Lr: 0.000700
2020-02-26 21:07:30,632 Epoch 1694: total training loss 0.00070
2020-02-26 21:07:30,632 EPOCH 1695
2020-02-26 21:07:40,568 Epoch 1695: total training loss 0.00075
2020-02-26 21:07:40,570 EPOCH 1696
2020-02-26 21:07:50,086 Epoch 1696 Step:   188250 Batch Loss:     0.000006 Tokens per Sec: 10192899, Lr: 0.000700
2020-02-26 21:07:50,668 Epoch 1696: total training loss 0.00071
2020-02-26 21:07:50,669 EPOCH 1697
2020-02-26 21:08:00,899 Epoch 1697: total training loss 0.00071
2020-02-26 21:08:00,899 EPOCH 1698
2020-02-26 21:08:10,856 Epoch 1698: total training loss 0.00070
2020-02-26 21:08:10,857 EPOCH 1699
2020-02-26 21:08:12,716 Epoch 1699 Step:   188500 Batch Loss:     0.000006 Tokens per Sec:  9988141, Lr: 0.000700
2020-02-26 21:08:20,913 Epoch 1699: total training loss 0.00071
2020-02-26 21:08:20,913 EPOCH 1700
2020-02-26 21:08:30,803 Epoch 1700: total training loss 0.00070
2020-02-26 21:08:30,804 EPOCH 1701
2020-02-26 21:08:35,164 Epoch 1701 Step:   188750 Batch Loss:     0.000009 Tokens per Sec: 10585734, Lr: 0.000700
2020-02-26 21:08:40,511 Epoch 1701: total training loss 0.00070
2020-02-26 21:08:40,511 EPOCH 1702
2020-02-26 21:08:50,171 Epoch 1702: total training loss 0.00073
2020-02-26 21:08:50,171 EPOCH 1703
2020-02-26 21:08:57,218 Epoch 1703 Step:   189000 Batch Loss:     0.000004 Tokens per Sec: 10527719, Lr: 0.000700
2020-02-26 21:09:00,049 Epoch 1703: total training loss 0.00071
2020-02-26 21:09:00,049 EPOCH 1704
2020-02-26 21:09:10,042 Epoch 1704: total training loss 0.00073
2020-02-26 21:09:10,042 EPOCH 1705
2020-02-26 21:09:19,486 Epoch 1705 Step:   189250 Batch Loss:     0.000006 Tokens per Sec: 10404709, Lr: 0.000700
2020-02-26 21:09:19,916 Epoch 1705: total training loss 0.00076
2020-02-26 21:09:19,917 EPOCH 1706
2020-02-26 21:09:29,715 Epoch 1706: total training loss 0.00071
2020-02-26 21:09:29,715 EPOCH 1707
2020-02-26 21:09:39,510 Epoch 1707: total training loss 0.00070
2020-02-26 21:09:39,510 EPOCH 1708
2020-02-26 21:09:41,501 Epoch 1708 Step:   189500 Batch Loss:     0.000006 Tokens per Sec: 10504987, Lr: 0.000700
2020-02-26 21:09:49,146 Epoch 1708: total training loss 0.00071
2020-02-26 21:09:49,146 EPOCH 1709
2020-02-26 21:09:58,820 Epoch 1709: total training loss 0.00071
2020-02-26 21:09:58,821 EPOCH 1710
2020-02-26 21:10:03,236 Epoch 1710 Step:   189750 Batch Loss:     0.000009 Tokens per Sec: 10678674, Lr: 0.000700
2020-02-26 21:10:08,503 Epoch 1710: total training loss 0.00071
2020-02-26 21:10:08,503 EPOCH 1711
2020-02-26 21:10:18,053 Epoch 1711: total training loss 0.00074
2020-02-26 21:10:18,054 EPOCH 1712
2020-02-26 21:10:24,676 Epoch 1712 Step:   190000 Batch Loss:     0.000006 Tokens per Sec: 10810853, Lr: 0.000700
2020-02-26 21:10:24,677 Model noise rate: 5
2020-02-26 21:11:26,149 Validation result at epoch 1712, step   190000: Val DTW Score:  10.69, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0400, GT DTW Score:      nan, duration: 61.4722s
2020-02-26 21:11:29,343 Epoch 1712: total training loss 0.00073
2020-02-26 21:11:29,343 EPOCH 1713
2020-02-26 21:11:39,294 Epoch 1713: total training loss 0.00071
2020-02-26 21:11:39,294 EPOCH 1714
2020-02-26 21:11:48,778 Epoch 1714 Step:   190250 Batch Loss:     0.000012 Tokens per Sec: 10323113, Lr: 0.000700
2020-02-26 21:11:49,161 Epoch 1714: total training loss 0.00071
2020-02-26 21:11:49,161 EPOCH 1715
2020-02-26 21:11:59,034 Epoch 1715: total training loss 0.00071
2020-02-26 21:11:59,034 EPOCH 1716
2020-02-26 21:12:08,778 Epoch 1716: total training loss 0.00073
2020-02-26 21:12:08,779 EPOCH 1717
2020-02-26 21:12:10,975 Epoch 1717 Step:   190500 Batch Loss:     0.000011 Tokens per Sec: 10429835, Lr: 0.000700
2020-02-26 21:12:18,757 Epoch 1717: total training loss 0.00073
2020-02-26 21:12:18,757 EPOCH 1718
2020-02-26 21:12:28,681 Epoch 1718: total training loss 0.00070
2020-02-26 21:12:28,681 EPOCH 1719
2020-02-26 21:12:33,162 Epoch 1719 Step:   190750 Batch Loss:     0.000007 Tokens per Sec: 10091320, Lr: 0.000700
2020-02-26 21:12:38,695 Epoch 1719: total training loss 0.00069
2020-02-26 21:12:38,696 EPOCH 1720
2020-02-26 21:12:48,509 Epoch 1720: total training loss 0.00070
2020-02-26 21:12:48,510 EPOCH 1721
2020-02-26 21:12:55,674 Epoch 1721 Step:   191000 Batch Loss:     0.000008 Tokens per Sec: 10513174, Lr: 0.000700
2020-02-26 21:12:58,360 Epoch 1721: total training loss 0.00070
2020-02-26 21:12:58,361 EPOCH 1722
2020-02-26 21:13:08,188 Epoch 1722: total training loss 0.00072
2020-02-26 21:13:08,189 EPOCH 1723
2020-02-26 21:13:17,616 Epoch 1723 Step:   191250 Batch Loss:     0.000003 Tokens per Sec: 10535255, Lr: 0.000700
2020-02-26 21:13:17,878 Epoch 1723: total training loss 0.00071
2020-02-26 21:13:17,878 EPOCH 1724
2020-02-26 21:13:27,604 Epoch 1724: total training loss 0.00072
2020-02-26 21:13:27,605 EPOCH 1725
2020-02-26 21:13:37,296 Epoch 1725: total training loss 0.00071
2020-02-26 21:13:37,296 EPOCH 1726
2020-02-26 21:13:39,442 Epoch 1726 Step:   191500 Batch Loss:     0.000008 Tokens per Sec: 10359466, Lr: 0.000700
2020-02-26 21:13:47,029 Epoch 1726: total training loss 0.00071
2020-02-26 21:13:47,030 EPOCH 1727
2020-02-26 21:13:56,866 Epoch 1727: total training loss 0.00071
2020-02-26 21:13:56,867 EPOCH 1728
2020-02-26 21:14:01,846 Epoch 1728 Step:   191750 Batch Loss:     0.000004 Tokens per Sec:  9853393, Lr: 0.000700
2020-02-26 21:14:07,264 Epoch 1728: total training loss 0.00071
2020-02-26 21:14:07,264 EPOCH 1729
2020-02-26 21:14:17,577 Epoch 1729: total training loss 0.00072
2020-02-26 21:14:17,577 EPOCH 1730
2020-02-26 21:14:24,805 Epoch 1730 Step:   192000 Batch Loss:     0.000004 Tokens per Sec:  9994934, Lr: 0.000700
2020-02-26 21:14:27,774 Epoch 1730: total training loss 0.00071
2020-02-26 21:14:27,774 EPOCH 1731
2020-02-26 21:14:37,946 Epoch 1731: total training loss 0.00073
2020-02-26 21:14:37,946 EPOCH 1732
2020-02-26 21:14:47,731 Epoch 1732 Step:   192250 Batch Loss:     0.000010 Tokens per Sec: 10261696, Lr: 0.000700
2020-02-26 21:14:47,908 Epoch 1732: total training loss 0.00071
2020-02-26 21:14:47,908 EPOCH 1733
2020-02-26 21:14:57,853 Epoch 1733: total training loss 0.00069
2020-02-26 21:14:57,853 EPOCH 1734
2020-02-26 21:15:07,662 Epoch 1734: total training loss 0.00070
2020-02-26 21:15:07,663 EPOCH 1735
2020-02-26 21:15:10,079 Epoch 1735 Step:   192500 Batch Loss:     0.000009 Tokens per Sec: 10332956, Lr: 0.000700
2020-02-26 21:15:17,396 Epoch 1735: total training loss 0.00072
2020-02-26 21:15:17,397 EPOCH 1736
2020-02-26 21:15:27,074 Epoch 1736: total training loss 0.00072
2020-02-26 21:15:27,075 EPOCH 1737
2020-02-26 21:15:31,816 Epoch 1737 Step:   192750 Batch Loss:     0.000005 Tokens per Sec: 10655975, Lr: 0.000700
2020-02-26 21:15:36,909 Epoch 1737: total training loss 0.00072
2020-02-26 21:15:36,910 EPOCH 1738
2020-02-26 21:15:46,815 Epoch 1738: total training loss 0.00071
2020-02-26 21:15:46,815 EPOCH 1739
2020-02-26 21:15:54,068 Epoch 1739 Step:   193000 Batch Loss:     0.000008 Tokens per Sec: 10277937, Lr: 0.000700
2020-02-26 21:15:56,731 Epoch 1739: total training loss 0.00072
2020-02-26 21:15:56,731 EPOCH 1740
2020-02-26 21:16:06,721 Epoch 1740: total training loss 0.00070
2020-02-26 21:16:06,721 EPOCH 1741
2020-02-26 21:16:16,595 Epoch 1741 Step:   193250 Batch Loss:     0.000006 Tokens per Sec: 10290118, Lr: 0.000700
2020-02-26 21:16:16,652 Epoch 1741: total training loss 0.00071
2020-02-26 21:16:16,652 EPOCH 1742
2020-02-26 21:16:26,424 Epoch 1742: total training loss 0.00072
2020-02-26 21:16:26,425 EPOCH 1743
2020-02-26 21:16:36,306 Epoch 1743: total training loss 0.00071
2020-02-26 21:16:36,307 EPOCH 1744
2020-02-26 21:16:38,627 Epoch 1744 Step:   193500 Batch Loss:     0.000007 Tokens per Sec: 10244442, Lr: 0.000700
2020-02-26 21:16:46,214 Epoch 1744: total training loss 0.00071
2020-02-26 21:16:46,215 EPOCH 1745
2020-02-26 21:16:56,259 Epoch 1745: total training loss 0.00069
2020-02-26 21:16:56,260 EPOCH 1746
2020-02-26 21:17:01,317 Epoch 1746 Step:   193750 Batch Loss:     0.000006 Tokens per Sec: 10213085, Lr: 0.000700
2020-02-26 21:17:06,203 Epoch 1746: total training loss 0.00069
2020-02-26 21:17:06,203 EPOCH 1747
2020-02-26 21:17:16,018 Epoch 1747: total training loss 0.00070
2020-02-26 21:17:16,019 EPOCH 1748
2020-02-26 21:17:23,316 Epoch 1748 Step:   194000 Batch Loss:     0.000005 Tokens per Sec: 10342134, Lr: 0.000700
2020-02-26 21:17:25,808 Epoch 1748: total training loss 0.00070
2020-02-26 21:17:25,808 EPOCH 1749
2020-02-26 21:17:35,734 Epoch 1749: total training loss 0.00073
2020-02-26 21:17:35,734 EPOCH 1750
2020-02-26 21:17:45,657 Epoch 1750 Step:   194250 Batch Loss:     0.000007 Tokens per Sec: 10376630, Lr: 0.000700
2020-02-26 21:17:45,658 Epoch 1750: total training loss 0.00072
2020-02-26 21:17:45,658 EPOCH 1751
2020-02-26 21:17:55,552 Epoch 1751: total training loss 0.00071
2020-02-26 21:17:55,552 EPOCH 1752
2020-02-26 21:18:05,324 Epoch 1752: total training loss 0.00071
2020-02-26 21:18:05,324 EPOCH 1753
2020-02-26 21:18:07,933 Epoch 1753 Step:   194500 Batch Loss:     0.000005 Tokens per Sec: 10633167, Lr: 0.000700
2020-02-26 21:18:15,070 Epoch 1753: total training loss 0.00072
2020-02-26 21:18:15,070 EPOCH 1754
2020-02-26 21:18:24,787 Epoch 1754: total training loss 0.00081
2020-02-26 21:18:24,788 EPOCH 1755
2020-02-26 21:18:29,529 Epoch 1755 Step:   194750 Batch Loss:     0.000004 Tokens per Sec: 10651493, Lr: 0.000700
2020-02-26 21:18:34,429 Epoch 1755: total training loss 0.00072
2020-02-26 21:18:34,429 EPOCH 1756
2020-02-26 21:18:44,052 Epoch 1756: total training loss 0.00070
2020-02-26 21:18:44,052 EPOCH 1757
2020-02-26 21:18:51,557 Epoch 1757 Step:   195000 Batch Loss:     0.000007 Tokens per Sec: 10622364, Lr: 0.000700
2020-02-26 21:18:53,746 Epoch 1757: total training loss 0.00070
2020-02-26 21:18:53,746 EPOCH 1758
2020-02-26 21:19:03,533 Epoch 1758: total training loss 0.00069
2020-02-26 21:19:03,533 EPOCH 1759
2020-02-26 21:19:13,448 Epoch 1759: total training loss 0.00069
2020-02-26 21:19:13,448 EPOCH 1760
2020-02-26 21:19:13,567 Epoch 1760 Step:   195250 Batch Loss:     0.000008 Tokens per Sec:  8854282, Lr: 0.000700
2020-02-26 21:19:23,280 Epoch 1760: total training loss 0.00070
2020-02-26 21:19:23,280 EPOCH 1761
2020-02-26 21:19:33,195 Epoch 1761: total training loss 0.00071
2020-02-26 21:19:33,196 EPOCH 1762
2020-02-26 21:19:35,870 Epoch 1762 Step:   195500 Batch Loss:     0.000007 Tokens per Sec: 10408899, Lr: 0.000700
2020-02-26 21:19:43,173 Epoch 1762: total training loss 0.00069
2020-02-26 21:19:43,173 EPOCH 1763
2020-02-26 21:19:53,090 Epoch 1763: total training loss 0.00071
2020-02-26 21:19:53,090 EPOCH 1764
2020-02-26 21:19:58,113 Epoch 1764 Step:   195750 Batch Loss:     0.000007 Tokens per Sec: 10396121, Lr: 0.000700
2020-02-26 21:20:02,893 Epoch 1764: total training loss 0.00070
2020-02-26 21:20:02,894 EPOCH 1765
2020-02-26 21:20:12,862 Epoch 1765: total training loss 0.00074
2020-02-26 21:20:12,862 EPOCH 1766
2020-02-26 21:20:20,371 Epoch 1766 Step:   196000 Batch Loss:     0.000006 Tokens per Sec: 10434368, Lr: 0.000700
2020-02-26 21:20:22,635 Epoch 1766: total training loss 0.00073
2020-02-26 21:20:22,635 EPOCH 1767
2020-02-26 21:20:32,453 Epoch 1767: total training loss 0.00078
2020-02-26 21:20:32,453 EPOCH 1768
2020-02-26 21:20:42,242 Epoch 1768: total training loss 0.00071
2020-02-26 21:20:42,243 EPOCH 1769
2020-02-26 21:20:42,421 Epoch 1769 Step:   196250 Batch Loss:     0.000005 Tokens per Sec:  9021693, Lr: 0.000700
2020-02-26 21:20:52,055 Epoch 1769: total training loss 0.00071
2020-02-26 21:20:52,056 EPOCH 1770
2020-02-26 21:21:01,802 Epoch 1770: total training loss 0.00074
2020-02-26 21:21:01,802 EPOCH 1771
2020-02-26 21:21:04,519 Epoch 1771 Step:   196500 Batch Loss:     0.000008 Tokens per Sec: 10608853, Lr: 0.000700
2020-02-26 21:21:11,576 Epoch 1771: total training loss 0.00075
2020-02-26 21:21:11,576 EPOCH 1772
2020-02-26 21:21:21,475 Epoch 1772: total training loss 0.00074
2020-02-26 21:21:21,476 EPOCH 1773
2020-02-26 21:21:26,680 Epoch 1773 Step:   196750 Batch Loss:     0.000007 Tokens per Sec: 10172704, Lr: 0.000700
2020-02-26 21:21:31,605 Epoch 1773: total training loss 0.00070
2020-02-26 21:21:31,605 EPOCH 1774
2020-02-26 21:21:41,867 Epoch 1774: total training loss 0.00069
2020-02-26 21:21:41,867 EPOCH 1775
2020-02-26 21:21:49,600 Epoch 1775 Step:   197000 Batch Loss:     0.000006 Tokens per Sec:  9960678, Lr: 0.000700
2020-02-26 21:21:52,049 Epoch 1775: total training loss 0.00070
2020-02-26 21:21:52,049 EPOCH 1776
2020-02-26 21:22:02,347 Epoch 1776: total training loss 0.00070
2020-02-26 21:22:02,348 EPOCH 1777
2020-02-26 21:22:12,570 Epoch 1777: total training loss 0.00069
2020-02-26 21:22:12,570 EPOCH 1778
2020-02-26 21:22:12,817 Epoch 1778 Step:   197250 Batch Loss:     0.000004 Tokens per Sec:  9716970, Lr: 0.000700
2020-02-26 21:22:22,397 Epoch 1778: total training loss 0.00069
2020-02-26 21:22:22,397 EPOCH 1779
2020-02-26 21:22:32,152 Epoch 1779: total training loss 0.00070
2020-02-26 21:22:32,152 EPOCH 1780
2020-02-26 21:22:34,878 Epoch 1780 Step:   197500 Batch Loss:     0.000005 Tokens per Sec: 10479687, Lr: 0.000700
2020-02-26 21:22:42,030 Epoch 1780: total training loss 0.00071
2020-02-26 21:22:42,031 EPOCH 1781
2020-02-26 21:22:51,883 Epoch 1781: total training loss 0.00069
2020-02-26 21:22:51,883 EPOCH 1782
2020-02-26 21:22:57,112 Epoch 1782 Step:   197750 Batch Loss:     0.000007 Tokens per Sec: 10496792, Lr: 0.000700
2020-02-26 21:23:01,659 Epoch 1782: total training loss 0.00072
2020-02-26 21:23:01,659 EPOCH 1783
2020-02-26 21:23:11,508 Epoch 1783: total training loss 0.00071
2020-02-26 21:23:11,509 EPOCH 1784
2020-02-26 21:23:19,172 Epoch 1784 Step:   198000 Batch Loss:     0.000005 Tokens per Sec: 10476507, Lr: 0.000700
2020-02-26 21:23:21,271 Epoch 1784: total training loss 0.00069
2020-02-26 21:23:21,272 EPOCH 1785
2020-02-26 21:23:31,113 Epoch 1785: total training loss 0.00069
2020-02-26 21:23:31,114 EPOCH 1786
2020-02-26 21:23:41,015 Epoch 1786: total training loss 0.00069
2020-02-26 21:23:41,015 EPOCH 1787
2020-02-26 21:23:41,336 Epoch 1787 Step:   198250 Batch Loss:     0.000006 Tokens per Sec:  9346334, Lr: 0.000700
2020-02-26 21:23:51,053 Epoch 1787: total training loss 0.00070
2020-02-26 21:23:51,054 EPOCH 1788
2020-02-26 21:24:01,070 Epoch 1788: total training loss 0.00072
2020-02-26 21:24:01,070 EPOCH 1789
2020-02-26 21:24:04,098 Epoch 1789 Step:   198500 Batch Loss:     0.000005 Tokens per Sec: 10140575, Lr: 0.000700
2020-02-26 21:24:11,205 Epoch 1789: total training loss 0.00069
2020-02-26 21:24:11,205 EPOCH 1790
2020-02-26 21:24:21,143 Epoch 1790: total training loss 0.00070
2020-02-26 21:24:21,143 EPOCH 1791
2020-02-26 21:24:26,673 Epoch 1791 Step:   198750 Batch Loss:     0.000006 Tokens per Sec: 10457380, Lr: 0.000700
2020-02-26 21:24:31,036 Epoch 1791: total training loss 0.00069
2020-02-26 21:24:31,036 EPOCH 1792
2020-02-26 21:24:40,845 Epoch 1792: total training loss 0.00070
2020-02-26 21:24:40,846 EPOCH 1793
2020-02-26 21:24:48,646 Epoch 1793 Step:   199000 Batch Loss:     0.000010 Tokens per Sec: 10412803, Lr: 0.000700
2020-02-26 21:24:50,677 Epoch 1793: total training loss 0.00072
2020-02-26 21:24:50,677 EPOCH 1794
2020-02-26 21:25:00,572 Epoch 1794: total training loss 0.00075
2020-02-26 21:25:00,573 EPOCH 1795
2020-02-26 21:25:10,614 Epoch 1795: total training loss 0.00071
2020-02-26 21:25:10,614 EPOCH 1796
2020-02-26 21:25:11,189 Epoch 1796 Step:   199250 Batch Loss:     0.000006 Tokens per Sec:  9875797, Lr: 0.000700
2020-02-26 21:25:20,448 Epoch 1796: total training loss 0.00068
2020-02-26 21:25:20,448 EPOCH 1797
2020-02-26 21:25:30,351 Epoch 1797: total training loss 0.00068
2020-02-26 21:25:30,352 EPOCH 1798
2020-02-26 21:25:33,135 Epoch 1798 Step:   199500 Batch Loss:     0.000005 Tokens per Sec: 10363043, Lr: 0.000700
2020-02-26 21:25:40,194 Epoch 1798: total training loss 0.00068
2020-02-26 21:25:40,195 EPOCH 1799
2020-02-26 21:25:49,928 Epoch 1799: total training loss 0.00073
2020-02-26 21:25:49,929 EPOCH 1800
2020-02-26 21:25:55,355 Epoch 1800 Step:   199750 Batch Loss:     0.000004 Tokens per Sec: 10586468, Lr: 0.000700
2020-02-26 21:25:59,670 Epoch 1800: total training loss 0.00072
2020-02-26 21:25:59,670 EPOCH 1801
2020-02-26 21:26:09,454 Epoch 1801: total training loss 0.00069
2020-02-26 21:26:09,454 EPOCH 1802
2020-02-26 21:26:17,311 Epoch 1802 Step:   200000 Batch Loss:     0.000006 Tokens per Sec: 10570988, Lr: 0.000700
2020-02-26 21:26:17,311 Model noise rate: 5
2020-02-26 21:27:20,101 Validation result at epoch 1802, step   200000: Val DTW Score:  10.68, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0402, GT DTW Score:      nan, duration: 62.7893s
2020-02-26 21:27:22,069 Epoch 1802: total training loss 0.00071
2020-02-26 21:27:22,069 EPOCH 1803
2020-02-26 21:27:32,036 Epoch 1803: total training loss 0.00072
2020-02-26 21:27:32,036 EPOCH 1804
2020-02-26 21:27:41,774 Epoch 1804: total training loss 0.00074
2020-02-26 21:27:41,774 EPOCH 1805
2020-02-26 21:27:42,316 Epoch 1805 Step:   200250 Batch Loss:     0.000004 Tokens per Sec: 10066170, Lr: 0.000700
2020-02-26 21:27:51,585 Epoch 1805: total training loss 0.00069
2020-02-26 21:27:51,585 EPOCH 1806
2020-02-26 21:28:01,418 Epoch 1806: total training loss 0.00072
2020-02-26 21:28:01,419 EPOCH 1807
2020-02-26 21:28:04,293 Epoch 1807 Step:   200500 Batch Loss:     0.000007 Tokens per Sec: 10445039, Lr: 0.000700
2020-02-26 21:28:11,182 Epoch 1807: total training loss 0.00070
2020-02-26 21:28:11,183 EPOCH 1808
2020-02-26 21:28:20,818 Epoch 1808: total training loss 0.00070
2020-02-26 21:28:20,818 EPOCH 1809
2020-02-26 21:28:26,036 Epoch 1809 Step:   200750 Batch Loss:     0.000006 Tokens per Sec: 10598843, Lr: 0.000700
2020-02-26 21:28:30,435 Epoch 1809: total training loss 0.00069
2020-02-26 21:28:30,436 EPOCH 1810
2020-02-26 21:28:40,293 Epoch 1810: total training loss 0.00068
2020-02-26 21:28:40,294 EPOCH 1811
2020-02-26 21:28:48,331 Epoch 1811 Step:   201000 Batch Loss:     0.000011 Tokens per Sec: 10262016, Lr: 0.000700
2020-02-26 21:28:50,310 Epoch 1811: total training loss 0.00070
2020-02-26 21:28:50,311 EPOCH 1812
2020-02-26 21:29:00,278 Epoch 1812: total training loss 0.00069
2020-02-26 21:29:00,278 EPOCH 1813
2020-02-26 21:29:10,185 Epoch 1813: total training loss 0.00069
2020-02-26 21:29:10,186 EPOCH 1814
2020-02-26 21:29:10,793 Epoch 1814 Step:   201250 Batch Loss:     0.000006 Tokens per Sec:  9860624, Lr: 0.000700
2020-02-26 21:29:20,241 Epoch 1814: total training loss 0.00068
2020-02-26 21:29:20,242 EPOCH 1815
2020-02-26 21:29:30,340 Epoch 1815: total training loss 0.00074
2020-02-26 21:29:30,340 EPOCH 1816
2020-02-26 21:29:33,471 Epoch 1816 Step:   201500 Batch Loss:     0.000008 Tokens per Sec: 10192961, Lr: 0.000700
2020-02-26 21:29:40,280 Epoch 1816: total training loss 0.00072
2020-02-26 21:29:40,280 EPOCH 1817
2020-02-26 21:29:50,129 Epoch 1817: total training loss 0.00072
2020-02-26 21:29:50,129 EPOCH 1818
2020-02-26 21:29:55,771 Epoch 1818 Step:   201750 Batch Loss:     0.000007 Tokens per Sec: 10409599, Lr: 0.000700
2020-02-26 21:29:59,955 Epoch 1818: total training loss 0.00069
2020-02-26 21:29:59,955 EPOCH 1819
2020-02-26 21:30:09,514 Epoch 1819: total training loss 0.00071
2020-02-26 21:30:09,514 EPOCH 1820
2020-02-26 21:30:17,378 Epoch 1820 Step:   202000 Batch Loss:     0.000008 Tokens per Sec: 10649859, Lr: 0.000700
2020-02-26 21:30:19,155 Epoch 1820: total training loss 0.00075
2020-02-26 21:30:19,155 EPOCH 1821
2020-02-26 21:30:28,881 Epoch 1821: total training loss 0.00074
2020-02-26 21:30:28,881 EPOCH 1822
2020-02-26 21:30:38,751 Epoch 1822: total training loss 0.00070
2020-02-26 21:30:38,753 EPOCH 1823
2020-02-26 21:30:39,516 Epoch 1823 Step:   202250 Batch Loss:     0.000005 Tokens per Sec: 10493976, Lr: 0.000700
2020-02-26 21:30:48,737 Epoch 1823: total training loss 0.00068
2020-02-26 21:30:48,738 EPOCH 1824
2020-02-26 21:30:58,619 Epoch 1824: total training loss 0.00069
2020-02-26 21:30:58,620 EPOCH 1825
2020-02-26 21:31:01,887 Epoch 1825 Step:   202500 Batch Loss:     0.000004 Tokens per Sec: 10425040, Lr: 0.000700
2020-02-26 21:31:08,406 Epoch 1825: total training loss 0.00068
2020-02-26 21:31:08,406 EPOCH 1826
2020-02-26 21:31:18,130 Epoch 1826: total training loss 0.00077
2020-02-26 21:31:18,131 EPOCH 1827
2020-02-26 21:31:23,785 Epoch 1827 Step:   202750 Batch Loss:     0.000004 Tokens per Sec: 10611824, Lr: 0.000700
2020-02-26 21:31:27,830 Epoch 1827: total training loss 0.00078
2020-02-26 21:31:27,831 EPOCH 1828
2020-02-26 21:31:37,511 Epoch 1828: total training loss 0.00077
2020-02-26 21:31:37,511 EPOCH 1829
2020-02-26 21:31:45,488 Epoch 1829 Step:   203000 Batch Loss:     0.000011 Tokens per Sec: 10456174, Lr: 0.000700
2020-02-26 21:31:47,271 Epoch 1829: total training loss 0.00070
2020-02-26 21:31:47,271 EPOCH 1830
2020-02-26 21:31:57,028 Epoch 1830: total training loss 0.00069
2020-02-26 21:31:57,029 EPOCH 1831
2020-02-26 21:32:06,991 Epoch 1831: total training loss 0.00069
2020-02-26 21:32:06,992 EPOCH 1832
2020-02-26 21:32:08,023 Epoch 1832 Step:   203250 Batch Loss:     0.000009 Tokens per Sec: 10022740, Lr: 0.000700
2020-02-26 21:32:17,210 Epoch 1832: total training loss 0.00070
2020-02-26 21:32:17,210 EPOCH 1833
2020-02-26 21:32:27,447 Epoch 1833: total training loss 0.00069
2020-02-26 21:32:27,447 EPOCH 1834
2020-02-26 21:32:30,907 Epoch 1834 Step:   203500 Batch Loss:     0.000005 Tokens per Sec:  9906785, Lr: 0.000700
2020-02-26 21:32:37,559 Epoch 1834: total training loss 0.00069
2020-02-26 21:32:37,560 EPOCH 1835
2020-02-26 21:32:47,886 Epoch 1835: total training loss 0.00069
2020-02-26 21:32:47,887 EPOCH 1836
2020-02-26 21:32:54,012 Epoch 1836 Step:   203750 Batch Loss:     0.000005 Tokens per Sec: 10293623, Lr: 0.000700
2020-02-26 21:32:57,967 Epoch 1836: total training loss 0.00068
2020-02-26 21:32:57,968 EPOCH 1837
2020-02-26 21:33:07,706 Epoch 1837: total training loss 0.00069
2020-02-26 21:33:07,706 EPOCH 1838
2020-02-26 21:33:15,811 Epoch 1838 Step:   204000 Batch Loss:     0.000005 Tokens per Sec: 10520646, Lr: 0.000700
2020-02-26 21:33:17,414 Epoch 1838: total training loss 0.00070
2020-02-26 21:33:17,414 EPOCH 1839
2020-02-26 21:33:27,217 Epoch 1839: total training loss 0.00079
2020-02-26 21:33:27,217 EPOCH 1840
2020-02-26 21:33:36,948 Epoch 1840: total training loss 0.00070
2020-02-26 21:33:36,949 EPOCH 1841
2020-02-26 21:33:38,031 Epoch 1841 Step:   204250 Batch Loss:     0.000012 Tokens per Sec: 10191631, Lr: 0.000700
2020-02-26 21:33:46,888 Epoch 1841: total training loss 0.00069
2020-02-26 21:33:46,889 EPOCH 1842
2020-02-26 21:33:56,840 Epoch 1842: total training loss 0.00068
2020-02-26 21:33:56,840 EPOCH 1843
2020-02-26 21:34:00,079 Epoch 1843 Step:   204500 Batch Loss:     0.000004 Tokens per Sec: 10389059, Lr: 0.000700
2020-02-26 21:34:06,614 Epoch 1843: total training loss 0.00072
2020-02-26 21:34:06,615 EPOCH 1844
2020-02-26 21:34:16,457 Epoch 1844: total training loss 0.00070
2020-02-26 21:34:16,458 EPOCH 1845
2020-02-26 21:34:22,127 Epoch 1845 Step:   204750 Batch Loss:     0.000007 Tokens per Sec: 10462511, Lr: 0.000700
2020-02-26 21:34:26,205 Epoch 1845: total training loss 0.00072
2020-02-26 21:34:26,205 EPOCH 1846
2020-02-26 21:34:36,045 Epoch 1846: total training loss 0.00073
2020-02-26 21:34:36,047 EPOCH 1847
2020-02-26 21:34:44,763 Epoch 1847 Step:   205000 Batch Loss:     0.000007 Tokens per Sec: 10018065, Lr: 0.000700
2020-02-26 21:34:46,281 Epoch 1847: total training loss 0.00070
2020-02-26 21:34:46,281 EPOCH 1848
2020-02-26 21:34:56,426 Epoch 1848: total training loss 0.00075
2020-02-26 21:34:56,426 EPOCH 1849
2020-02-26 21:35:06,377 Epoch 1849: total training loss 0.00072
2020-02-26 21:35:06,377 EPOCH 1850
2020-02-26 21:35:07,354 Epoch 1850 Step:   205250 Batch Loss:     0.000009 Tokens per Sec: 10016355, Lr: 0.000700
2020-02-26 21:35:16,442 Epoch 1850: total training loss 0.00069
2020-02-26 21:35:16,443 EPOCH 1851
2020-02-26 21:35:26,499 Epoch 1851: total training loss 0.00068
2020-02-26 21:35:26,500 EPOCH 1852
2020-02-26 21:35:29,979 Epoch 1852 Step:   205500 Batch Loss:     0.000008 Tokens per Sec: 10398881, Lr: 0.000700
2020-02-26 21:35:36,304 Epoch 1852: total training loss 0.00068
2020-02-26 21:35:36,304 EPOCH 1853
2020-02-26 21:35:46,219 Epoch 1853: total training loss 0.00069
2020-02-26 21:35:46,220 EPOCH 1854
2020-02-26 21:35:52,181 Epoch 1854 Step:   205750 Batch Loss:     0.000012 Tokens per Sec: 10355109, Lr: 0.000700
2020-02-26 21:35:56,140 Epoch 1854: total training loss 0.00090
2020-02-26 21:35:56,141 EPOCH 1855
2020-02-26 21:36:05,891 Epoch 1855: total training loss 0.00076
2020-02-26 21:36:05,891 EPOCH 1856
2020-02-26 21:36:14,097 Epoch 1856 Step:   206000 Batch Loss:     0.000005 Tokens per Sec: 10722026, Lr: 0.000700
2020-02-26 21:36:15,489 Epoch 1856: total training loss 0.00070
2020-02-26 21:36:15,489 EPOCH 1857
2020-02-26 21:36:25,033 Epoch 1857: total training loss 0.00070
2020-02-26 21:36:25,034 EPOCH 1858
2020-02-26 21:36:34,729 Epoch 1858: total training loss 0.00068
2020-02-26 21:36:34,730 EPOCH 1859
2020-02-26 21:36:35,841 Epoch 1859 Step:   206250 Batch Loss:     0.000003 Tokens per Sec: 10775996, Lr: 0.000700
2020-02-26 21:36:44,426 Epoch 1859: total training loss 0.00068
2020-02-26 21:36:44,427 EPOCH 1860
2020-02-26 21:36:54,256 Epoch 1860: total training loss 0.00067
2020-02-26 21:36:54,257 EPOCH 1861
2020-02-26 21:36:57,870 Epoch 1861 Step:   206500 Batch Loss:     0.000005 Tokens per Sec: 10546911, Lr: 0.000700
2020-02-26 21:37:04,087 Epoch 1861: total training loss 0.00067
2020-02-26 21:37:04,088 EPOCH 1862
2020-02-26 21:37:13,975 Epoch 1862: total training loss 0.00068
2020-02-26 21:37:13,975 EPOCH 1863
2020-02-26 21:37:20,183 Epoch 1863 Step:   206750 Batch Loss:     0.000007 Tokens per Sec: 10495287, Lr: 0.000700
2020-02-26 21:37:23,810 Epoch 1863: total training loss 0.00072
2020-02-26 21:37:23,810 EPOCH 1864
2020-02-26 21:37:33,712 Epoch 1864: total training loss 0.00069
2020-02-26 21:37:33,713 EPOCH 1865
2020-02-26 21:37:42,233 Epoch 1865 Step:   207000 Batch Loss:     0.000007 Tokens per Sec: 10386526, Lr: 0.000700
2020-02-26 21:37:43,620 Epoch 1865: total training loss 0.00069
2020-02-26 21:37:43,620 EPOCH 1866
2020-02-26 21:37:53,380 Epoch 1866: total training loss 0.00069
2020-02-26 21:37:53,381 EPOCH 1867
2020-02-26 21:38:03,285 Epoch 1867: total training loss 0.00068
2020-02-26 21:38:03,286 EPOCH 1868
2020-02-26 21:38:04,404 Epoch 1868 Step:   207250 Batch Loss:     0.000009 Tokens per Sec: 10120088, Lr: 0.000700
2020-02-26 21:38:13,085 Epoch 1868: total training loss 0.00068
2020-02-26 21:38:13,086 EPOCH 1869
2020-02-26 21:38:22,800 Epoch 1869: total training loss 0.00069
2020-02-26 21:38:22,801 EPOCH 1870
2020-02-26 21:38:26,453 Epoch 1870 Step:   207500 Batch Loss:     0.000004 Tokens per Sec: 10611262, Lr: 0.000700
2020-02-26 21:38:32,644 Epoch 1870: total training loss 0.00068
2020-02-26 21:38:32,645 EPOCH 1871
2020-02-26 21:38:42,760 Epoch 1871: total training loss 0.00067
2020-02-26 21:38:42,762 EPOCH 1872
2020-02-26 21:38:49,387 Epoch 1872 Step:   207750 Batch Loss:     0.000007 Tokens per Sec:  9901155, Lr: 0.000700
2020-02-26 21:38:53,163 Epoch 1872: total training loss 0.00069
2020-02-26 21:38:53,164 EPOCH 1873
2020-02-26 21:39:03,700 Epoch 1873: total training loss 0.00067
2020-02-26 21:39:03,701 EPOCH 1874
2020-02-26 21:39:12,677 Epoch 1874 Step:   208000 Batch Loss:     0.000008 Tokens per Sec:  9928028, Lr: 0.000700
2020-02-26 21:39:14,036 Epoch 1874: total training loss 0.00068
2020-02-26 21:39:14,037 EPOCH 1875
2020-02-26 21:39:24,115 Epoch 1875: total training loss 0.00069
2020-02-26 21:39:24,115 EPOCH 1876
2020-02-26 21:39:33,764 Epoch 1876: total training loss 0.00073
2020-02-26 21:39:33,765 EPOCH 1877
2020-02-26 21:39:34,874 Epoch 1877 Step:   208250 Batch Loss:     0.000007 Tokens per Sec: 10350013, Lr: 0.000700
2020-02-26 21:39:43,291 Epoch 1877: total training loss 0.00072
2020-02-26 21:39:43,291 EPOCH 1878
2020-02-26 21:39:52,812 Epoch 1878: total training loss 0.00071
2020-02-26 21:39:52,812 EPOCH 1879
2020-02-26 21:39:56,302 Epoch 1879 Step:   208500 Batch Loss:     0.000004 Tokens per Sec: 10414671, Lr: 0.000700
2020-02-26 21:40:02,417 Epoch 1879: total training loss 0.00070
2020-02-26 21:40:02,417 EPOCH 1880
2020-02-26 21:40:11,938 Epoch 1880: total training loss 0.00073
2020-02-26 21:40:11,938 EPOCH 1881
2020-02-26 21:40:17,993 Epoch 1881 Step:   208750 Batch Loss:     0.000009 Tokens per Sec: 10757986, Lr: 0.000700
2020-02-26 21:40:21,524 Epoch 1881: total training loss 0.00068
2020-02-26 21:40:21,524 EPOCH 1882
2020-02-26 21:40:31,141 Epoch 1882: total training loss 0.00069
2020-02-26 21:40:31,142 EPOCH 1883
2020-02-26 21:40:39,676 Epoch 1883 Step:   209000 Batch Loss:     0.000007 Tokens per Sec: 10634363, Lr: 0.000700
2020-02-26 21:40:40,815 Epoch 1883: total training loss 0.00069
2020-02-26 21:40:40,816 EPOCH 1884
2020-02-26 21:40:50,542 Epoch 1884: total training loss 0.00068
2020-02-26 21:40:50,543 EPOCH 1885
2020-02-26 21:41:00,578 Epoch 1885: total training loss 0.00067
2020-02-26 21:41:00,579 EPOCH 1886
2020-02-26 21:41:01,972 Epoch 1886 Step:   209250 Batch Loss:     0.000010 Tokens per Sec:  9835911, Lr: 0.000700
2020-02-26 21:41:10,879 Epoch 1886: total training loss 0.00068
2020-02-26 21:41:10,879 EPOCH 1887
2020-02-26 21:41:21,125 Epoch 1887: total training loss 0.00068
2020-02-26 21:41:21,126 EPOCH 1888
2020-02-26 21:41:25,155 Epoch 1888 Step:   209500 Batch Loss:     0.000007 Tokens per Sec: 10032513, Lr: 0.000700
2020-02-26 21:41:31,356 Epoch 1888: total training loss 0.00072
2020-02-26 21:41:31,357 EPOCH 1889
2020-02-26 21:41:41,648 Epoch 1889: total training loss 0.00070
2020-02-26 21:41:41,649 EPOCH 1890
2020-02-26 21:41:47,965 Epoch 1890 Step:   209750 Batch Loss:     0.000007 Tokens per Sec: 10288760, Lr: 0.000700
2020-02-26 21:41:51,625 Epoch 1890: total training loss 0.00070
2020-02-26 21:41:51,626 EPOCH 1891
2020-02-26 21:42:01,532 Epoch 1891: total training loss 0.00069
2020-02-26 21:42:01,532 EPOCH 1892
2020-02-26 21:42:10,243 Epoch 1892 Step:   210000 Batch Loss:     0.000010 Tokens per Sec: 10520363, Lr: 0.000700
2020-02-26 21:42:10,244 Model noise rate: 5
2020-02-26 21:43:10,830 Validation result at epoch 1892, step   210000: Val DTW Score:  10.67, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0404, GT DTW Score:      nan, duration: 60.5864s
2020-02-26 21:43:11,932 Epoch 1892: total training loss 0.00072
2020-02-26 21:43:11,932 EPOCH 1893
2020-02-26 21:43:21,920 Epoch 1893: total training loss 0.00070
2020-02-26 21:43:21,921 EPOCH 1894
2020-02-26 21:43:32,011 Epoch 1894: total training loss 0.00068
2020-02-26 21:43:32,011 EPOCH 1895
2020-02-26 21:43:33,545 Epoch 1895 Step:   210250 Batch Loss:     0.000005 Tokens per Sec:  9914058, Lr: 0.000700
2020-02-26 21:43:42,084 Epoch 1895: total training loss 0.00067
2020-02-26 21:43:42,084 EPOCH 1896
2020-02-26 21:43:52,114 Epoch 1896: total training loss 0.00069
2020-02-26 21:43:52,115 EPOCH 1897
2020-02-26 21:43:56,152 Epoch 1897 Step:   210500 Batch Loss:     0.000006 Tokens per Sec: 10010120, Lr: 0.000700
2020-02-26 21:44:02,260 Epoch 1897: total training loss 0.00068
2020-02-26 21:44:02,260 EPOCH 1898
2020-02-26 21:44:12,093 Epoch 1898: total training loss 0.00067
2020-02-26 21:44:12,094 EPOCH 1899
2020-02-26 21:44:18,367 Epoch 1899 Step:   210750 Batch Loss:     0.000005 Tokens per Sec: 10427718, Lr: 0.000700
2020-02-26 21:44:21,950 Epoch 1899: total training loss 0.00068
2020-02-26 21:44:21,950 EPOCH 1900
2020-02-26 21:44:31,856 Epoch 1900: total training loss 0.00068
2020-02-26 21:44:31,856 EPOCH 1901
2020-02-26 21:44:40,920 Epoch 1901 Step:   211000 Batch Loss:     0.000007 Tokens per Sec: 10189542, Lr: 0.000700
2020-02-26 21:44:41,905 Epoch 1901: total training loss 0.00070
2020-02-26 21:44:41,905 EPOCH 1902
2020-02-26 21:44:51,991 Epoch 1902: total training loss 0.00069
2020-02-26 21:44:51,992 EPOCH 1903
2020-02-26 21:45:01,764 Epoch 1903: total training loss 0.00069
2020-02-26 21:45:01,765 EPOCH 1904
2020-02-26 21:45:03,276 Epoch 1904 Step:   211250 Batch Loss:     0.000005 Tokens per Sec: 10225936, Lr: 0.000700
2020-02-26 21:45:11,567 Epoch 1904: total training loss 0.00068
2020-02-26 21:45:11,568 EPOCH 1905
2020-02-26 21:45:21,352 Epoch 1905: total training loss 0.00068
2020-02-26 21:45:21,352 EPOCH 1906
2020-02-26 21:45:25,151 Epoch 1906 Step:   211500 Batch Loss:     0.000009 Tokens per Sec: 10502558, Lr: 0.000700
2020-02-26 21:45:31,035 Epoch 1906: total training loss 0.00079
2020-02-26 21:45:31,035 EPOCH 1907
2020-02-26 21:45:40,774 Epoch 1907: total training loss 0.00074
2020-02-26 21:45:40,775 EPOCH 1908
2020-02-26 21:45:47,199 Epoch 1908 Step:   211750 Batch Loss:     0.000005 Tokens per Sec: 10571895, Lr: 0.000700
2020-02-26 21:45:50,582 Epoch 1908: total training loss 0.00070
2020-02-26 21:45:50,583 EPOCH 1909
2020-02-26 21:46:00,479 Epoch 1909: total training loss 0.00070
2020-02-26 21:46:00,480 EPOCH 1910
2020-02-26 21:46:09,755 Epoch 1910 Step:   212000 Batch Loss:     0.000005 Tokens per Sec: 10134665, Lr: 0.000700
2020-02-26 21:46:10,673 Epoch 1910: total training loss 0.00069
2020-02-26 21:46:10,674 EPOCH 1911
2020-02-26 21:46:20,974 Epoch 1911: total training loss 0.00068
2020-02-26 21:46:20,975 EPOCH 1912
2020-02-26 21:46:31,255 Epoch 1912: total training loss 0.00067
2020-02-26 21:46:31,255 EPOCH 1913
2020-02-26 21:46:32,888 Epoch 1913 Step:   212250 Batch Loss:     0.000006 Tokens per Sec: 10076433, Lr: 0.000700
2020-02-26 21:46:41,290 Epoch 1913: total training loss 0.00067
2020-02-26 21:46:41,290 EPOCH 1914
2020-02-26 21:46:51,193 Epoch 1914: total training loss 0.00067
2020-02-26 21:46:51,194 EPOCH 1915
2020-02-26 21:46:55,300 Epoch 1915 Step:   212500 Batch Loss:     0.000005 Tokens per Sec: 10373124, Lr: 0.000700
2020-02-26 21:47:00,965 Epoch 1915: total training loss 0.00068
2020-02-26 21:47:00,965 EPOCH 1916
2020-02-26 21:47:10,736 Epoch 1916: total training loss 0.00069
2020-02-26 21:47:10,737 EPOCH 1917
2020-02-26 21:47:17,122 Epoch 1917 Step:   212750 Batch Loss:     0.000004 Tokens per Sec: 10582890, Lr: 0.000700
2020-02-26 21:47:20,459 Epoch 1917: total training loss 0.00067
2020-02-26 21:47:20,460 EPOCH 1918
2020-02-26 21:47:30,227 Epoch 1918: total training loss 0.00069
2020-02-26 21:47:30,228 EPOCH 1919
2020-02-26 21:47:39,461 Epoch 1919 Step:   213000 Batch Loss:     0.000006 Tokens per Sec: 10165398, Lr: 0.000700
2020-02-26 21:47:40,352 Epoch 1919: total training loss 0.00068
2020-02-26 21:47:40,352 EPOCH 1920
2020-02-26 21:47:50,561 Epoch 1920: total training loss 0.00069
2020-02-26 21:47:50,562 EPOCH 1921
2020-02-26 21:48:00,795 Epoch 1921: total training loss 0.00073
2020-02-26 21:48:00,795 EPOCH 1922
2020-02-26 21:48:02,530 Epoch 1922 Step:   213250 Batch Loss:     0.000007 Tokens per Sec:  9993360, Lr: 0.000700
2020-02-26 21:48:11,299 Epoch 1922: total training loss 0.00071
2020-02-26 21:48:11,300 EPOCH 1923
2020-02-26 21:48:21,679 Epoch 1923: total training loss 0.00068
2020-02-26 21:48:21,681 EPOCH 1924
2020-02-26 21:48:25,979 Epoch 1924 Step:   213500 Batch Loss:     0.000005 Tokens per Sec: 10005284, Lr: 0.000700
2020-02-26 21:48:31,899 Epoch 1924: total training loss 0.00069
2020-02-26 21:48:31,899 EPOCH 1925
2020-02-26 21:48:42,363 Epoch 1925: total training loss 0.00069
2020-02-26 21:48:42,363 EPOCH 1926
2020-02-26 21:48:49,486 Epoch 1926 Step:   213750 Batch Loss:     0.000005 Tokens per Sec:  9822487, Lr: 0.000700
2020-02-26 21:48:52,762 Epoch 1926: total training loss 0.00068
2020-02-26 21:48:52,762 EPOCH 1927
2020-02-26 21:49:03,073 Epoch 1927: total training loss 0.00068
2020-02-26 21:49:03,074 EPOCH 1928
2020-02-26 21:49:12,703 Epoch 1928 Step:   214000 Batch Loss:     0.000005 Tokens per Sec:  9916065, Lr: 0.000700
2020-02-26 21:49:13,415 Epoch 1928: total training loss 0.00068
2020-02-26 21:49:13,416 EPOCH 1929
2020-02-26 21:49:23,611 Epoch 1929: total training loss 0.00068
2020-02-26 21:49:23,611 EPOCH 1930
2020-02-26 21:49:33,855 Epoch 1930: total training loss 0.00069
2020-02-26 21:49:33,856 EPOCH 1931
2020-02-26 21:49:35,608 Epoch 1931 Step:   214250 Batch Loss:     0.000006 Tokens per Sec: 10463964, Lr: 0.000700
2020-02-26 21:49:43,831 Epoch 1931: total training loss 0.00067
2020-02-26 21:49:43,832 EPOCH 1932
2020-02-26 21:49:53,639 Epoch 1932: total training loss 0.00070
2020-02-26 21:49:53,640 EPOCH 1933
2020-02-26 21:49:57,620 Epoch 1933 Step:   214500 Batch Loss:     0.000004 Tokens per Sec: 10492391, Lr: 0.000700
2020-02-26 21:50:03,428 Epoch 1933: total training loss 0.00071
2020-02-26 21:50:03,429 EPOCH 1934
2020-02-26 21:50:13,231 Epoch 1934: total training loss 0.00071
2020-02-26 21:50:13,232 EPOCH 1935
2020-02-26 21:50:19,817 Epoch 1935 Step:   214750 Batch Loss:     0.000006 Tokens per Sec: 10625599, Lr: 0.000700
2020-02-26 21:50:22,840 Epoch 1935: total training loss 0.00071
2020-02-26 21:50:22,841 EPOCH 1936
2020-02-26 21:50:32,805 Epoch 1936: total training loss 0.00068
2020-02-26 21:50:32,806 EPOCH 1937
2020-02-26 21:50:42,365 Epoch 1937 Step:   215000 Batch Loss:     0.000004 Tokens per Sec: 10008341, Lr: 0.000700
2020-02-26 21:50:43,076 Epoch 1937: total training loss 0.00073
2020-02-26 21:50:43,076 EPOCH 1938
2020-02-26 21:50:53,144 Epoch 1938: total training loss 0.00073
2020-02-26 21:50:53,144 EPOCH 1939
2020-02-26 21:51:03,131 Epoch 1939: total training loss 0.00068
2020-02-26 21:51:03,132 EPOCH 1940
2020-02-26 21:51:05,149 Epoch 1940 Step:   215250 Batch Loss:     0.000007 Tokens per Sec: 10033679, Lr: 0.000700
2020-02-26 21:51:13,314 Epoch 1940: total training loss 0.00067
2020-02-26 21:51:13,316 EPOCH 1941
2020-02-26 21:51:23,439 Epoch 1941: total training loss 0.00067
2020-02-26 21:51:23,440 EPOCH 1942
2020-02-26 21:51:27,845 Epoch 1942 Step:   215500 Batch Loss:     0.000007 Tokens per Sec: 10512526, Lr: 0.000700
2020-02-26 21:51:33,348 Epoch 1942: total training loss 0.00068
2020-02-26 21:51:33,348 EPOCH 1943
2020-02-26 21:51:43,225 Epoch 1943: total training loss 0.00069
2020-02-26 21:51:43,226 EPOCH 1944
2020-02-26 21:51:49,849 Epoch 1944 Step:   215750 Batch Loss:     0.000005 Tokens per Sec: 10296315, Lr: 0.000700
2020-02-26 21:51:53,140 Epoch 1944: total training loss 0.00070
2020-02-26 21:51:53,140 EPOCH 1945
2020-02-26 21:52:03,220 Epoch 1945: total training loss 0.00073
2020-02-26 21:52:03,220 EPOCH 1946
2020-02-26 21:52:12,659 Epoch 1946 Step:   216000 Batch Loss:     0.000007 Tokens per Sec: 10348258, Lr: 0.000700
2020-02-26 21:52:13,226 Epoch 1946: total training loss 0.00068
2020-02-26 21:52:13,227 EPOCH 1947
2020-02-26 21:52:23,053 Epoch 1947: total training loss 0.00067
2020-02-26 21:52:23,054 EPOCH 1948
2020-02-26 21:52:33,062 Epoch 1948: total training loss 0.00071
2020-02-26 21:52:33,062 EPOCH 1949
2020-02-26 21:52:34,979 Epoch 1949 Step:   216250 Batch Loss:     0.000008 Tokens per Sec: 10219094, Lr: 0.000700
2020-02-26 21:52:43,042 Epoch 1949: total training loss 0.00073
2020-02-26 21:52:43,042 EPOCH 1950
2020-02-26 21:52:53,085 Epoch 1950: total training loss 0.00072
2020-02-26 21:52:53,086 EPOCH 1951
2020-02-26 21:52:57,664 Epoch 1951 Step:   216500 Batch Loss:     0.000004 Tokens per Sec: 10521222, Lr: 0.000700
2020-02-26 21:53:03,006 Epoch 1951: total training loss 0.00070
2020-02-26 21:53:03,007 EPOCH 1952
2020-02-26 21:53:12,976 Epoch 1952: total training loss 0.00068
2020-02-26 21:53:12,977 EPOCH 1953
2020-02-26 21:53:19,875 Epoch 1953 Step:   216750 Batch Loss:     0.000006 Tokens per Sec: 10553012, Lr: 0.000700
2020-02-26 21:53:22,780 Epoch 1953: total training loss 0.00066
2020-02-26 21:53:22,780 EPOCH 1954
2020-02-26 21:53:32,515 Epoch 1954: total training loss 0.00067
2020-02-26 21:53:32,516 EPOCH 1955
2020-02-26 21:53:41,941 Epoch 1955 Step:   217000 Batch Loss:     0.000005 Tokens per Sec: 10402001, Lr: 0.000700
2020-02-26 21:53:42,391 Epoch 1955: total training loss 0.00067
2020-02-26 21:53:42,392 EPOCH 1956
2020-02-26 21:53:52,130 Epoch 1956: total training loss 0.00067
2020-02-26 21:53:52,131 EPOCH 1957
2020-02-26 21:54:02,070 Epoch 1957: total training loss 0.00066
2020-02-26 21:54:02,070 EPOCH 1958
2020-02-26 21:54:04,163 Epoch 1958 Step:   217250 Batch Loss:     0.000008 Tokens per Sec: 10706543, Lr: 0.000700
2020-02-26 21:54:11,914 Epoch 1958: total training loss 0.00067
2020-02-26 21:54:11,914 EPOCH 1959
2020-02-26 21:54:21,759 Epoch 1959: total training loss 0.00068
2020-02-26 21:54:21,759 EPOCH 1960
2020-02-26 21:54:26,176 Epoch 1960 Step:   217500 Batch Loss:     0.000003 Tokens per Sec: 10499879, Lr: 0.000700
2020-02-26 21:54:31,620 Epoch 1960: total training loss 0.00069
2020-02-26 21:54:31,620 EPOCH 1961
2020-02-26 21:54:41,370 Epoch 1961: total training loss 0.00074
2020-02-26 21:54:41,371 EPOCH 1962
2020-02-26 21:54:48,341 Epoch 1962 Step:   217750 Batch Loss:     0.000010 Tokens per Sec: 10425460, Lr: 0.000700
2020-02-26 21:54:51,125 Epoch 1962: total training loss 0.00070
2020-02-26 21:54:51,125 EPOCH 1963
2020-02-26 21:55:00,920 Epoch 1963: total training loss 0.00070
2020-02-26 21:55:00,921 EPOCH 1964
2020-02-26 21:55:10,311 Epoch 1964 Step:   218000 Batch Loss:     0.000003 Tokens per Sec: 10488553, Lr: 0.000700
2020-02-26 21:55:10,702 Epoch 1964: total training loss 0.00070
2020-02-26 21:55:10,702 EPOCH 1965
2020-02-26 21:55:20,462 Epoch 1965: total training loss 0.00071
2020-02-26 21:55:20,462 EPOCH 1966
2020-02-26 21:55:30,316 Epoch 1966: total training loss 0.00074
2020-02-26 21:55:30,316 EPOCH 1967
2020-02-26 21:55:32,472 Epoch 1967 Step:   218250 Batch Loss:     0.000005 Tokens per Sec: 10712235, Lr: 0.000700
2020-02-26 21:55:40,116 Epoch 1967: total training loss 0.00068
2020-02-26 21:55:40,116 EPOCH 1968
2020-02-26 21:55:49,974 Epoch 1968: total training loss 0.00066
2020-02-26 21:55:49,974 EPOCH 1969
2020-02-26 21:55:54,619 Epoch 1969 Step:   218500 Batch Loss:     0.000003 Tokens per Sec: 10445113, Lr: 0.000700
2020-02-26 21:55:59,783 Epoch 1969: total training loss 0.00066
2020-02-26 21:55:59,784 EPOCH 1970
2020-02-26 21:56:09,657 Epoch 1970: total training loss 0.00068
2020-02-26 21:56:09,658 EPOCH 1971
2020-02-26 21:56:16,731 Epoch 1971 Step:   218750 Batch Loss:     0.000007 Tokens per Sec: 10257547, Lr: 0.000700
2020-02-26 21:56:19,605 Epoch 1971: total training loss 0.00068
2020-02-26 21:56:19,606 EPOCH 1972
2020-02-26 21:56:29,558 Epoch 1972: total training loss 0.00071
2020-02-26 21:56:29,559 EPOCH 1973
2020-02-26 21:56:39,193 Epoch 1973 Step:   219000 Batch Loss:     0.000007 Tokens per Sec: 10460725, Lr: 0.000700
2020-02-26 21:56:39,446 Epoch 1973: total training loss 0.00068
2020-02-26 21:56:39,446 EPOCH 1974
2020-02-26 21:56:49,203 Epoch 1974: total training loss 0.00067
2020-02-26 21:56:49,204 EPOCH 1975
2020-02-26 21:56:59,217 Epoch 1975: total training loss 0.00068
2020-02-26 21:56:59,218 EPOCH 1976
2020-02-26 21:57:01,477 Epoch 1976 Step:   219250 Batch Loss:     0.000004 Tokens per Sec: 10202424, Lr: 0.000700
2020-02-26 21:57:09,187 Epoch 1976: total training loss 0.00067
2020-02-26 21:57:09,187 EPOCH 1977
2020-02-26 21:57:18,766 Epoch 1977: total training loss 0.00067
2020-02-26 21:57:18,766 EPOCH 1978
2020-02-26 21:57:23,428 Epoch 1978 Step:   219500 Batch Loss:     0.000006 Tokens per Sec: 10556379, Lr: 0.000700
2020-02-26 21:57:28,556 Epoch 1978: total training loss 0.00068
2020-02-26 21:57:28,556 EPOCH 1979
2020-02-26 21:57:38,426 Epoch 1979: total training loss 0.00067
2020-02-26 21:57:38,427 EPOCH 1980
2020-02-26 21:57:45,782 Epoch 1980 Step:   219750 Batch Loss:     0.000006 Tokens per Sec: 10268677, Lr: 0.000700
2020-02-26 21:57:48,386 Epoch 1980: total training loss 0.00068
2020-02-26 21:57:48,387 EPOCH 1981
2020-02-26 21:57:58,526 Epoch 1981: total training loss 0.00070
2020-02-26 21:57:58,529 EPOCH 1982
2020-02-26 21:58:08,358 Epoch 1982 Step:   220000 Batch Loss:     0.000005 Tokens per Sec: 10224038, Lr: 0.000700
2020-02-26 21:58:08,359 Model noise rate: 5
2020-02-26 21:59:11,579 Validation result at epoch 1982, step   220000: Val DTW Score:  10.68, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0407, GT DTW Score:      nan, duration: 63.2204s
2020-02-26 21:59:11,809 Epoch 1982: total training loss 0.00070
2020-02-26 21:59:11,809 EPOCH 1983
2020-02-26 21:59:22,171 Epoch 1983: total training loss 0.00068
2020-02-26 21:59:22,171 EPOCH 1984
2020-02-26 21:59:32,643 Epoch 1984: total training loss 0.00068
2020-02-26 21:59:32,643 EPOCH 1985
2020-02-26 21:59:35,067 Epoch 1985 Step:   220250 Batch Loss:     0.000009 Tokens per Sec:  9573578, Lr: 0.000700
2020-02-26 21:59:42,963 Epoch 1985: total training loss 0.00068
2020-02-26 21:59:42,964 EPOCH 1986
2020-02-26 21:59:52,950 Epoch 1986: total training loss 0.00077
2020-02-26 21:59:52,950 EPOCH 1987
2020-02-26 21:59:57,847 Epoch 1987 Step:   220500 Batch Loss:     0.000007 Tokens per Sec: 10388075, Lr: 0.000700
2020-02-26 22:00:03,000 Epoch 1987: total training loss 0.00069
2020-02-26 22:00:03,001 EPOCH 1988
2020-02-26 22:00:12,915 Epoch 1988: total training loss 0.00069
2020-02-26 22:00:12,915 EPOCH 1989
2020-02-26 22:00:20,215 Epoch 1989 Step:   220750 Batch Loss:     0.000008 Tokens per Sec: 10443382, Lr: 0.000700
2020-02-26 22:00:22,766 Epoch 1989: total training loss 0.00068
2020-02-26 22:00:22,766 EPOCH 1990
2020-02-26 22:00:32,516 Epoch 1990: total training loss 0.00069
2020-02-26 22:00:32,517 EPOCH 1991
2020-02-26 22:00:42,216 Epoch 1991 Step:   221000 Batch Loss:     0.000006 Tokens per Sec: 10422036, Lr: 0.000700
2020-02-26 22:00:42,292 Epoch 1991: total training loss 0.00069
2020-02-26 22:00:42,292 EPOCH 1992
2020-02-26 22:00:52,150 Epoch 1992: total training loss 0.00069
2020-02-26 22:00:52,151 EPOCH 1993
2020-02-26 22:01:02,007 Epoch 1993: total training loss 0.00067
2020-02-26 22:01:02,008 EPOCH 1994
2020-02-26 22:01:04,433 Epoch 1994 Step:   221250 Batch Loss:     0.000007 Tokens per Sec: 10397962, Lr: 0.000700
2020-02-26 22:01:11,883 Epoch 1994: total training loss 0.00068
2020-02-26 22:01:11,884 EPOCH 1995
2020-02-26 22:01:21,725 Epoch 1995: total training loss 0.00067
2020-02-26 22:01:21,726 EPOCH 1996
2020-02-26 22:01:26,434 Epoch 1996 Step:   221500 Batch Loss:     0.000005 Tokens per Sec: 10451141, Lr: 0.000700
2020-02-26 22:01:31,572 Epoch 1996: total training loss 0.00068
2020-02-26 22:01:31,572 EPOCH 1997
2020-02-26 22:01:41,641 Epoch 1997: total training loss 0.00068
2020-02-26 22:01:41,643 EPOCH 1998
2020-02-26 22:01:49,392 Epoch 1998 Step:   221750 Batch Loss:     0.000007 Tokens per Sec:  9955906, Lr: 0.000700
2020-02-26 22:01:51,877 Epoch 1998: total training loss 0.00068
2020-02-26 22:01:51,878 EPOCH 1999
2020-02-26 22:02:02,230 Epoch 1999: total training loss 0.00069
2020-02-26 22:02:02,231 EPOCH 2000
2020-02-26 22:02:12,497 Epoch 2000 Step:   222000 Batch Loss:     0.000010 Tokens per Sec: 10057478, Lr: 0.000700
2020-02-26 22:02:12,498 Epoch 2000: total training loss 0.00069
2020-02-26 22:02:12,498 EPOCH 2001
2020-02-26 22:02:22,923 Epoch 2001: total training loss 0.00069
2020-02-26 22:02:22,924 EPOCH 2002
2020-02-26 22:02:33,250 Epoch 2002: total training loss 0.00071
2020-02-26 22:02:33,251 EPOCH 2003
2020-02-26 22:02:36,096 Epoch 2003 Step:   222250 Batch Loss:     0.000005 Tokens per Sec: 10384641, Lr: 0.000700
2020-02-26 22:02:43,359 Epoch 2003: total training loss 0.00067
2020-02-26 22:02:43,360 EPOCH 2004
2020-02-26 22:02:53,493 Epoch 2004: total training loss 0.00069
2020-02-26 22:02:53,494 EPOCH 2005
2020-02-26 22:02:58,746 Epoch 2005 Step:   222500 Batch Loss:     0.000007 Tokens per Sec: 10153863, Lr: 0.000700
2020-02-26 22:03:03,648 Epoch 2005: total training loss 0.00068
2020-02-26 22:03:03,648 EPOCH 2006
2020-02-26 22:03:13,621 Epoch 2006: total training loss 0.00072
2020-02-26 22:03:13,622 EPOCH 2007
2020-02-26 22:03:20,897 Epoch 2007 Step:   222750 Batch Loss:     0.000010 Tokens per Sec: 10219978, Lr: 0.000700
2020-02-26 22:03:23,574 Epoch 2007: total training loss 0.00068
2020-02-26 22:03:23,574 EPOCH 2008
2020-02-26 22:03:33,534 Epoch 2008: total training loss 0.00070
2020-02-26 22:03:33,534 EPOCH 2009
2020-02-26 22:03:43,433 Epoch 2009: total training loss 0.00067
2020-02-26 22:03:43,433 EPOCH 2010
2020-02-26 22:03:43,569 Epoch 2010 Step:   223000 Batch Loss:     0.000007 Tokens per Sec:  9246819, Lr: 0.000700
2020-02-26 22:03:53,454 Epoch 2010: total training loss 0.00066
2020-02-26 22:03:53,455 EPOCH 2011
2020-02-26 22:04:03,453 Epoch 2011: total training loss 0.00068
2020-02-26 22:04:03,455 EPOCH 2012
2020-02-26 22:04:05,987 Epoch 2012 Step:   223250 Batch Loss:     0.000006 Tokens per Sec: 10196085, Lr: 0.000700
2020-02-26 22:04:13,581 Epoch 2012: total training loss 0.00067
2020-02-26 22:04:13,583 EPOCH 2013
2020-02-26 22:04:24,009 Epoch 2013: total training loss 0.00066
2020-02-26 22:04:24,010 EPOCH 2014
2020-02-26 22:04:29,530 Epoch 2014 Step:   223500 Batch Loss:     0.000006 Tokens per Sec:  9763666, Lr: 0.000700
2020-02-26 22:04:34,675 Epoch 2014: total training loss 0.00067
2020-02-26 22:04:34,676 EPOCH 2015
2020-02-26 22:04:45,186 Epoch 2015: total training loss 0.00071
2020-02-26 22:04:45,186 EPOCH 2016
2020-02-26 22:04:53,142 Epoch 2016 Step:   223750 Batch Loss:     0.000007 Tokens per Sec: 10105536, Lr: 0.000700
2020-02-26 22:04:55,390 Epoch 2016: total training loss 0.00069
2020-02-26 22:04:55,391 EPOCH 2017
2020-02-26 22:05:05,707 Epoch 2017: total training loss 0.00071
2020-02-26 22:05:05,707 EPOCH 2018
2020-02-26 22:05:15,780 Epoch 2018: total training loss 0.00070
2020-02-26 22:05:15,780 EPOCH 2019
2020-02-26 22:05:15,997 Epoch 2019 Step:   224000 Batch Loss:     0.000008 Tokens per Sec:  9400803, Lr: 0.000700
2020-02-26 22:05:25,711 Epoch 2019: total training loss 0.00066
2020-02-26 22:05:25,712 EPOCH 2020
2020-02-26 22:05:35,812 Epoch 2020: total training loss 0.00066
2020-02-26 22:05:35,814 EPOCH 2021
2020-02-26 22:05:38,685 Epoch 2021 Step:   224250 Batch Loss:     0.000007 Tokens per Sec: 10214610, Lr: 0.000700
2020-02-26 22:05:45,839 Epoch 2021: total training loss 0.00065
2020-02-26 22:05:45,839 EPOCH 2022
2020-02-26 22:05:55,895 Epoch 2022: total training loss 0.00066
2020-02-26 22:05:55,896 EPOCH 2023
2020-02-26 22:06:01,245 Epoch 2023 Step:   224500 Batch Loss:     0.000010 Tokens per Sec: 10356786, Lr: 0.000700
2020-02-26 22:06:05,847 Epoch 2023: total training loss 0.00066
2020-02-26 22:06:05,848 EPOCH 2024
2020-02-26 22:06:15,867 Epoch 2024: total training loss 0.00067
2020-02-26 22:06:15,868 EPOCH 2025
2020-02-26 22:06:23,418 Epoch 2025 Step:   224750 Batch Loss:     0.000007 Tokens per Sec: 10430665, Lr: 0.000700
2020-02-26 22:06:25,806 Epoch 2025: total training loss 0.00066
2020-02-26 22:06:25,806 EPOCH 2026
2020-02-26 22:06:35,804 Epoch 2026: total training loss 0.00073
2020-02-26 22:06:35,805 EPOCH 2027
2020-02-26 22:06:46,030 Epoch 2027: total training loss 0.00068
2020-02-26 22:06:46,032 EPOCH 2028
2020-02-26 22:06:46,374 Epoch 2028 Step:   225000 Batch Loss:     0.000007 Tokens per Sec:  8165845, Lr: 0.000700
2020-02-26 22:06:56,407 Epoch 2028: total training loss 0.00068
2020-02-26 22:06:56,407 EPOCH 2029
2020-02-26 22:07:06,635 Epoch 2029: total training loss 0.00067
2020-02-26 22:07:06,636 EPOCH 2030
2020-02-26 22:07:09,583 Epoch 2030 Step:   225250 Batch Loss:     0.000009 Tokens per Sec:  9591406, Lr: 0.000700
2020-02-26 22:07:17,203 Epoch 2030: total training loss 0.00067
2020-02-26 22:07:17,204 EPOCH 2031
2020-02-26 22:07:27,647 Epoch 2031: total training loss 0.00069
2020-02-26 22:07:27,648 EPOCH 2032
2020-02-26 22:07:33,011 Epoch 2032 Step:   225500 Batch Loss:     0.000005 Tokens per Sec: 10294040, Lr: 0.000700
2020-02-26 22:07:37,828 Epoch 2032: total training loss 0.00069
2020-02-26 22:07:37,828 EPOCH 2033
2020-02-26 22:07:47,992 Epoch 2033: total training loss 0.00071
2020-02-26 22:07:47,992 EPOCH 2034
2020-02-26 22:07:55,847 Epoch 2034 Step:   225750 Batch Loss:     0.000009 Tokens per Sec: 10020033, Lr: 0.000700
2020-02-26 22:07:58,102 Epoch 2034: total training loss 0.00070
2020-02-26 22:07:58,103 EPOCH 2035
2020-02-26 22:08:08,079 Epoch 2035: total training loss 0.00069
2020-02-26 22:08:08,080 EPOCH 2036
2020-02-26 22:08:18,061 Epoch 2036: total training loss 0.00069
2020-02-26 22:08:18,062 EPOCH 2037
2020-02-26 22:08:18,437 Epoch 2037 Step:   226000 Batch Loss:     0.000003 Tokens per Sec:  9646036, Lr: 0.000700
2020-02-26 22:08:27,990 Epoch 2037: total training loss 0.00070
2020-02-26 22:08:27,990 EPOCH 2038
2020-02-26 22:08:37,875 Epoch 2038: total training loss 0.00069
2020-02-26 22:08:37,876 EPOCH 2039
2020-02-26 22:08:40,650 Epoch 2039 Step:   226250 Batch Loss:     0.000008 Tokens per Sec: 10229812, Lr: 0.000700
2020-02-26 22:08:47,858 Epoch 2039: total training loss 0.00066
2020-02-26 22:08:47,859 EPOCH 2040
2020-02-26 22:08:57,892 Epoch 2040: total training loss 0.00065
2020-02-26 22:08:57,894 EPOCH 2041
2020-02-26 22:09:03,218 Epoch 2041 Step:   226500 Batch Loss:     0.000005 Tokens per Sec: 10356001, Lr: 0.000700
2020-02-26 22:09:07,821 Epoch 2041: total training loss 0.00066
2020-02-26 22:09:07,821 EPOCH 2042
2020-02-26 22:09:17,869 Epoch 2042: total training loss 0.00065
2020-02-26 22:09:17,870 EPOCH 2043
2020-02-26 22:09:25,816 Epoch 2043 Step:   226750 Batch Loss:     0.000006 Tokens per Sec: 10332620, Lr: 0.000700
2020-02-26 22:09:27,875 Epoch 2043: total training loss 0.00065
2020-02-26 22:09:27,875 EPOCH 2044
2020-02-26 22:09:38,478 Epoch 2044: total training loss 0.00066
2020-02-26 22:09:38,479 EPOCH 2045
2020-02-26 22:09:48,946 Epoch 2045: total training loss 0.00067
2020-02-26 22:09:48,946 EPOCH 2046
2020-02-26 22:09:49,478 Epoch 2046 Step:   227000 Batch Loss:     0.000008 Tokens per Sec:  9623454, Lr: 0.000700
2020-02-26 22:09:59,329 Epoch 2046: total training loss 0.00071
2020-02-26 22:09:59,330 EPOCH 2047
2020-02-26 22:10:09,701 Epoch 2047: total training loss 0.00071
2020-02-26 22:10:09,702 EPOCH 2048
2020-02-26 22:10:12,691 Epoch 2048 Step:   227250 Batch Loss:     0.000005 Tokens per Sec: 10075688, Lr: 0.000700
2020-02-26 22:10:19,716 Epoch 2048: total training loss 0.00071
2020-02-26 22:10:19,716 EPOCH 2049
2020-02-26 22:10:29,524 Epoch 2049: total training loss 0.00066
2020-02-26 22:10:29,525 EPOCH 2050
2020-02-26 22:10:35,082 Epoch 2050 Step:   227500 Batch Loss:     0.000008 Tokens per Sec: 10042436, Lr: 0.000700
2020-02-26 22:10:39,639 Epoch 2050: total training loss 0.00066
2020-02-26 22:10:39,639 EPOCH 2051
2020-02-26 22:10:49,817 Epoch 2051: total training loss 0.00068
2020-02-26 22:10:49,821 EPOCH 2052
2020-02-26 22:10:57,859 Epoch 2052 Step:   227750 Batch Loss:     0.000007 Tokens per Sec: 10303578, Lr: 0.000700
2020-02-26 22:10:59,850 Epoch 2052: total training loss 0.00066
2020-02-26 22:10:59,851 EPOCH 2053
2020-02-26 22:11:09,821 Epoch 2053: total training loss 0.00066
2020-02-26 22:11:09,822 EPOCH 2054
2020-02-26 22:11:19,849 Epoch 2054: total training loss 0.00066
2020-02-26 22:11:19,849 EPOCH 2055
2020-02-26 22:11:20,356 Epoch 2055 Step:   228000 Batch Loss:     0.000005 Tokens per Sec:  9944577, Lr: 0.000700
2020-02-26 22:11:29,819 Epoch 2055: total training loss 0.00067
2020-02-26 22:11:29,819 EPOCH 2056
2020-02-26 22:11:39,744 Epoch 2056: total training loss 0.00067
2020-02-26 22:11:39,744 EPOCH 2057
2020-02-26 22:11:42,811 Epoch 2057 Step:   228250 Batch Loss:     0.000005 Tokens per Sec: 10293074, Lr: 0.000700
2020-02-26 22:11:49,875 Epoch 2057: total training loss 0.00066
2020-02-26 22:11:49,876 EPOCH 2058
2020-02-26 22:11:59,795 Epoch 2058: total training loss 0.00068
2020-02-26 22:11:59,797 EPOCH 2059
2020-02-26 22:12:05,722 Epoch 2059 Step:   228500 Batch Loss:     0.000004 Tokens per Sec:  9858765, Lr: 0.000700
2020-02-26 22:12:10,009 Epoch 2059: total training loss 0.00072
2020-02-26 22:12:10,009 EPOCH 2060
2020-02-26 22:12:20,453 Epoch 2060: total training loss 0.00068
2020-02-26 22:12:20,455 EPOCH 2061
2020-02-26 22:12:29,061 Epoch 2061 Step:   228750 Batch Loss:     0.000006 Tokens per Sec:  9678452, Lr: 0.000700
2020-02-26 22:12:31,118 Epoch 2061: total training loss 0.00066
2020-02-26 22:12:31,119 EPOCH 2062
2020-02-26 22:12:41,736 Epoch 2062: total training loss 0.00066
2020-02-26 22:12:41,736 EPOCH 2063
2020-02-26 22:12:52,231 Epoch 2063: total training loss 0.00067
2020-02-26 22:12:52,232 EPOCH 2064
2020-02-26 22:12:52,856 Epoch 2064 Step:   229000 Batch Loss:     0.000006 Tokens per Sec:  9790317, Lr: 0.000700
2020-02-26 22:13:02,361 Epoch 2064: total training loss 0.00066
2020-02-26 22:13:02,362 EPOCH 2065
2020-02-26 22:13:12,524 Epoch 2065: total training loss 0.00067
2020-02-26 22:13:12,525 EPOCH 2066
2020-02-26 22:13:15,667 Epoch 2066 Step:   229250 Batch Loss:     0.000003 Tokens per Sec:  9881442, Lr: 0.000700
2020-02-26 22:13:22,801 Epoch 2066: total training loss 0.00070
2020-02-26 22:13:22,801 EPOCH 2067
2020-02-26 22:13:32,772 Epoch 2067: total training loss 0.00073
2020-02-26 22:13:32,773 EPOCH 2068
2020-02-26 22:13:38,500 Epoch 2068 Step:   229500 Batch Loss:     0.000004 Tokens per Sec: 10392968, Lr: 0.000700
2020-02-26 22:13:42,674 Epoch 2068: total training loss 0.00067
2020-02-26 22:13:42,675 EPOCH 2069
2020-02-26 22:13:52,659 Epoch 2069: total training loss 0.00066
2020-02-26 22:13:52,660 EPOCH 2070
2020-02-26 22:14:00,934 Epoch 2070 Step:   229750 Batch Loss:     0.000006 Tokens per Sec: 10298636, Lr: 0.000700
2020-02-26 22:14:02,694 Epoch 2070: total training loss 0.00067
2020-02-26 22:14:02,694 EPOCH 2071
2020-02-26 22:14:12,775 Epoch 2071: total training loss 0.00066
2020-02-26 22:14:12,775 EPOCH 2072
2020-02-26 22:14:22,802 Epoch 2072: total training loss 0.00066
2020-02-26 22:14:22,803 EPOCH 2073
2020-02-26 22:14:23,523 Epoch 2073 Step:   230000 Batch Loss:     0.000008 Tokens per Sec:  9991744, Lr: 0.000700
2020-02-26 22:14:23,523 Model noise rate: 5
2020-02-26 22:15:26,845 Validation result at epoch 2073, step   230000: Val DTW Score:  10.68, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0409, GT DTW Score:      nan, duration: 63.3212s
2020-02-26 22:15:36,158 Epoch 2073: total training loss 0.00069
2020-02-26 22:15:36,159 EPOCH 2074
2020-02-26 22:15:46,148 Epoch 2074: total training loss 0.00071
2020-02-26 22:15:46,148 EPOCH 2075
2020-02-26 22:15:49,472 Epoch 2075 Step:   230250 Batch Loss:     0.000004 Tokens per Sec: 10323861, Lr: 0.000700
2020-02-26 22:15:56,124 Epoch 2075: total training loss 0.00067
2020-02-26 22:15:56,124 EPOCH 2076
2020-02-26 22:16:06,049 Epoch 2076: total training loss 0.00067
2020-02-26 22:16:06,049 EPOCH 2077
2020-02-26 22:16:11,761 Epoch 2077 Step:   230500 Batch Loss:     0.000003 Tokens per Sec: 10384791, Lr: 0.000700
2020-02-26 22:16:16,042 Epoch 2077: total training loss 0.00071
2020-02-26 22:16:16,043 EPOCH 2078
2020-02-26 22:16:25,995 Epoch 2078: total training loss 0.00073
2020-02-26 22:16:25,996 EPOCH 2079
2020-02-26 22:16:34,141 Epoch 2079 Step:   230750 Batch Loss:     0.000006 Tokens per Sec: 10410135, Lr: 0.000700
2020-02-26 22:16:35,833 Epoch 2079: total training loss 0.00067
2020-02-26 22:16:35,834 EPOCH 2080
2020-02-26 22:16:45,795 Epoch 2080: total training loss 0.00065
2020-02-26 22:16:45,795 EPOCH 2081
2020-02-26 22:16:55,685 Epoch 2081: total training loss 0.00065
2020-02-26 22:16:55,686 EPOCH 2082
2020-02-26 22:16:56,615 Epoch 2082 Step:   231000 Batch Loss:     0.000006 Tokens per Sec:  9839993, Lr: 0.000700
2020-02-26 22:17:05,694 Epoch 2082: total training loss 0.00065
2020-02-26 22:17:05,696 EPOCH 2083
2020-02-26 22:17:15,993 Epoch 2083: total training loss 0.00065
2020-02-26 22:17:15,994 EPOCH 2084
2020-02-26 22:17:19,485 Epoch 2084 Step:   231250 Batch Loss:     0.000005 Tokens per Sec:  9722068, Lr: 0.000700
2020-02-26 22:17:26,550 Epoch 2084: total training loss 0.00065
2020-02-26 22:17:26,551 EPOCH 2085
2020-02-26 22:17:37,139 Epoch 2085: total training loss 0.00065
2020-02-26 22:17:37,139 EPOCH 2086
2020-02-26 22:17:43,341 Epoch 2086 Step:   231500 Batch Loss:     0.000006 Tokens per Sec:  9787242, Lr: 0.000700
2020-02-26 22:17:47,701 Epoch 2086: total training loss 0.00067
2020-02-26 22:17:47,702 EPOCH 2087
2020-02-26 22:17:58,126 Epoch 2087: total training loss 0.00072
2020-02-26 22:17:58,127 EPOCH 2088
2020-02-26 22:18:06,518 Epoch 2088 Step:   231750 Batch Loss:     0.000004 Tokens per Sec: 10219638, Lr: 0.000700
2020-02-26 22:18:08,251 Epoch 2088: total training loss 0.00067
2020-02-26 22:18:08,251 EPOCH 2089
2020-02-26 22:18:18,284 Epoch 2089: total training loss 0.00069
2020-02-26 22:18:18,285 EPOCH 2090
2020-02-26 22:18:28,385 Epoch 2090: total training loss 0.00066
2020-02-26 22:18:28,385 EPOCH 2091
2020-02-26 22:18:29,260 Epoch 2091 Step:   232000 Batch Loss:     0.000009 Tokens per Sec:  9431252, Lr: 0.000700
2020-02-26 22:18:38,560 Epoch 2091: total training loss 0.00067
2020-02-26 22:18:38,562 EPOCH 2092
2020-02-26 22:18:48,674 Epoch 2092: total training loss 0.00066
2020-02-26 22:18:48,675 EPOCH 2093
2020-02-26 22:18:52,225 Epoch 2093 Step:   232250 Batch Loss:     0.000005 Tokens per Sec: 10366492, Lr: 0.000700
2020-02-26 22:18:58,658 Epoch 2093: total training loss 0.00068
2020-02-26 22:18:58,659 EPOCH 2094
2020-02-26 22:19:08,568 Epoch 2094: total training loss 0.00070
2020-02-26 22:19:08,569 EPOCH 2095
2020-02-26 22:19:14,728 Epoch 2095 Step:   232500 Batch Loss:     0.000007 Tokens per Sec: 10356221, Lr: 0.000700
2020-02-26 22:19:18,566 Epoch 2095: total training loss 0.00068
2020-02-26 22:19:18,567 EPOCH 2096
2020-02-26 22:19:28,321 Epoch 2096: total training loss 0.00067
2020-02-26 22:19:28,322 EPOCH 2097
2020-02-26 22:19:36,715 Epoch 2097 Step:   232750 Batch Loss:     0.000008 Tokens per Sec: 10253742, Lr: 0.000700
2020-02-26 22:19:38,292 Epoch 2097: total training loss 0.00066
2020-02-26 22:19:38,292 EPOCH 2098
2020-02-26 22:19:48,327 Epoch 2098: total training loss 0.00065
2020-02-26 22:19:48,328 EPOCH 2099
2020-02-26 22:19:58,568 Epoch 2099: total training loss 0.00066
2020-02-26 22:19:58,569 EPOCH 2100
2020-02-26 22:19:59,707 Epoch 2100 Step:   233000 Batch Loss:     0.000004 Tokens per Sec: 10256495, Lr: 0.000700
2020-02-26 22:20:08,930 Epoch 2100: total training loss 0.00065
2020-02-26 22:20:08,931 EPOCH 2101
2020-02-26 22:20:19,233 Epoch 2101: total training loss 0.00065
2020-02-26 22:20:19,234 EPOCH 2102
2020-02-26 22:20:22,665 Epoch 2102 Step:   233250 Batch Loss:     0.000008 Tokens per Sec: 10041380, Lr: 0.000700
2020-02-26 22:20:29,345 Epoch 2102: total training loss 0.00065
2020-02-26 22:20:29,346 EPOCH 2103
2020-02-26 22:20:39,278 Epoch 2103: total training loss 0.00069
2020-02-26 22:20:39,278 EPOCH 2104
2020-02-26 22:20:45,431 Epoch 2104 Step:   233500 Batch Loss:     0.000008 Tokens per Sec: 10320339, Lr: 0.000700
2020-02-26 22:20:49,309 Epoch 2104: total training loss 0.00072
2020-02-26 22:20:49,310 EPOCH 2105
2020-02-26 22:20:59,398 Epoch 2105: total training loss 0.00066
2020-02-26 22:20:59,398 EPOCH 2106
2020-02-26 22:21:07,941 Epoch 2106 Step:   233750 Batch Loss:     0.000006 Tokens per Sec: 10267704, Lr: 0.000700
2020-02-26 22:21:09,411 Epoch 2106: total training loss 0.00068
2020-02-26 22:21:09,412 EPOCH 2107
2020-02-26 22:21:19,270 Epoch 2107: total training loss 0.00070
2020-02-26 22:21:19,271 EPOCH 2108
2020-02-26 22:21:29,197 Epoch 2108: total training loss 0.00070
2020-02-26 22:21:29,197 EPOCH 2109
2020-02-26 22:21:30,189 Epoch 2109 Step:   234000 Batch Loss:     0.000007 Tokens per Sec: 10251867, Lr: 0.000700
2020-02-26 22:21:39,087 Epoch 2109: total training loss 0.00067
2020-02-26 22:21:39,088 EPOCH 2110
2020-02-26 22:21:49,017 Epoch 2110: total training loss 0.00065
2020-02-26 22:21:49,018 EPOCH 2111
2020-02-26 22:21:52,408 Epoch 2111 Step:   234250 Batch Loss:     0.000007 Tokens per Sec: 10429093, Lr: 0.000700
2020-02-26 22:21:58,857 Epoch 2111: total training loss 0.00067
2020-02-26 22:21:58,858 EPOCH 2112
2020-02-26 22:22:08,844 Epoch 2112: total training loss 0.00068
2020-02-26 22:22:08,845 EPOCH 2113
2020-02-26 22:22:14,723 Epoch 2113 Step:   234500 Batch Loss:     0.000011 Tokens per Sec: 10366432, Lr: 0.000700
2020-02-26 22:22:18,829 Epoch 2113: total training loss 0.00067
2020-02-26 22:22:18,830 EPOCH 2114
2020-02-26 22:22:28,895 Epoch 2114: total training loss 0.00066
2020-02-26 22:22:28,896 EPOCH 2115
2020-02-26 22:22:37,956 Epoch 2115 Step:   234750 Batch Loss:     0.000008 Tokens per Sec:  9734236, Lr: 0.000700
2020-02-26 22:22:39,374 Epoch 2115: total training loss 0.00066
2020-02-26 22:22:39,374 EPOCH 2116
2020-02-26 22:22:49,609 Epoch 2116: total training loss 0.00066
2020-02-26 22:22:49,610 EPOCH 2117
2020-02-26 22:23:00,047 Epoch 2117: total training loss 0.00066
2020-02-26 22:23:00,047 EPOCH 2118
2020-02-26 22:23:01,356 Epoch 2118 Step:   235000 Batch Loss:     0.000005 Tokens per Sec: 10179744, Lr: 0.000700
2020-02-26 22:23:10,473 Epoch 2118: total training loss 0.00066
2020-02-26 22:23:10,474 EPOCH 2119
2020-02-26 22:23:20,490 Epoch 2119: total training loss 0.00066
2020-02-26 22:23:20,491 EPOCH 2120
2020-02-26 22:23:24,318 Epoch 2120 Step:   235250 Batch Loss:     0.000004 Tokens per Sec: 10210853, Lr: 0.000700
2020-02-26 22:23:30,604 Epoch 2120: total training loss 0.00069
2020-02-26 22:23:30,605 EPOCH 2121
2020-02-26 22:23:40,617 Epoch 2121: total training loss 0.00077
2020-02-26 22:23:40,617 EPOCH 2122
2020-02-26 22:23:47,089 Epoch 2122 Step:   235500 Batch Loss:     0.000005 Tokens per Sec: 10300232, Lr: 0.000700
2020-02-26 22:23:50,604 Epoch 2122: total training loss 0.00069
2020-02-26 22:23:50,605 EPOCH 2123
2020-02-26 22:24:00,532 Epoch 2123: total training loss 0.00065
2020-02-26 22:24:00,533 EPOCH 2124
2020-02-26 22:24:09,188 Epoch 2124 Step:   235750 Batch Loss:     0.000006 Tokens per Sec: 10283142, Lr: 0.000700
2020-02-26 22:24:10,544 Epoch 2124: total training loss 0.00065
2020-02-26 22:24:10,544 EPOCH 2125
2020-02-26 22:24:20,560 Epoch 2125: total training loss 0.00064
2020-02-26 22:24:20,560 EPOCH 2126
2020-02-26 22:24:30,534 Epoch 2126: total training loss 0.00066
2020-02-26 22:24:30,535 EPOCH 2127
2020-02-26 22:24:31,766 Epoch 2127 Step:   236000 Batch Loss:     0.000005 Tokens per Sec: 10142687, Lr: 0.000700
2020-02-26 22:24:40,548 Epoch 2127: total training loss 0.00070
2020-02-26 22:24:40,548 EPOCH 2128
2020-02-26 22:24:50,649 Epoch 2128: total training loss 0.00070
2020-02-26 22:24:50,650 EPOCH 2129
2020-02-26 22:24:54,409 Epoch 2129 Step:   236250 Batch Loss:     0.000006 Tokens per Sec: 10094983, Lr: 0.000700
2020-02-26 22:25:00,736 Epoch 2129: total training loss 0.00066
2020-02-26 22:25:00,737 EPOCH 2130
2020-02-26 22:25:11,076 Epoch 2130: total training loss 0.00065
2020-02-26 22:25:11,077 EPOCH 2131
2020-02-26 22:25:17,725 Epoch 2131 Step:   236500 Batch Loss:     0.000005 Tokens per Sec:  9949919, Lr: 0.000700
2020-02-26 22:25:21,413 Epoch 2131: total training loss 0.00068
2020-02-26 22:25:21,413 EPOCH 2132
2020-02-26 22:25:31,812 Epoch 2132: total training loss 0.00067
2020-02-26 22:25:31,813 EPOCH 2133
2020-02-26 22:25:40,872 Epoch 2133 Step:   236750 Batch Loss:     0.000006 Tokens per Sec:  9880733, Lr: 0.000700
2020-02-26 22:25:42,170 Epoch 2133: total training loss 0.00066
2020-02-26 22:25:42,171 EPOCH 2134
2020-02-26 22:25:52,482 Epoch 2134: total training loss 0.00064
2020-02-26 22:25:52,482 EPOCH 2135
2020-02-26 22:26:02,534 Epoch 2135: total training loss 0.00065
2020-02-26 22:26:02,535 EPOCH 2136
2020-02-26 22:26:03,835 Epoch 2136 Step:   237000 Batch Loss:     0.000006 Tokens per Sec:  9824133, Lr: 0.000700
2020-02-26 22:26:12,725 Epoch 2136: total training loss 0.00075
2020-02-26 22:26:12,727 EPOCH 2137
2020-02-26 22:26:22,949 Epoch 2137: total training loss 0.00070
2020-02-26 22:26:22,949 EPOCH 2138
2020-02-26 22:26:26,904 Epoch 2138 Step:   237250 Batch Loss:     0.000007 Tokens per Sec:  9839669, Lr: 0.000700
2020-02-26 22:26:33,162 Epoch 2138: total training loss 0.00068
2020-02-26 22:26:33,163 EPOCH 2139
2020-02-26 22:26:43,101 Epoch 2139: total training loss 0.00066
2020-02-26 22:26:43,102 EPOCH 2140
2020-02-26 22:26:49,668 Epoch 2140 Step:   237500 Batch Loss:     0.000006 Tokens per Sec: 10281908, Lr: 0.000700
2020-02-26 22:26:53,092 Epoch 2140: total training loss 0.00069
2020-02-26 22:26:53,092 EPOCH 2141
2020-02-26 22:27:03,084 Epoch 2141: total training loss 0.00066
2020-02-26 22:27:03,085 EPOCH 2142
2020-02-26 22:27:11,794 Epoch 2142 Step:   237750 Batch Loss:     0.000009 Tokens per Sec: 10508862, Lr: 0.000700
2020-02-26 22:27:12,873 Epoch 2142: total training loss 0.00066
2020-02-26 22:27:12,873 EPOCH 2143
2020-02-26 22:27:22,806 Epoch 2143: total training loss 0.00066
2020-02-26 22:27:22,806 EPOCH 2144
2020-02-26 22:27:32,728 Epoch 2144: total training loss 0.00066
2020-02-26 22:27:32,729 EPOCH 2145
2020-02-26 22:27:34,226 Epoch 2145 Step:   238000 Batch Loss:     0.000006 Tokens per Sec: 10593765, Lr: 0.000700
2020-02-26 22:27:42,683 Epoch 2145: total training loss 0.00066
2020-02-26 22:27:42,684 EPOCH 2146
2020-02-26 22:27:52,695 Epoch 2146: total training loss 0.00067
2020-02-26 22:27:52,697 EPOCH 2147
2020-02-26 22:27:56,952 Epoch 2147 Step:   238250 Batch Loss:     0.000007 Tokens per Sec:  9710033, Lr: 0.000700
2020-02-26 22:28:03,283 Epoch 2147: total training loss 0.00068
2020-02-26 22:28:03,284 EPOCH 2148
2020-02-26 22:28:13,627 Epoch 2148: total training loss 0.00066
2020-02-26 22:28:13,628 EPOCH 2149
2020-02-26 22:28:20,303 Epoch 2149 Step:   238500 Batch Loss:     0.000008 Tokens per Sec:  9956461, Lr: 0.000700
2020-02-26 22:28:23,786 Epoch 2149: total training loss 0.00066
2020-02-26 22:28:23,787 EPOCH 2150
2020-02-26 22:28:33,860 Epoch 2150: total training loss 0.00066
2020-02-26 22:28:33,861 EPOCH 2151
2020-02-26 22:28:43,187 Epoch 2151 Step:   238750 Batch Loss:     0.000007 Tokens per Sec:  9906438, Lr: 0.000700
2020-02-26 22:28:44,254 Epoch 2151: total training loss 0.00065
2020-02-26 22:28:44,255 EPOCH 2152
2020-02-26 22:28:54,477 Epoch 2152: total training loss 0.00065
2020-02-26 22:28:54,478 EPOCH 2153
2020-02-26 22:29:04,920 Epoch 2153: total training loss 0.00065
2020-02-26 22:29:04,921 EPOCH 2154
2020-02-26 22:29:06,576 Epoch 2154 Step:   239000 Batch Loss:     0.000004 Tokens per Sec:  9851300, Lr: 0.000700
2020-02-26 22:29:14,937 Epoch 2154: total training loss 0.00066
2020-02-26 22:29:14,937 EPOCH 2155
2020-02-26 22:29:24,938 Epoch 2155: total training loss 0.00065
2020-02-26 22:29:24,938 EPOCH 2156
2020-02-26 22:29:29,048 Epoch 2156 Step:   239250 Batch Loss:     0.000005 Tokens per Sec: 10205273, Lr: 0.000700
2020-02-26 22:29:35,015 Epoch 2156: total training loss 0.00065
2020-02-26 22:29:35,015 EPOCH 2157
2020-02-26 22:29:45,043 Epoch 2157: total training loss 0.00066
2020-02-26 22:29:45,043 EPOCH 2158
2020-02-26 22:29:51,698 Epoch 2158 Step:   239500 Batch Loss:     0.000007 Tokens per Sec: 10270997, Lr: 0.000700
2020-02-26 22:29:55,061 Epoch 2158: total training loss 0.00069
2020-02-26 22:29:55,061 EPOCH 2159
2020-02-26 22:30:05,057 Epoch 2159: total training loss 0.00069
2020-02-26 22:30:05,058 EPOCH 2160
2020-02-26 22:30:14,116 Epoch 2160 Step:   239750 Batch Loss:     0.000005 Tokens per Sec: 10370423, Lr: 0.000700
2020-02-26 22:30:15,007 Epoch 2160: total training loss 0.00067
2020-02-26 22:30:15,008 EPOCH 2161
2020-02-26 22:30:24,905 Epoch 2161: total training loss 0.00078
2020-02-26 22:30:24,906 EPOCH 2162
2020-02-26 22:30:34,622 Epoch 2162: total training loss 0.00069
2020-02-26 22:30:34,623 EPOCH 2163
2020-02-26 22:30:36,164 Epoch 2163 Step:   240000 Batch Loss:     0.000008 Tokens per Sec: 10301651, Lr: 0.000700
2020-02-26 22:30:36,164 Model noise rate: 5
2020-02-26 22:31:39,176 Validation result at epoch 2163, step   240000: Val DTW Score:  10.66, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0402, GT DTW Score:      nan, duration: 63.0120s
2020-02-26 22:31:48,341 Epoch 2163: total training loss 0.00071
2020-02-26 22:31:48,342 EPOCH 2164
2020-02-26 22:31:58,784 Epoch 2164: total training loss 0.00069
2020-02-26 22:31:58,784 EPOCH 2165
2020-02-26 22:32:03,117 Epoch 2165 Step:   240250 Batch Loss:     0.000006 Tokens per Sec: 10132156, Lr: 0.000700
2020-02-26 22:32:08,927 Epoch 2165: total training loss 0.00066
2020-02-26 22:32:08,928 EPOCH 2166
2020-02-26 22:32:19,070 Epoch 2166: total training loss 0.00064
2020-02-26 22:32:19,070 EPOCH 2167
2020-02-26 22:32:25,655 Epoch 2167 Step:   240500 Batch Loss:     0.000005 Tokens per Sec: 10258457, Lr: 0.000700
2020-02-26 22:32:29,106 Epoch 2167: total training loss 0.00064
2020-02-26 22:32:29,107 EPOCH 2168
2020-02-26 22:32:39,087 Epoch 2168: total training loss 0.00068
2020-02-26 22:32:39,088 EPOCH 2169
2020-02-26 22:32:48,306 Epoch 2169 Step:   240750 Batch Loss:     0.000007 Tokens per Sec: 10312174, Lr: 0.000700
2020-02-26 22:32:49,051 Epoch 2169: total training loss 0.00068
2020-02-26 22:32:49,051 EPOCH 2170
2020-02-26 22:32:59,028 Epoch 2170: total training loss 0.00068
2020-02-26 22:32:59,028 EPOCH 2171
2020-02-26 22:33:09,034 Epoch 2171: total training loss 0.00065
2020-02-26 22:33:09,035 EPOCH 2172
2020-02-26 22:33:10,795 Epoch 2172 Step:   241000 Batch Loss:     0.000008 Tokens per Sec: 10304522, Lr: 0.000700
2020-02-26 22:33:19,004 Epoch 2172: total training loss 0.00064
2020-02-26 22:33:19,005 EPOCH 2173
2020-02-26 22:33:28,927 Epoch 2173: total training loss 0.00065
2020-02-26 22:33:28,928 EPOCH 2174
2020-02-26 22:33:33,204 Epoch 2174 Step:   241250 Batch Loss:     0.000007 Tokens per Sec: 10250103, Lr: 0.000700
2020-02-26 22:33:38,915 Epoch 2174: total training loss 0.00077
2020-02-26 22:33:38,915 EPOCH 2175
2020-02-26 22:33:48,974 Epoch 2175: total training loss 0.00069
2020-02-26 22:33:48,975 EPOCH 2176
2020-02-26 22:33:55,617 Epoch 2176 Step:   241500 Batch Loss:     0.000007 Tokens per Sec: 10125000, Lr: 0.000700
2020-02-26 22:33:59,138 Epoch 2176: total training loss 0.00072
2020-02-26 22:33:59,139 EPOCH 2177
2020-02-26 22:34:09,621 Epoch 2177: total training loss 0.00067
2020-02-26 22:34:09,621 EPOCH 2178
2020-02-26 22:34:19,185 Epoch 2178 Step:   241750 Batch Loss:     0.000007 Tokens per Sec: 10104204, Lr: 0.000700
2020-02-26 22:34:19,894 Epoch 2178: total training loss 0.00066
2020-02-26 22:34:19,895 EPOCH 2179
2020-02-26 22:34:30,225 Epoch 2179: total training loss 0.00065
2020-02-26 22:34:30,225 EPOCH 2180
2020-02-26 22:34:40,609 Epoch 2180: total training loss 0.00064
2020-02-26 22:34:40,609 EPOCH 2181
2020-02-26 22:34:42,485 Epoch 2181 Step:   242000 Batch Loss:     0.000008 Tokens per Sec:  9646619, Lr: 0.000700
2020-02-26 22:34:50,836 Epoch 2181: total training loss 0.00066
2020-02-26 22:34:50,837 EPOCH 2182
2020-02-26 22:35:01,033 Epoch 2182: total training loss 0.00064
2020-02-26 22:35:01,034 EPOCH 2183
2020-02-26 22:35:05,241 Epoch 2183 Step:   242250 Batch Loss:     0.000005 Tokens per Sec:  9852881, Lr: 0.000700
2020-02-26 22:35:11,348 Epoch 2183: total training loss 0.00065
2020-02-26 22:35:11,348 EPOCH 2184
2020-02-26 22:35:21,490 Epoch 2184: total training loss 0.00064
2020-02-26 22:35:21,491 EPOCH 2185
2020-02-26 22:35:28,261 Epoch 2185 Step:   242500 Batch Loss:     0.000006 Tokens per Sec: 10277548, Lr: 0.000700
2020-02-26 22:35:31,447 Epoch 2185: total training loss 0.00066
2020-02-26 22:35:31,447 EPOCH 2186
2020-02-26 22:35:41,395 Epoch 2186: total training loss 0.00065
2020-02-26 22:35:41,396 EPOCH 2187
2020-02-26 22:35:50,710 Epoch 2187 Step:   242750 Batch Loss:     0.000005 Tokens per Sec: 10397716, Lr: 0.000700
2020-02-26 22:35:51,322 Epoch 2187: total training loss 0.00068
2020-02-26 22:35:51,322 EPOCH 2188
2020-02-26 22:36:01,217 Epoch 2188: total training loss 0.00065
2020-02-26 22:36:01,217 EPOCH 2189
2020-02-26 22:36:11,284 Epoch 2189: total training loss 0.00064
2020-02-26 22:36:11,285 EPOCH 2190
2020-02-26 22:36:13,228 Epoch 2190 Step:   243000 Batch Loss:     0.000006 Tokens per Sec: 10511134, Lr: 0.000700
2020-02-26 22:36:21,251 Epoch 2190: total training loss 0.00066
2020-02-26 22:36:21,251 EPOCH 2191
2020-02-26 22:36:31,246 Epoch 2191: total training loss 0.00066
2020-02-26 22:36:31,249 EPOCH 2192
2020-02-26 22:36:35,776 Epoch 2192 Step:   243250 Batch Loss:     0.000010 Tokens per Sec: 10429607, Lr: 0.000700
2020-02-26 22:36:41,497 Epoch 2192: total training loss 0.00067
2020-02-26 22:36:41,498 EPOCH 2193
2020-02-26 22:36:51,860 Epoch 2193: total training loss 0.00069
2020-02-26 22:36:51,862 EPOCH 2194
2020-02-26 22:36:58,872 Epoch 2194 Step:   243500 Batch Loss:     0.000008 Tokens per Sec:  9849094, Lr: 0.000700
2020-02-26 22:37:02,365 Epoch 2194: total training loss 0.00066
2020-02-26 22:37:02,366 EPOCH 2195
2020-02-26 22:37:12,899 Epoch 2195: total training loss 0.00065
2020-02-26 22:37:12,900 EPOCH 2196
2020-02-26 22:37:22,867 Epoch 2196 Step:   243750 Batch Loss:     0.000005 Tokens per Sec:  9681140, Lr: 0.000700
2020-02-26 22:37:23,477 Epoch 2196: total training loss 0.00064
2020-02-26 22:37:23,478 EPOCH 2197
2020-02-26 22:37:33,561 Epoch 2197: total training loss 0.00064
2020-02-26 22:37:33,562 EPOCH 2198
2020-02-26 22:37:43,777 Epoch 2198: total training loss 0.00067
2020-02-26 22:37:43,778 EPOCH 2199
2020-02-26 22:37:45,779 Epoch 2199 Step:   244000 Batch Loss:     0.000007 Tokens per Sec:  9888755, Lr: 0.000700
2020-02-26 22:37:53,880 Epoch 2199: total training loss 0.00065
2020-02-26 22:37:53,880 EPOCH 2200
2020-02-26 22:38:03,998 Epoch 2200: total training loss 0.00064
2020-02-26 22:38:03,998 EPOCH 2201
2020-02-26 22:38:08,524 Epoch 2201 Step:   244250 Batch Loss:     0.000009 Tokens per Sec: 10352627, Lr: 0.000700
2020-02-26 22:38:13,913 Epoch 2201: total training loss 0.00064
2020-02-26 22:38:13,914 EPOCH 2202
2020-02-26 22:38:23,787 Epoch 2202: total training loss 0.00065
2020-02-26 22:38:23,788 EPOCH 2203
2020-02-26 22:38:30,843 Epoch 2203 Step:   244500 Batch Loss:     0.000007 Tokens per Sec: 10154205, Lr: 0.000700
2020-02-26 22:38:33,877 Epoch 2203: total training loss 0.00066
2020-02-26 22:38:33,877 EPOCH 2204
2020-02-26 22:38:43,953 Epoch 2204: total training loss 0.00065
2020-02-26 22:38:43,953 EPOCH 2205
2020-02-26 22:38:53,465 Epoch 2205 Step:   244750 Batch Loss:     0.000006 Tokens per Sec: 10314518, Lr: 0.000700
2020-02-26 22:38:53,896 Epoch 2205: total training loss 0.00067
2020-02-26 22:38:53,897 EPOCH 2206
2020-02-26 22:39:03,906 Epoch 2206: total training loss 0.00066
2020-02-26 22:39:03,906 EPOCH 2207
2020-02-26 22:39:13,891 Epoch 2207: total training loss 0.00065
2020-02-26 22:39:13,893 EPOCH 2208
2020-02-26 22:39:16,061 Epoch 2208 Step:   245000 Batch Loss:     0.000005 Tokens per Sec: 10139353, Lr: 0.000700
2020-02-26 22:39:24,007 Epoch 2208: total training loss 0.00067
2020-02-26 22:39:24,007 EPOCH 2209
2020-02-26 22:39:34,242 Epoch 2209: total training loss 0.00065
2020-02-26 22:39:34,242 EPOCH 2210
2020-02-26 22:39:38,950 Epoch 2210 Step:   245250 Batch Loss:     0.000005 Tokens per Sec:  9877773, Lr: 0.000700
2020-02-26 22:39:44,704 Epoch 2210: total training loss 0.00065
2020-02-26 22:39:44,705 EPOCH 2211
2020-02-26 22:39:55,099 Epoch 2211: total training loss 0.00067
2020-02-26 22:39:55,100 EPOCH 2212
2020-02-26 22:40:02,488 Epoch 2212 Step:   245500 Batch Loss:     0.000007 Tokens per Sec:  9923046, Lr: 0.000700
2020-02-26 22:40:05,463 Epoch 2212: total training loss 0.00070
2020-02-26 22:40:05,464 EPOCH 2213
2020-02-26 22:40:15,545 Epoch 2213: total training loss 0.00066
2020-02-26 22:40:15,547 EPOCH 2214
2020-02-26 22:40:25,456 Epoch 2214 Step:   245750 Batch Loss:     0.000009 Tokens per Sec:  9978308, Lr: 0.000700
2020-02-26 22:40:25,848 Epoch 2214: total training loss 0.00066
2020-02-26 22:40:25,848 EPOCH 2215
2020-02-26 22:40:36,074 Epoch 2215: total training loss 0.00067
2020-02-26 22:40:36,074 EPOCH 2216
2020-02-26 22:40:46,181 Epoch 2216: total training loss 0.00070
2020-02-26 22:40:46,182 EPOCH 2217
2020-02-26 22:40:48,263 Epoch 2217 Step:   246000 Batch Loss:     0.000005 Tokens per Sec: 10381985, Lr: 0.000700
2020-02-26 22:40:56,145 Epoch 2217: total training loss 0.00065
2020-02-26 22:40:56,145 EPOCH 2218
2020-02-26 22:41:05,959 Epoch 2218: total training loss 0.00065
2020-02-26 22:41:05,960 EPOCH 2219
2020-02-26 22:41:10,456 Epoch 2219 Step:   246250 Batch Loss:     0.000007 Tokens per Sec: 10438282, Lr: 0.000700
2020-02-26 22:41:15,707 Epoch 2219: total training loss 0.00064
2020-02-26 22:41:15,708 EPOCH 2220
2020-02-26 22:41:25,592 Epoch 2220: total training loss 0.00066
2020-02-26 22:41:25,593 EPOCH 2221
2020-02-26 22:41:32,597 Epoch 2221 Step:   246500 Batch Loss:     0.000009 Tokens per Sec: 10380173, Lr: 0.000700
2020-02-26 22:41:35,626 Epoch 2221: total training loss 0.00065
2020-02-26 22:41:35,626 EPOCH 2222
2020-02-26 22:41:45,568 Epoch 2222: total training loss 0.00065
2020-02-26 22:41:45,569 EPOCH 2223
2020-02-26 22:41:55,105 Epoch 2223 Step:   246750 Batch Loss:     0.000004 Tokens per Sec: 10404819, Lr: 0.000700
2020-02-26 22:41:55,363 Epoch 2223: total training loss 0.00066
2020-02-26 22:41:55,363 EPOCH 2224
2020-02-26 22:42:05,730 Epoch 2224: total training loss 0.00067
2020-02-26 22:42:05,732 EPOCH 2225
2020-02-26 22:42:16,348 Epoch 2225: total training loss 0.00066
2020-02-26 22:42:16,349 EPOCH 2226
2020-02-26 22:42:18,898 Epoch 2226 Step:   247000 Batch Loss:     0.000006 Tokens per Sec:  9655587, Lr: 0.000700
2020-02-26 22:42:27,049 Epoch 2226: total training loss 0.00064
2020-02-26 22:42:27,049 EPOCH 2227
2020-02-26 22:42:37,623 Epoch 2227: total training loss 0.00065
2020-02-26 22:42:37,623 EPOCH 2228
2020-02-26 22:42:42,669 Epoch 2228 Step:   247250 Batch Loss:     0.000005 Tokens per Sec:  9579073, Lr: 0.000700
2020-02-26 22:42:48,130 Epoch 2228: total training loss 0.00066
2020-02-26 22:42:48,130 EPOCH 2229
2020-02-26 22:42:58,158 Epoch 2229: total training loss 0.00064
2020-02-26 22:42:58,158 EPOCH 2230
2020-02-26 22:43:05,562 Epoch 2230 Step:   247500 Batch Loss:     0.000008 Tokens per Sec: 10180594, Lr: 0.000700
2020-02-26 22:43:08,194 Epoch 2230: total training loss 0.00066
2020-02-26 22:43:08,195 EPOCH 2231
2020-02-26 22:43:18,217 Epoch 2231: total training loss 0.00067
2020-02-26 22:43:18,217 EPOCH 2232
2020-02-26 22:43:28,060 Epoch 2232 Step:   247750 Batch Loss:     0.000007 Tokens per Sec: 10225651, Lr: 0.000700
2020-02-26 22:43:28,285 Epoch 2232: total training loss 0.00067
2020-02-26 22:43:28,285 EPOCH 2233
2020-02-26 22:43:38,305 Epoch 2233: total training loss 0.00066
2020-02-26 22:43:38,306 EPOCH 2234
2020-02-26 22:43:48,400 Epoch 2234: total training loss 0.00064
2020-02-26 22:43:48,400 EPOCH 2235
2020-02-26 22:43:50,747 Epoch 2235 Step:   248000 Batch Loss:     0.000008 Tokens per Sec: 10179355, Lr: 0.000700
2020-02-26 22:43:58,410 Epoch 2235: total training loss 0.00065
2020-02-26 22:43:58,411 EPOCH 2236
2020-02-26 22:44:08,420 Epoch 2236: total training loss 0.00067
2020-02-26 22:44:08,420 EPOCH 2237
2020-02-26 22:44:13,320 Epoch 2237 Step:   248250 Batch Loss:     0.000004 Tokens per Sec: 10377330, Lr: 0.000700
2020-02-26 22:44:18,340 Epoch 2237: total training loss 0.00064
2020-02-26 22:44:18,340 EPOCH 2238
2020-02-26 22:44:28,326 Epoch 2238: total training loss 0.00072
2020-02-26 22:44:28,327 EPOCH 2239
2020-02-26 22:44:35,787 Epoch 2239 Step:   248500 Batch Loss:     0.000007 Tokens per Sec: 10293995, Lr: 0.000700
2020-02-26 22:44:38,392 Epoch 2239: total training loss 0.00067
2020-02-26 22:44:38,393 EPOCH 2240
2020-02-26 22:44:48,740 Epoch 2240: total training loss 0.00065
2020-02-26 22:44:48,753 EPOCH 2241
2020-02-26 22:44:59,197 Epoch 2241 Step:   248750 Batch Loss:     0.000005 Tokens per Sec:  9771473, Lr: 0.000700
2020-02-26 22:44:59,301 Epoch 2241: total training loss 0.00064
2020-02-26 22:44:59,301 EPOCH 2242
2020-02-26 22:45:09,745 Epoch 2242: total training loss 0.00065
2020-02-26 22:45:09,746 EPOCH 2243
2020-02-26 22:45:19,940 Epoch 2243: total training loss 0.00067
2020-02-26 22:45:19,941 EPOCH 2244
2020-02-26 22:45:22,460 Epoch 2244 Step:   249000 Batch Loss:     0.000005 Tokens per Sec: 10040424, Lr: 0.000700
2020-02-26 22:45:30,326 Epoch 2244: total training loss 0.00064
2020-02-26 22:45:30,327 EPOCH 2245
2020-02-26 22:45:40,311 Epoch 2245: total training loss 0.00067
2020-02-26 22:45:40,311 EPOCH 2246
2020-02-26 22:45:45,266 Epoch 2246 Step:   249250 Batch Loss:     0.000005 Tokens per Sec: 10170355, Lr: 0.000700
2020-02-26 22:45:50,362 Epoch 2246: total training loss 0.00065
2020-02-26 22:45:50,363 EPOCH 2247
2020-02-26 22:46:00,425 Epoch 2247: total training loss 0.00067
2020-02-26 22:46:00,426 EPOCH 2248
2020-02-26 22:46:07,978 Epoch 2248 Step:   249500 Batch Loss:     0.000006 Tokens per Sec: 10150527, Lr: 0.000700
2020-02-26 22:46:10,551 Epoch 2248: total training loss 0.00070
2020-02-26 22:46:10,551 EPOCH 2249
2020-02-26 22:46:20,632 Epoch 2249: total training loss 0.00068
2020-02-26 22:46:20,633 EPOCH 2250
2020-02-26 22:46:30,577 Epoch 2250 Step:   249750 Batch Loss:     0.000006 Tokens per Sec: 10310821, Lr: 0.000700
2020-02-26 22:46:30,578 Epoch 2250: total training loss 0.00066
2020-02-26 22:46:30,578 EPOCH 2251
2020-02-26 22:46:40,575 Epoch 2251: total training loss 0.00065
2020-02-26 22:46:40,576 EPOCH 2252
2020-02-26 22:46:50,455 Epoch 2252: total training loss 0.00064
2020-02-26 22:46:50,456 EPOCH 2253
2020-02-26 22:46:52,856 Epoch 2253 Step:   250000 Batch Loss:     0.000005 Tokens per Sec: 10024893, Lr: 0.000700
2020-02-26 22:46:52,856 Model noise rate: 5
2020-02-26 22:47:54,644 Validation result at epoch 2253, step   250000: Val DTW Score:  10.68, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0411, GT DTW Score:      nan, duration: 61.7879s
2020-02-26 22:48:02,884 Epoch 2253: total training loss 0.00063
2020-02-26 22:48:02,884 EPOCH 2254
2020-02-26 22:48:13,519 Epoch 2254: total training loss 0.00063
2020-02-26 22:48:13,520 EPOCH 2255
2020-02-26 22:48:18,516 Epoch 2255 Step:   250250 Batch Loss:     0.000005 Tokens per Sec:  9986853, Lr: 0.000700
2020-02-26 22:48:23,700 Epoch 2255: total training loss 0.00065
2020-02-26 22:48:23,701 EPOCH 2256
2020-02-26 22:48:33,940 Epoch 2256: total training loss 0.00068
2020-02-26 22:48:33,941 EPOCH 2257
2020-02-26 22:48:41,562 Epoch 2257 Step:   250500 Batch Loss:     0.000005 Tokens per Sec: 10165744, Lr: 0.000700
2020-02-26 22:48:44,045 Epoch 2257: total training loss 0.00067
2020-02-26 22:48:44,046 EPOCH 2258
2020-02-26 22:48:54,188 Epoch 2258: total training loss 0.00064
2020-02-26 22:48:54,189 EPOCH 2259
2020-02-26 22:49:03,992 Epoch 2259: total training loss 0.00064
2020-02-26 22:49:03,993 EPOCH 2260
2020-02-26 22:49:04,096 Epoch 2260 Step:   250750 Batch Loss:     0.000006 Tokens per Sec:  7901005, Lr: 0.000700
2020-02-26 22:49:13,915 Epoch 2260: total training loss 0.00066
2020-02-26 22:49:13,916 EPOCH 2261
2020-02-26 22:49:23,893 Epoch 2261: total training loss 0.00064
2020-02-26 22:49:23,894 EPOCH 2262
2020-02-26 22:49:26,752 Epoch 2262 Step:   251000 Batch Loss:     0.000008 Tokens per Sec: 10177914, Lr: 0.000700
2020-02-26 22:49:33,922 Epoch 2262: total training loss 0.00067
2020-02-26 22:49:33,924 EPOCH 2263
2020-02-26 22:49:43,921 Epoch 2263: total training loss 0.00065
2020-02-26 22:49:43,922 EPOCH 2264
2020-02-26 22:49:49,033 Epoch 2264 Step:   251250 Batch Loss:     0.000006 Tokens per Sec: 10257793, Lr: 0.000700
2020-02-26 22:49:53,897 Epoch 2264: total training loss 0.00067
2020-02-26 22:49:53,897 EPOCH 2265
2020-02-26 22:50:03,933 Epoch 2265: total training loss 0.00072
2020-02-26 22:50:03,934 EPOCH 2266
2020-02-26 22:50:11,928 Epoch 2266 Step:   251500 Batch Loss:     0.000006 Tokens per Sec:  9722403, Lr: 0.000700
2020-02-26 22:50:14,503 Epoch 2266: total training loss 0.00070
2020-02-26 22:50:14,503 EPOCH 2267
2020-02-26 22:50:24,943 Epoch 2267: total training loss 0.00066
2020-02-26 22:50:24,944 EPOCH 2268
2020-02-26 22:50:35,054 Epoch 2268: total training loss 0.00063
2020-02-26 22:50:35,054 EPOCH 2269
2020-02-26 22:50:35,264 Epoch 2269 Step:   251750 Batch Loss:     0.000006 Tokens per Sec:  7953915, Lr: 0.000700
2020-02-26 22:50:45,258 Epoch 2269: total training loss 0.00064
2020-02-26 22:50:45,258 EPOCH 2270
2020-02-26 22:50:55,253 Epoch 2270: total training loss 0.00064
2020-02-26 22:50:55,254 EPOCH 2271
2020-02-26 22:50:57,937 Epoch 2271 Step:   252000 Batch Loss:     0.000005 Tokens per Sec: 10324887, Lr: 0.000700
2020-02-26 22:51:05,061 Epoch 2271: total training loss 0.00064
2020-02-26 22:51:05,061 EPOCH 2272
2020-02-26 22:51:14,863 Epoch 2272: total training loss 0.00064
2020-02-26 22:51:14,864 EPOCH 2273
2020-02-26 22:51:19,849 Epoch 2273 Step:   252250 Batch Loss:     0.000006 Tokens per Sec: 10353960, Lr: 0.000700
2020-02-26 22:51:24,738 Epoch 2273: total training loss 0.00066
2020-02-26 22:51:24,738 EPOCH 2274
2020-02-26 22:51:34,579 Epoch 2274: total training loss 0.00070
2020-02-26 22:51:34,580 EPOCH 2275
2020-02-26 22:51:42,252 Epoch 2275 Step:   252500 Batch Loss:     0.000005 Tokens per Sec: 10336907, Lr: 0.000700
2020-02-26 22:51:44,520 Epoch 2275: total training loss 0.00068
2020-02-26 22:51:44,521 EPOCH 2276
2020-02-26 22:51:54,425 Epoch 2276: total training loss 0.00066
2020-02-26 22:51:54,427 EPOCH 2277
2020-02-26 22:52:04,373 Epoch 2277: total training loss 0.00064
2020-02-26 22:52:04,373 EPOCH 2278
2020-02-26 22:52:04,612 Epoch 2278 Step:   252750 Batch Loss:     0.000005 Tokens per Sec:  8655562, Lr: 0.000700
2020-02-26 22:52:14,275 Epoch 2278: total training loss 0.00064
2020-02-26 22:52:14,276 EPOCH 2279
2020-02-26 22:52:24,205 Epoch 2279: total training loss 0.00065
2020-02-26 22:52:24,205 EPOCH 2280
2020-02-26 22:52:27,066 Epoch 2280 Step:   253000 Batch Loss:     0.000008 Tokens per Sec: 10368249, Lr: 0.000700
2020-02-26 22:52:34,223 Epoch 2280: total training loss 0.00063
2020-02-26 22:52:34,223 EPOCH 2281
2020-02-26 22:52:44,501 Epoch 2281: total training loss 0.00063
2020-02-26 22:52:44,503 EPOCH 2282
2020-02-26 22:52:50,118 Epoch 2282 Step:   253250 Batch Loss:     0.000004 Tokens per Sec:  9615011, Lr: 0.000700
2020-02-26 22:52:55,118 Epoch 2282: total training loss 0.00064
2020-02-26 22:52:55,118 EPOCH 2283
2020-02-26 22:53:06,008 Epoch 2283: total training loss 0.00063
2020-02-26 22:53:06,010 EPOCH 2284
2020-02-26 22:53:14,576 Epoch 2284 Step:   253500 Batch Loss:     0.000005 Tokens per Sec:  9544605, Lr: 0.000700
2020-02-26 22:53:16,817 Epoch 2284: total training loss 0.00064
2020-02-26 22:53:16,818 EPOCH 2285
2020-02-26 22:53:27,481 Epoch 2285: total training loss 0.00064
2020-02-26 22:53:27,481 EPOCH 2286
2020-02-26 22:53:37,970 Epoch 2286: total training loss 0.00063
2020-02-26 22:53:37,971 EPOCH 2287
2020-02-26 22:53:38,393 Epoch 2287 Step:   253750 Batch Loss:     0.000005 Tokens per Sec:  9836336, Lr: 0.000700
2020-02-26 22:53:48,293 Epoch 2287: total training loss 0.00063
2020-02-26 22:53:48,293 EPOCH 2288
2020-02-26 22:53:58,381 Epoch 2288: total training loss 0.00064
2020-02-26 22:53:58,382 EPOCH 2289
2020-02-26 22:54:01,238 Epoch 2289 Step:   254000 Batch Loss:     0.000006 Tokens per Sec:  9989257, Lr: 0.000700
2020-02-26 22:54:08,429 Epoch 2289: total training loss 0.00066
2020-02-26 22:54:08,430 EPOCH 2290
2020-02-26 22:54:18,383 Epoch 2290: total training loss 0.00074
2020-02-26 22:54:18,383 EPOCH 2291
2020-02-26 22:54:23,561 Epoch 2291 Step:   254250 Batch Loss:     0.000006 Tokens per Sec: 10021102, Lr: 0.000700
2020-02-26 22:54:28,367 Epoch 2291: total training loss 0.00070
2020-02-26 22:54:28,367 EPOCH 2292
2020-02-26 22:54:38,261 Epoch 2292: total training loss 0.00066
2020-02-26 22:54:38,261 EPOCH 2293
2020-02-26 22:54:46,008 Epoch 2293 Step:   254500 Batch Loss:     0.000007 Tokens per Sec: 10464140, Lr: 0.000700
2020-02-26 22:54:48,056 Epoch 2293: total training loss 0.00065
2020-02-26 22:54:48,056 EPOCH 2294
2020-02-26 22:54:58,010 Epoch 2294: total training loss 0.00063
2020-02-26 22:54:58,011 EPOCH 2295
2020-02-26 22:55:08,151 Epoch 2295: total training loss 0.00065
2020-02-26 22:55:08,151 EPOCH 2296
2020-02-26 22:55:08,602 Epoch 2296 Step:   254750 Batch Loss:     0.000008 Tokens per Sec:  9873644, Lr: 0.000700
2020-02-26 22:55:18,270 Epoch 2296: total training loss 0.00066
2020-02-26 22:55:18,272 EPOCH 2297
2020-02-26 22:55:28,772 Epoch 2297: total training loss 0.00067
2020-02-26 22:55:28,773 EPOCH 2298
2020-02-26 22:55:32,277 Epoch 2298 Step:   255000 Batch Loss:     0.000007 Tokens per Sec:  9885883, Lr: 0.000700
2020-02-26 22:55:39,313 Epoch 2298: total training loss 0.00066
2020-02-26 22:55:39,314 EPOCH 2299
2020-02-26 22:55:49,703 Epoch 2299: total training loss 0.00064
2020-02-26 22:55:49,704 EPOCH 2300
2020-02-26 22:55:55,477 Epoch 2300 Step:   255250 Batch Loss:     0.000007 Tokens per Sec:  9628515, Lr: 0.000700
2020-02-26 22:56:00,261 Epoch 2300: total training loss 0.00066
2020-02-26 22:56:00,262 EPOCH 2301
2020-02-26 22:56:10,878 Epoch 2301: total training loss 0.00066
2020-02-26 22:56:10,879 EPOCH 2302
2020-02-26 22:56:19,060 Epoch 2302 Step:   255500 Batch Loss:     0.000002 Tokens per Sec: 10005791, Lr: 0.000700
2020-02-26 22:56:21,069 Epoch 2302: total training loss 0.00064
2020-02-26 22:56:21,070 EPOCH 2303
2020-02-26 22:56:31,194 Epoch 2303: total training loss 0.00065
2020-02-26 22:56:31,197 EPOCH 2304
2020-02-26 22:56:41,518 Epoch 2304: total training loss 0.00066
2020-02-26 22:56:41,519 EPOCH 2305
2020-02-26 22:56:42,048 Epoch 2305 Step:   255750 Batch Loss:     0.000005 Tokens per Sec:  9204178, Lr: 0.000700
2020-02-26 22:56:51,757 Epoch 2305: total training loss 0.00066
2020-02-26 22:56:51,758 EPOCH 2306
2020-02-26 22:57:01,963 Epoch 2306: total training loss 0.00066
2020-02-26 22:57:01,964 EPOCH 2307
2020-02-26 22:57:05,190 Epoch 2307 Step:   256000 Batch Loss:     0.000002 Tokens per Sec: 10171180, Lr: 0.000700
2020-02-26 22:57:12,029 Epoch 2307: total training loss 0.00065
2020-02-26 22:57:12,029 EPOCH 2308
2020-02-26 22:57:22,171 Epoch 2308: total training loss 0.00063
2020-02-26 22:57:22,172 EPOCH 2309
2020-02-26 22:57:27,891 Epoch 2309 Step:   256250 Batch Loss:     0.000003 Tokens per Sec: 10419807, Lr: 0.000700
2020-02-26 22:57:32,094 Epoch 2309: total training loss 0.00068
2020-02-26 22:57:32,095 EPOCH 2310
2020-02-26 22:57:42,034 Epoch 2310: total training loss 0.00079
2020-02-26 22:57:42,034 EPOCH 2311
2020-02-26 22:57:50,128 Epoch 2311 Step:   256500 Batch Loss:     0.000006 Tokens per Sec: 10293204, Lr: 0.000700
2020-02-26 22:57:51,979 Epoch 2311: total training loss 0.00066
2020-02-26 22:57:51,979 EPOCH 2312
2020-02-26 22:58:02,122 Epoch 2312: total training loss 0.00064
2020-02-26 22:58:02,124 EPOCH 2313
2020-02-26 22:58:12,336 Epoch 2313: total training loss 0.00063
2020-02-26 22:58:12,337 EPOCH 2314
2020-02-26 22:58:13,092 Epoch 2314 Step:   256750 Batch Loss:     0.000004 Tokens per Sec:  9612070, Lr: 0.000700
2020-02-26 22:58:22,832 Epoch 2314: total training loss 0.00063
2020-02-26 22:58:22,834 EPOCH 2315
2020-02-26 22:58:33,439 Epoch 2315: total training loss 0.00064
2020-02-26 22:58:33,440 EPOCH 2316
2020-02-26 22:58:36,829 Epoch 2316 Step:   257000 Batch Loss:     0.000006 Tokens per Sec:  9451031, Lr: 0.000700
2020-02-26 22:58:44,048 Epoch 2316: total training loss 0.00063
2020-02-26 22:58:44,049 EPOCH 2317
2020-02-26 22:58:54,519 Epoch 2317: total training loss 0.00063
2020-02-26 22:58:54,520 EPOCH 2318
2020-02-26 22:59:00,142 Epoch 2318 Step:   257250 Batch Loss:     0.000010 Tokens per Sec: 10023828, Lr: 0.000700
2020-02-26 22:59:04,722 Epoch 2318: total training loss 0.00064
2020-02-26 22:59:04,722 EPOCH 2319
2020-02-26 22:59:14,897 Epoch 2319: total training loss 0.00065
2020-02-26 22:59:14,897 EPOCH 2320
2020-02-26 22:59:23,231 Epoch 2320 Step:   257500 Batch Loss:     0.000003 Tokens per Sec: 10029813, Lr: 0.000700
2020-02-26 22:59:25,148 Epoch 2320: total training loss 0.00064
2020-02-26 22:59:25,148 EPOCH 2321
2020-02-26 22:59:35,274 Epoch 2321: total training loss 0.00072
2020-02-26 22:59:35,275 EPOCH 2322
2020-02-26 22:59:45,207 Epoch 2322: total training loss 0.00071
2020-02-26 22:59:45,207 EPOCH 2323
2020-02-26 22:59:45,976 Epoch 2323 Step:   257750 Batch Loss:     0.000005 Tokens per Sec: 10156589, Lr: 0.000700
2020-02-26 22:59:55,188 Epoch 2323: total training loss 0.00065
2020-02-26 22:59:55,188 EPOCH 2324
2020-02-26 23:00:05,186 Epoch 2324: total training loss 0.00063
2020-02-26 23:00:05,187 EPOCH 2325
2020-02-26 23:00:08,569 Epoch 2325 Step:   258000 Batch Loss:     0.000005 Tokens per Sec: 10263074, Lr: 0.000700
2020-02-26 23:00:15,296 Epoch 2325: total training loss 0.00062
2020-02-26 23:00:15,296 EPOCH 2326
2020-02-26 23:00:25,320 Epoch 2326: total training loss 0.00064
2020-02-26 23:00:25,320 EPOCH 2327
2020-02-26 23:00:31,019 Epoch 2327 Step:   258250 Batch Loss:     0.000005 Tokens per Sec: 10333807, Lr: 0.000700
2020-02-26 23:00:35,299 Epoch 2327: total training loss 0.00067
2020-02-26 23:00:35,299 EPOCH 2328
2020-02-26 23:00:45,193 Epoch 2328: total training loss 0.00068
2020-02-26 23:00:45,193 EPOCH 2329
2020-02-26 23:00:53,410 Epoch 2329 Step:   258500 Batch Loss:     0.000012 Tokens per Sec: 10308370, Lr: 0.000700
2020-02-26 23:00:55,106 Epoch 2329: total training loss 0.00066
2020-02-26 23:00:55,107 EPOCH 2330
2020-02-26 23:01:05,103 Epoch 2330: total training loss 0.00064
2020-02-26 23:01:05,104 EPOCH 2331
2020-02-26 23:01:15,050 Epoch 2331: total training loss 0.00064
2020-02-26 23:01:15,051 EPOCH 2332
2020-02-26 23:01:15,916 Epoch 2332 Step:   258750 Batch Loss:     0.000008 Tokens per Sec: 10145950, Lr: 0.000700
2020-02-26 23:01:25,015 Epoch 2332: total training loss 0.00066
2020-02-26 23:01:25,017 EPOCH 2333
2020-02-26 23:01:34,998 Epoch 2333: total training loss 0.00064
2020-02-26 23:01:34,999 EPOCH 2334
2020-02-26 23:01:38,266 Epoch 2334 Step:   259000 Batch Loss:     0.000010 Tokens per Sec: 10202676, Lr: 0.000700
2020-02-26 23:01:45,066 Epoch 2334: total training loss 0.00065
2020-02-26 23:01:45,069 EPOCH 2335
2020-02-26 23:01:55,640 Epoch 2335: total training loss 0.00063
2020-02-26 23:01:55,640 EPOCH 2336
2020-02-26 23:02:01,832 Epoch 2336 Step:   259250 Batch Loss:     0.000003 Tokens per Sec:  9564753, Lr: 0.000700
2020-02-26 23:02:06,330 Epoch 2336: total training loss 0.00063
2020-02-26 23:02:06,330 EPOCH 2337
2020-02-26 23:02:16,697 Epoch 2337: total training loss 0.00062
2020-02-26 23:02:16,714 EPOCH 2338
2020-02-26 23:02:25,151 Epoch 2338 Step:   259500 Batch Loss:     0.000005 Tokens per Sec: 10210942, Lr: 0.000700
2020-02-26 23:02:26,793 Epoch 2338: total training loss 0.00062
2020-02-26 23:02:26,794 EPOCH 2339
2020-02-26 23:02:36,678 Epoch 2339: total training loss 0.00065
2020-02-26 23:02:36,678 EPOCH 2340
2020-02-26 23:02:46,615 Epoch 2340: total training loss 0.00064
2020-02-26 23:02:46,615 EPOCH 2341
2020-02-26 23:02:47,501 Epoch 2341 Step:   259750 Batch Loss:     0.000006 Tokens per Sec:  9728520, Lr: 0.000700
2020-02-26 23:02:56,606 Epoch 2341: total training loss 0.00065
2020-02-26 23:02:56,607 EPOCH 2342
2020-02-26 23:03:06,390 Epoch 2342: total training loss 0.00067
2020-02-26 23:03:06,391 EPOCH 2343
2020-02-26 23:03:09,626 Epoch 2343 Step:   260000 Batch Loss:     0.000006 Tokens per Sec: 10177430, Lr: 0.000700
2020-02-26 23:03:09,627 Model noise rate: 5
2020-02-26 23:04:11,451 Validation result at epoch 2343, step   260000: Val DTW Score:  10.67, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0408, GT DTW Score:      nan, duration: 61.8237s
2020-02-26 23:04:18,473 Epoch 2343: total training loss 0.00066
2020-02-26 23:04:18,474 EPOCH 2344
2020-02-26 23:04:28,794 Epoch 2344: total training loss 0.00062
2020-02-26 23:04:28,795 EPOCH 2345
2020-02-26 23:04:35,065 Epoch 2345 Step:   260250 Batch Loss:     0.000005 Tokens per Sec:  9738197, Lr: 0.000490
2020-02-26 23:04:39,317 Epoch 2345: total training loss 0.00060
2020-02-26 23:04:39,317 EPOCH 2346
2020-02-26 23:04:49,724 Epoch 2346: total training loss 0.00060
2020-02-26 23:04:49,725 EPOCH 2347
2020-02-26 23:04:58,406 Epoch 2347 Step:   260500 Batch Loss:     0.000004 Tokens per Sec: 10157582, Lr: 0.000490
2020-02-26 23:04:59,906 Epoch 2347: total training loss 0.00060
2020-02-26 23:04:59,906 EPOCH 2348
2020-02-26 23:05:10,022 Epoch 2348: total training loss 0.00060
2020-02-26 23:05:10,023 EPOCH 2349
2020-02-26 23:05:20,089 Epoch 2349: total training loss 0.00059
2020-02-26 23:05:20,090 EPOCH 2350
2020-02-26 23:05:21,041 Epoch 2350 Step:   260750 Batch Loss:     0.000007 Tokens per Sec:  9779314, Lr: 0.000490
2020-02-26 23:05:30,181 Epoch 2350: total training loss 0.00060
2020-02-26 23:05:30,181 EPOCH 2351
2020-02-26 23:05:40,242 Epoch 2351: total training loss 0.00060
2020-02-26 23:05:40,242 EPOCH 2352
2020-02-26 23:05:43,668 Epoch 2352 Step:   261000 Batch Loss:     0.000007 Tokens per Sec: 10137162, Lr: 0.000490
2020-02-26 23:05:50,287 Epoch 2352: total training loss 0.00060
2020-02-26 23:05:50,287 EPOCH 2353
2020-02-26 23:06:00,333 Epoch 2353: total training loss 0.00063
2020-02-26 23:06:00,333 EPOCH 2354
2020-02-26 23:06:06,297 Epoch 2354 Step:   261250 Batch Loss:     0.000011 Tokens per Sec: 10284500, Lr: 0.000490
2020-02-26 23:06:10,284 Epoch 2354: total training loss 0.00078
2020-02-26 23:06:10,284 EPOCH 2355
2020-02-26 23:06:20,320 Epoch 2355: total training loss 0.00065
2020-02-26 23:06:20,320 EPOCH 2356
2020-02-26 23:06:28,751 Epoch 2356 Step:   261500 Batch Loss:     0.000006 Tokens per Sec: 10362345, Lr: 0.000490
2020-02-26 23:06:30,226 Epoch 2356: total training loss 0.00060
2020-02-26 23:06:30,226 EPOCH 2357
2020-02-26 23:06:40,176 Epoch 2357: total training loss 0.00060
2020-02-26 23:06:40,178 EPOCH 2358
2020-02-26 23:06:50,276 Epoch 2358: total training loss 0.00060
2020-02-26 23:06:50,277 EPOCH 2359
2020-02-26 23:06:51,292 Epoch 2359 Step:   261750 Batch Loss:     0.000004 Tokens per Sec:  9543772, Lr: 0.000490
2020-02-26 23:07:00,731 Epoch 2359: total training loss 0.00060
2020-02-26 23:07:00,732 EPOCH 2360
2020-02-26 23:07:11,030 Epoch 2360: total training loss 0.00060
2020-02-26 23:07:11,031 EPOCH 2361
2020-02-26 23:07:15,070 Epoch 2361 Step:   262000 Batch Loss:     0.000003 Tokens per Sec:  9900861, Lr: 0.000490
2020-02-26 23:07:21,375 Epoch 2361: total training loss 0.00060
2020-02-26 23:07:21,376 EPOCH 2362
2020-02-26 23:07:31,949 Epoch 2362: total training loss 0.00060
2020-02-26 23:07:31,951 EPOCH 2363
2020-02-26 23:07:38,172 Epoch 2363 Step:   262250 Batch Loss:     0.000007 Tokens per Sec: 10128391, Lr: 0.000490
2020-02-26 23:07:42,156 Epoch 2363: total training loss 0.00062
2020-02-26 23:07:42,156 EPOCH 2364
2020-02-26 23:07:52,309 Epoch 2364: total training loss 0.00060
2020-02-26 23:07:52,309 EPOCH 2365
2020-02-26 23:08:01,119 Epoch 2365 Step:   262500 Batch Loss:     0.000005 Tokens per Sec: 10149322, Lr: 0.000490
2020-02-26 23:08:02,501 Epoch 2365: total training loss 0.00059
2020-02-26 23:08:02,501 EPOCH 2366
2020-02-26 23:08:12,580 Epoch 2366: total training loss 0.00060
2020-02-26 23:08:12,580 EPOCH 2367
2020-02-26 23:08:22,435 Epoch 2367: total training loss 0.00060
2020-02-26 23:08:22,436 EPOCH 2368
2020-02-26 23:08:23,522 Epoch 2368 Step:   262750 Batch Loss:     0.000003 Tokens per Sec:  9882065, Lr: 0.000490
2020-02-26 23:08:32,376 Epoch 2368: total training loss 0.00060
2020-02-26 23:08:32,377 EPOCH 2369
2020-02-26 23:08:42,256 Epoch 2369: total training loss 0.00060
2020-02-26 23:08:42,256 EPOCH 2370
2020-02-26 23:08:45,842 Epoch 2370 Step:   263000 Batch Loss:     0.000005 Tokens per Sec: 10335595, Lr: 0.000490
2020-02-26 23:08:52,141 Epoch 2370: total training loss 0.00059
2020-02-26 23:08:52,142 EPOCH 2371
2020-02-26 23:09:02,246 Epoch 2371: total training loss 0.00060
2020-02-26 23:09:02,246 EPOCH 2372
2020-02-26 23:09:08,429 Epoch 2372 Step:   263250 Batch Loss:     0.000004 Tokens per Sec: 10473874, Lr: 0.000490
2020-02-26 23:09:12,277 Epoch 2372: total training loss 0.00060
2020-02-26 23:09:12,278 EPOCH 2373
2020-02-26 23:09:22,268 Epoch 2373: total training loss 0.00061
2020-02-26 23:09:22,270 EPOCH 2374
2020-02-26 23:09:31,325 Epoch 2374 Step:   263500 Batch Loss:     0.000005 Tokens per Sec: 10047639, Lr: 0.000490
2020-02-26 23:09:32,606 Epoch 2374: total training loss 0.00060
2020-02-26 23:09:32,606 EPOCH 2375
2020-02-26 23:09:43,121 Epoch 2375: total training loss 0.00060
2020-02-26 23:09:43,122 EPOCH 2376
2020-02-26 23:09:53,735 Epoch 2376: total training loss 0.00061
2020-02-26 23:09:53,735 EPOCH 2377
2020-02-26 23:09:55,095 Epoch 2377 Step:   263750 Batch Loss:     0.000005 Tokens per Sec: 10207294, Lr: 0.000490
2020-02-26 23:10:04,272 Epoch 2377: total training loss 0.00065
2020-02-26 23:10:04,273 EPOCH 2378
2020-02-26 23:10:14,812 Epoch 2378: total training loss 0.00061
2020-02-26 23:10:14,813 EPOCH 2379
2020-02-26 23:10:18,486 Epoch 2379 Step:   264000 Batch Loss:     0.000007 Tokens per Sec:  9937042, Lr: 0.000490
2020-02-26 23:10:24,945 Epoch 2379: total training loss 0.00060
2020-02-26 23:10:24,946 EPOCH 2380
2020-02-26 23:10:34,948 Epoch 2380: total training loss 0.00060
2020-02-26 23:10:34,948 EPOCH 2381
2020-02-26 23:10:41,563 Epoch 2381 Step:   264250 Batch Loss:     0.000007 Tokens per Sec: 10044367, Lr: 0.000490
2020-02-26 23:10:45,135 Epoch 2381: total training loss 0.00060
2020-02-26 23:10:45,136 EPOCH 2382
2020-02-26 23:10:55,289 Epoch 2382: total training loss 0.00059
2020-02-26 23:10:55,291 EPOCH 2383
2020-02-26 23:11:04,160 Epoch 2383 Step:   264500 Batch Loss:     0.000003 Tokens per Sec: 10271338, Lr: 0.000490
2020-02-26 23:11:05,338 Epoch 2383: total training loss 0.00059
2020-02-26 23:11:05,338 EPOCH 2384
2020-02-26 23:11:15,173 Epoch 2384: total training loss 0.00060
2020-02-26 23:11:15,174 EPOCH 2385
2020-02-26 23:11:25,135 Epoch 2385: total training loss 0.00060
2020-02-26 23:11:25,136 EPOCH 2386
2020-02-26 23:11:26,363 Epoch 2386 Step:   264750 Batch Loss:     0.000004 Tokens per Sec:  9834067, Lr: 0.000490
2020-02-26 23:11:35,036 Epoch 2386: total training loss 0.00060
2020-02-26 23:11:35,037 EPOCH 2387
2020-02-26 23:11:44,836 Epoch 2387: total training loss 0.00060
2020-02-26 23:11:44,837 EPOCH 2388
2020-02-26 23:11:48,472 Epoch 2388 Step:   265000 Batch Loss:     0.000003 Tokens per Sec: 10499539, Lr: 0.000490
2020-02-26 23:11:54,626 Epoch 2388: total training loss 0.00061
2020-02-26 23:11:54,626 EPOCH 2389
2020-02-26 23:12:04,475 Epoch 2389: total training loss 0.00063
2020-02-26 23:12:04,476 EPOCH 2390
2020-02-26 23:12:10,729 Epoch 2390 Step:   265250 Batch Loss:     0.000003 Tokens per Sec: 10288653, Lr: 0.000490
2020-02-26 23:12:14,389 Epoch 2390: total training loss 0.00063
2020-02-26 23:12:14,389 EPOCH 2391
2020-02-26 23:12:24,518 Epoch 2391: total training loss 0.00061
2020-02-26 23:12:24,520 EPOCH 2392
2020-02-26 23:12:33,659 Epoch 2392 Step:   265500 Batch Loss:     0.000006 Tokens per Sec:  9979472, Lr: 0.000490
2020-02-26 23:12:34,796 Epoch 2392: total training loss 0.00060
2020-02-26 23:12:34,797 EPOCH 2393
2020-02-26 23:12:45,194 Epoch 2393: total training loss 0.00065
2020-02-26 23:12:45,195 EPOCH 2394
2020-02-26 23:12:55,751 Epoch 2394: total training loss 0.00063
2020-02-26 23:12:55,752 EPOCH 2395
2020-02-26 23:12:57,256 Epoch 2395 Step:   265750 Batch Loss:     0.000006 Tokens per Sec:  9686216, Lr: 0.000490
2020-02-26 23:13:06,161 Epoch 2395: total training loss 0.00060
2020-02-26 23:13:06,162 EPOCH 2396
2020-02-26 23:13:16,373 Epoch 2396: total training loss 0.00060
2020-02-26 23:13:16,373 EPOCH 2397
2020-02-26 23:13:20,523 Epoch 2397 Step:   266000 Batch Loss:     0.000003 Tokens per Sec: 10114683, Lr: 0.000490
2020-02-26 23:13:26,600 Epoch 2397: total training loss 0.00060
2020-02-26 23:13:26,601 EPOCH 2398
2020-02-26 23:13:36,605 Epoch 2398: total training loss 0.00060
2020-02-26 23:13:36,606 EPOCH 2399
2020-02-26 23:13:43,320 Epoch 2399 Step:   266250 Batch Loss:     0.000002 Tokens per Sec: 10255621, Lr: 0.000490
2020-02-26 23:13:46,691 Epoch 2399: total training loss 0.00061
2020-02-26 23:13:46,691 EPOCH 2400
2020-02-26 23:13:56,671 Epoch 2400: total training loss 0.00060
2020-02-26 23:13:56,672 EPOCH 2401
2020-02-26 23:14:05,678 Epoch 2401 Step:   266500 Batch Loss:     0.000005 Tokens per Sec: 10268565, Lr: 0.000490
2020-02-26 23:14:06,659 Epoch 2401: total training loss 0.00060
2020-02-26 23:14:06,659 EPOCH 2402
2020-02-26 23:14:16,711 Epoch 2402: total training loss 0.00060
2020-02-26 23:14:16,711 EPOCH 2403
2020-02-26 23:14:26,672 Epoch 2403: total training loss 0.00060
2020-02-26 23:14:26,673 EPOCH 2404
2020-02-26 23:14:28,157 Epoch 2404 Step:   266750 Batch Loss:     0.000004 Tokens per Sec: 10314343, Lr: 0.000490
2020-02-26 23:14:36,529 Epoch 2404: total training loss 0.00061
2020-02-26 23:14:36,530 EPOCH 2405
2020-02-26 23:14:46,440 Epoch 2405: total training loss 0.00061
2020-02-26 23:14:46,440 EPOCH 2406
2020-02-26 23:14:50,577 Epoch 2406 Step:   267000 Batch Loss:     0.000005 Tokens per Sec: 10496970, Lr: 0.000490
2020-02-26 23:14:56,386 Epoch 2406: total training loss 0.00060
2020-02-26 23:14:56,388 EPOCH 2407
2020-02-26 23:15:06,796 Epoch 2407: total training loss 0.00059
2020-02-26 23:15:06,797 EPOCH 2408
2020-02-26 23:15:13,917 Epoch 2408 Step:   267250 Batch Loss:     0.000008 Tokens per Sec:  9575060, Lr: 0.000490
2020-02-26 23:15:17,509 Epoch 2408: total training loss 0.00059
2020-02-26 23:15:17,510 EPOCH 2409
2020-02-26 23:15:27,787 Epoch 2409: total training loss 0.00060
2020-02-26 23:15:27,788 EPOCH 2410
2020-02-26 23:15:37,202 Epoch 2410 Step:   267500 Batch Loss:     0.000004 Tokens per Sec:  9876770, Lr: 0.000490
2020-02-26 23:15:38,151 Epoch 2410: total training loss 0.00061
2020-02-26 23:15:38,152 EPOCH 2411
2020-02-26 23:15:48,390 Epoch 2411: total training loss 0.00062
2020-02-26 23:15:48,390 EPOCH 2412
2020-02-26 23:15:58,390 Epoch 2412: total training loss 0.00060
2020-02-26 23:15:58,391 EPOCH 2413
2020-02-26 23:16:00,146 Epoch 2413 Step:   267750 Batch Loss:     0.000005 Tokens per Sec: 10222880, Lr: 0.000490
2020-02-26 23:16:08,442 Epoch 2413: total training loss 0.00059
2020-02-26 23:16:08,443 EPOCH 2414
2020-02-26 23:16:18,539 Epoch 2414: total training loss 0.00059
2020-02-26 23:16:18,539 EPOCH 2415
2020-02-26 23:16:22,825 Epoch 2415 Step:   268000 Batch Loss:     0.000006 Tokens per Sec: 10233114, Lr: 0.000490
2020-02-26 23:16:28,577 Epoch 2415: total training loss 0.00059
2020-02-26 23:16:28,578 EPOCH 2416
2020-02-26 23:16:38,488 Epoch 2416: total training loss 0.00060
2020-02-26 23:16:38,489 EPOCH 2417
2020-02-26 23:16:45,119 Epoch 2417 Step:   268250 Batch Loss:     0.000005 Tokens per Sec: 10301290, Lr: 0.000490
2020-02-26 23:16:48,438 Epoch 2417: total training loss 0.00060
2020-02-26 23:16:48,440 EPOCH 2418
2020-02-26 23:16:58,308 Epoch 2418: total training loss 0.00060
2020-02-26 23:16:58,308 EPOCH 2419
2020-02-26 23:17:07,415 Epoch 2419 Step:   268500 Batch Loss:     0.000005 Tokens per Sec: 10326368, Lr: 0.000490
2020-02-26 23:17:08,238 Epoch 2419: total training loss 0.00060
2020-02-26 23:17:08,238 EPOCH 2420
2020-02-26 23:17:18,189 Epoch 2420: total training loss 0.00060
2020-02-26 23:17:18,190 EPOCH 2421
2020-02-26 23:17:28,170 Epoch 2421: total training loss 0.00061
2020-02-26 23:17:28,172 EPOCH 2422
2020-02-26 23:17:30,042 Epoch 2422 Step:   268750 Batch Loss:     0.000004 Tokens per Sec: 10065363, Lr: 0.000490
2020-02-26 23:17:38,354 Epoch 2422: total training loss 0.00061
2020-02-26 23:17:38,355 EPOCH 2423
2020-02-26 23:17:48,630 Epoch 2423: total training loss 0.00059
2020-02-26 23:17:48,630 EPOCH 2424
2020-02-26 23:17:53,283 Epoch 2424 Step:   269000 Batch Loss:     0.000006 Tokens per Sec: 10166307, Lr: 0.000490
2020-02-26 23:17:58,911 Epoch 2424: total training loss 0.00059
2020-02-26 23:17:58,912 EPOCH 2425
2020-02-26 23:18:09,023 Epoch 2425: total training loss 0.00059
2020-02-26 23:18:09,024 EPOCH 2426
2020-02-26 23:18:15,691 Epoch 2426 Step:   269250 Batch Loss:     0.000005 Tokens per Sec: 10071497, Lr: 0.000490
2020-02-26 23:18:19,131 Epoch 2426: total training loss 0.00059
2020-02-26 23:18:19,132 EPOCH 2427
2020-02-26 23:18:29,094 Epoch 2427: total training loss 0.00059
2020-02-26 23:18:29,094 EPOCH 2428
2020-02-26 23:18:38,389 Epoch 2428 Step:   269500 Batch Loss:     0.000004 Tokens per Sec: 10305855, Lr: 0.000490
2020-02-26 23:18:39,073 Epoch 2428: total training loss 0.00060
2020-02-26 23:18:39,073 EPOCH 2429
2020-02-26 23:18:48,938 Epoch 2429: total training loss 0.00063
2020-02-26 23:18:48,939 EPOCH 2430
2020-02-26 23:18:58,871 Epoch 2430: total training loss 0.00061
2020-02-26 23:18:58,872 EPOCH 2431
2020-02-26 23:19:00,594 Epoch 2431 Step:   269750 Batch Loss:     0.000004 Tokens per Sec: 10022496, Lr: 0.000490
2020-02-26 23:19:08,765 Epoch 2431: total training loss 0.00060
2020-02-26 23:19:08,766 EPOCH 2432
2020-02-26 23:19:18,898 Epoch 2432: total training loss 0.00061
2020-02-26 23:19:18,899 EPOCH 2433
2020-02-26 23:19:23,191 Epoch 2433 Step:   270000 Batch Loss:     0.000005 Tokens per Sec: 10332516, Lr: 0.000490
2020-02-26 23:19:23,191 Model noise rate: 5
2020-02-26 23:20:24,910 Validation result at epoch 2433, step   270000: Val DTW Score:  10.71, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0420, GT DTW Score:      nan, duration: 61.7187s
2020-02-26 23:20:31,015 Epoch 2433: total training loss 0.00061
2020-02-26 23:20:31,015 EPOCH 2434
2020-02-26 23:20:41,416 Epoch 2434: total training loss 0.00060
2020-02-26 23:20:41,416 EPOCH 2435
2020-02-26 23:20:48,716 Epoch 2435 Step:   270250 Batch Loss:     0.000004 Tokens per Sec:  9876543, Lr: 0.000490
2020-02-26 23:20:51,765 Epoch 2435: total training loss 0.00059
2020-02-26 23:20:51,766 EPOCH 2436
2020-02-26 23:21:01,884 Epoch 2436: total training loss 0.00059
2020-02-26 23:21:01,884 EPOCH 2437
2020-02-26 23:21:11,324 Epoch 2437 Step:   270500 Batch Loss:     0.000004 Tokens per Sec: 10289388, Lr: 0.000490
2020-02-26 23:21:11,989 Epoch 2437: total training loss 0.00061
2020-02-26 23:21:11,989 EPOCH 2438
2020-02-26 23:21:21,893 Epoch 2438: total training loss 0.00060
2020-02-26 23:21:21,895 EPOCH 2439
2020-02-26 23:21:31,857 Epoch 2439: total training loss 0.00061
2020-02-26 23:21:31,858 EPOCH 2440
2020-02-26 23:21:33,673 Epoch 2440 Step:   270750 Batch Loss:     0.000006 Tokens per Sec: 10180329, Lr: 0.000490
2020-02-26 23:21:41,842 Epoch 2440: total training loss 0.00060
2020-02-26 23:21:41,842 EPOCH 2441
2020-02-26 23:21:51,734 Epoch 2441: total training loss 0.00063
2020-02-26 23:21:51,735 EPOCH 2442
2020-02-26 23:21:55,956 Epoch 2442 Step:   271000 Batch Loss:     0.000006 Tokens per Sec: 10338787, Lr: 0.000490
2020-02-26 23:22:01,577 Epoch 2442: total training loss 0.00062
2020-02-26 23:22:01,577 EPOCH 2443
2020-02-26 23:22:11,237 Epoch 2443: total training loss 0.00062
2020-02-26 23:22:11,238 EPOCH 2444
2020-02-26 23:22:17,892 Epoch 2444 Step:   271250 Batch Loss:     0.000006 Tokens per Sec: 10496054, Lr: 0.000490
2020-02-26 23:22:21,024 Epoch 2444: total training loss 0.00060
2020-02-26 23:22:21,024 EPOCH 2445
2020-02-26 23:22:30,939 Epoch 2445: total training loss 0.00059
2020-02-26 23:22:30,939 EPOCH 2446
2020-02-26 23:22:40,232 Epoch 2446 Step:   271500 Batch Loss:     0.000006 Tokens per Sec: 10416403, Lr: 0.000490
2020-02-26 23:22:40,775 Epoch 2446: total training loss 0.00059
2020-02-26 23:22:40,775 EPOCH 2447
2020-02-26 23:22:50,670 Epoch 2447: total training loss 0.00059
2020-02-26 23:22:50,671 EPOCH 2448
2020-02-26 23:23:01,064 Epoch 2448: total training loss 0.00059
2020-02-26 23:23:01,065 EPOCH 2449
2020-02-26 23:23:02,911 Epoch 2449 Step:   271750 Batch Loss:     0.000003 Tokens per Sec:  9803758, Lr: 0.000490
2020-02-26 23:23:11,658 Epoch 2449: total training loss 0.00059
2020-02-26 23:23:11,658 EPOCH 2450
2020-02-26 23:23:22,413 Epoch 2450: total training loss 0.00060
2020-02-26 23:23:22,415 EPOCH 2451
2020-02-26 23:23:27,277 Epoch 2451 Step:   272000 Batch Loss:     0.000006 Tokens per Sec:  9833861, Lr: 0.000490
2020-02-26 23:23:33,012 Epoch 2451: total training loss 0.00059
2020-02-26 23:23:33,012 EPOCH 2452
2020-02-26 23:23:43,664 Epoch 2452: total training loss 0.00059
2020-02-26 23:23:43,664 EPOCH 2453
2020-02-26 23:23:50,817 Epoch 2453 Step:   272250 Batch Loss:     0.000004 Tokens per Sec: 10104421, Lr: 0.000490
2020-02-26 23:23:53,865 Epoch 2453: total training loss 0.00059
2020-02-26 23:23:53,865 EPOCH 2454
2020-02-26 23:24:04,008 Epoch 2454: total training loss 0.00059
2020-02-26 23:24:04,008 EPOCH 2455
2020-02-26 23:24:13,625 Epoch 2455 Step:   272500 Batch Loss:     0.000004 Tokens per Sec: 10217432, Lr: 0.000490
2020-02-26 23:24:14,079 Epoch 2455: total training loss 0.00059
2020-02-26 23:24:14,079 EPOCH 2456
2020-02-26 23:24:24,194 Epoch 2456: total training loss 0.00060
2020-02-26 23:24:24,194 EPOCH 2457
2020-02-26 23:24:34,027 Epoch 2457: total training loss 0.00064
2020-02-26 23:24:34,027 EPOCH 2458
2020-02-26 23:24:36,085 Epoch 2458 Step:   272750 Batch Loss:     0.000006 Tokens per Sec: 10047263, Lr: 0.000490
2020-02-26 23:24:44,025 Epoch 2458: total training loss 0.00061
2020-02-26 23:24:44,026 EPOCH 2459
2020-02-26 23:24:53,975 Epoch 2459: total training loss 0.00060
2020-02-26 23:24:53,975 EPOCH 2460
2020-02-26 23:24:58,608 Epoch 2460 Step:   273000 Batch Loss:     0.000007 Tokens per Sec: 10205422, Lr: 0.000490
2020-02-26 23:25:04,060 Epoch 2460: total training loss 0.00059
2020-02-26 23:25:04,060 EPOCH 2461
2020-02-26 23:25:14,118 Epoch 2461: total training loss 0.00059
2020-02-26 23:25:14,118 EPOCH 2462
2020-02-26 23:25:21,340 Epoch 2462 Step:   273250 Batch Loss:     0.000008 Tokens per Sec: 10274604, Lr: 0.000490
2020-02-26 23:25:24,122 Epoch 2462: total training loss 0.00060
2020-02-26 23:25:24,122 EPOCH 2463
2020-02-26 23:25:34,561 Epoch 2463: total training loss 0.00059
2020-02-26 23:25:34,562 EPOCH 2464
2020-02-26 23:25:44,583 Epoch 2464 Step:   273500 Batch Loss:     0.000004 Tokens per Sec:  9943574, Lr: 0.000490
2020-02-26 23:25:44,923 Epoch 2464: total training loss 0.00059
2020-02-26 23:25:44,923 EPOCH 2465
2020-02-26 23:25:55,232 Epoch 2465: total training loss 0.00061
2020-02-26 23:25:55,232 EPOCH 2466
2020-02-26 23:26:05,544 Epoch 2466: total training loss 0.00061
2020-02-26 23:26:05,545 EPOCH 2467
2020-02-26 23:26:07,637 Epoch 2467 Step:   273750 Batch Loss:     0.000005 Tokens per Sec:  9979998, Lr: 0.000490
2020-02-26 23:26:15,616 Epoch 2467: total training loss 0.00061
2020-02-26 23:26:15,617 EPOCH 2468
2020-02-26 23:26:25,908 Epoch 2468: total training loss 0.00060
2020-02-26 23:26:25,909 EPOCH 2469
2020-02-26 23:26:30,969 Epoch 2469 Step:   274000 Batch Loss:     0.000004 Tokens per Sec:  9984477, Lr: 0.000490
2020-02-26 23:26:36,125 Epoch 2469: total training loss 0.00061
2020-02-26 23:26:36,125 EPOCH 2470
2020-02-26 23:26:46,327 Epoch 2470: total training loss 0.00059
2020-02-26 23:26:46,328 EPOCH 2471
2020-02-26 23:26:53,409 Epoch 2471 Step:   274250 Batch Loss:     0.000005 Tokens per Sec: 10076555, Lr: 0.000490
2020-02-26 23:26:56,370 Epoch 2471: total training loss 0.00062
2020-02-26 23:26:56,371 EPOCH 2472
2020-02-26 23:27:06,227 Epoch 2472: total training loss 0.00061
2020-02-26 23:27:06,227 EPOCH 2473
2020-02-26 23:27:15,864 Epoch 2473 Step:   274500 Batch Loss:     0.000006 Tokens per Sec: 10353556, Lr: 0.000490
2020-02-26 23:27:16,167 Epoch 2473: total training loss 0.00059
2020-02-26 23:27:16,167 EPOCH 2474
2020-02-26 23:27:26,184 Epoch 2474: total training loss 0.00058
2020-02-26 23:27:26,184 EPOCH 2475
2020-02-26 23:27:36,180 Epoch 2475: total training loss 0.00058
2020-02-26 23:27:36,180 EPOCH 2476
2020-02-26 23:27:38,501 Epoch 2476 Step:   274750 Batch Loss:     0.000004 Tokens per Sec: 10536655, Lr: 0.000490
2020-02-26 23:27:46,165 Epoch 2476: total training loss 0.00059
2020-02-26 23:27:46,166 EPOCH 2477
2020-02-26 23:27:56,092 Epoch 2477: total training loss 0.00059
2020-02-26 23:27:56,093 EPOCH 2478
2020-02-26 23:28:00,788 Epoch 2478 Step:   275000 Batch Loss:     0.000008 Tokens per Sec: 10105347, Lr: 0.000490
2020-02-26 23:28:06,164 Epoch 2478: total training loss 0.00059
2020-02-26 23:28:06,165 EPOCH 2479
2020-02-26 23:28:16,574 Epoch 2479: total training loss 0.00059
2020-02-26 23:28:16,574 EPOCH 2480
2020-02-26 23:28:24,127 Epoch 2480 Step:   275250 Batch Loss:     0.000004 Tokens per Sec:  9629511, Lr: 0.000490
2020-02-26 23:28:27,271 Epoch 2480: total training loss 0.00059
2020-02-26 23:28:27,272 EPOCH 2481
2020-02-26 23:28:37,946 Epoch 2481: total training loss 0.00063
2020-02-26 23:28:37,947 EPOCH 2482
2020-02-26 23:28:48,455 Epoch 2482 Step:   275500 Batch Loss:     0.000008 Tokens per Sec:  9546854, Lr: 0.000490
2020-02-26 23:28:48,669 Epoch 2482: total training loss 0.00061
2020-02-26 23:28:48,669 EPOCH 2483
2020-02-26 23:28:58,949 Epoch 2483: total training loss 0.00060
2020-02-26 23:28:58,950 EPOCH 2484
2020-02-26 23:29:08,984 Epoch 2484: total training loss 0.00059
2020-02-26 23:29:08,984 EPOCH 2485
2020-02-26 23:29:11,422 Epoch 2485 Step:   275750 Batch Loss:     0.000006 Tokens per Sec: 10254454, Lr: 0.000490
2020-02-26 23:29:19,099 Epoch 2485: total training loss 0.00060
2020-02-26 23:29:19,100 EPOCH 2486
2020-02-26 23:29:29,198 Epoch 2486: total training loss 0.00060
2020-02-26 23:29:29,198 EPOCH 2487
2020-02-26 23:29:33,854 Epoch 2487 Step:   276000 Batch Loss:     0.000010 Tokens per Sec: 10263686, Lr: 0.000490
2020-02-26 23:29:39,154 Epoch 2487: total training loss 0.00073
2020-02-26 23:29:39,154 EPOCH 2488
2020-02-26 23:29:49,157 Epoch 2488: total training loss 0.00061
2020-02-26 23:29:49,158 EPOCH 2489
2020-02-26 23:29:56,568 Epoch 2489 Step:   276250 Batch Loss:     0.000005 Tokens per Sec: 10270853, Lr: 0.000490
2020-02-26 23:29:59,203 Epoch 2489: total training loss 0.00059
2020-02-26 23:29:59,203 EPOCH 2490
2020-02-26 23:30:09,205 Epoch 2490: total training loss 0.00059
2020-02-26 23:30:09,205 EPOCH 2491
2020-02-26 23:30:19,005 Epoch 2491 Step:   276500 Batch Loss:     0.000006 Tokens per Sec: 10295974, Lr: 0.000490
2020-02-26 23:30:19,147 Epoch 2491: total training loss 0.00059
2020-02-26 23:30:19,147 EPOCH 2492
2020-02-26 23:30:29,070 Epoch 2492: total training loss 0.00059
2020-02-26 23:30:29,071 EPOCH 2493
2020-02-26 23:30:39,167 Epoch 2493: total training loss 0.00059
2020-02-26 23:30:39,168 EPOCH 2494
2020-02-26 23:30:41,670 Epoch 2494 Step:   276750 Batch Loss:     0.000004 Tokens per Sec: 10103620, Lr: 0.000490
2020-02-26 23:30:49,560 Epoch 2494: total training loss 0.00060
2020-02-26 23:30:49,560 EPOCH 2495
2020-02-26 23:31:00,070 Epoch 2495: total training loss 0.00062
2020-02-26 23:31:00,071 EPOCH 2496
2020-02-26 23:31:05,067 Epoch 2496 Step:   277000 Batch Loss:     0.000007 Tokens per Sec:  9921093, Lr: 0.000490
2020-02-26 23:31:10,339 Epoch 2496: total training loss 0.00061
2020-02-26 23:31:10,339 EPOCH 2497
2020-02-26 23:31:20,807 Epoch 2497: total training loss 0.00059
2020-02-26 23:31:20,809 EPOCH 2498
2020-02-26 23:31:28,378 Epoch 2498 Step:   277250 Batch Loss:     0.000005 Tokens per Sec: 10254298, Lr: 0.000490
2020-02-26 23:31:30,906 Epoch 2498: total training loss 0.00058
2020-02-26 23:31:30,906 EPOCH 2499
2020-02-26 23:31:40,973 Epoch 2499: total training loss 0.00060
2020-02-26 23:31:40,974 EPOCH 2500
2020-02-26 23:31:51,118 Epoch 2500 Step:   277500 Batch Loss:     0.000006 Tokens per Sec: 10166301, Lr: 0.000490
2020-02-26 23:31:51,119 Epoch 2500: total training loss 0.00061
2020-02-26 23:31:51,119 EPOCH 2501
2020-02-26 23:32:01,061 Epoch 2501: total training loss 0.00059
2020-02-26 23:32:01,061 EPOCH 2502
2020-02-26 23:32:10,804 Epoch 2502: total training loss 0.00059
2020-02-26 23:32:10,805 EPOCH 2503
2020-02-26 23:32:13,361 Epoch 2503 Step:   277750 Batch Loss:     0.000008 Tokens per Sec: 10620392, Lr: 0.000490
2020-02-26 23:32:20,687 Epoch 2503: total training loss 0.00060
2020-02-26 23:32:20,688 EPOCH 2504
2020-02-26 23:32:30,581 Epoch 2504: total training loss 0.00061
2020-02-26 23:32:30,581 EPOCH 2505
2020-02-26 23:32:35,557 Epoch 2505 Step:   278000 Batch Loss:     0.000009 Tokens per Sec: 10452833, Lr: 0.000490
2020-02-26 23:32:40,288 Epoch 2505: total training loss 0.00061
2020-02-26 23:32:40,289 EPOCH 2506
2020-02-26 23:32:50,093 Epoch 2506: total training loss 0.00059
2020-02-26 23:32:50,510 EPOCH 2507
2020-02-26 23:32:58,273 Epoch 2507 Step:   278250 Batch Loss:     0.000003 Tokens per Sec: 10107185, Lr: 0.000490
2020-02-26 23:33:00,501 Epoch 2507: total training loss 0.00059
2020-02-26 23:33:00,501 EPOCH 2508
2020-02-26 23:33:10,285 Epoch 2508: total training loss 0.00058
2020-02-26 23:33:10,285 EPOCH 2509
2020-02-26 23:33:20,061 Epoch 2509: total training loss 0.00060
2020-02-26 23:33:20,061 EPOCH 2510
2020-02-26 23:33:20,184 Epoch 2510 Step:   278500 Batch Loss:     0.000007 Tokens per Sec:  8527953, Lr: 0.000490
2020-02-26 23:33:30,060 Epoch 2510: total training loss 0.00060
2020-02-26 23:33:30,061 EPOCH 2511
2020-02-26 23:33:40,153 Epoch 2511: total training loss 0.00059
2020-02-26 23:33:40,154 EPOCH 2512
2020-02-26 23:33:42,869 Epoch 2512 Step:   278750 Batch Loss:     0.000003 Tokens per Sec: 10248922, Lr: 0.000490
2020-02-26 23:33:50,597 Epoch 2512: total training loss 0.00059
2020-02-26 23:33:50,600 EPOCH 2513
2020-02-26 23:34:01,030 Epoch 2513: total training loss 0.00060
2020-02-26 23:34:01,030 EPOCH 2514
2020-02-26 23:34:06,580 Epoch 2514 Step:   279000 Batch Loss:     0.000005 Tokens per Sec:  9522617, Lr: 0.000490
2020-02-26 23:34:11,702 Epoch 2514: total training loss 0.00059
2020-02-26 23:34:11,703 EPOCH 2515
2020-02-26 23:34:22,352 Epoch 2515: total training loss 0.00059
2020-02-26 23:34:22,353 EPOCH 2516
2020-02-26 23:34:30,097 Epoch 2516 Step:   279250 Batch Loss:     0.000007 Tokens per Sec:  9848989, Lr: 0.000490
2020-02-26 23:34:32,682 Epoch 2516: total training loss 0.00059
2020-02-26 23:34:32,683 EPOCH 2517
2020-02-26 23:34:42,761 Epoch 2517: total training loss 0.00059
2020-02-26 23:34:42,762 EPOCH 2518
2020-02-26 23:34:52,998 Epoch 2518: total training loss 0.00058
2020-02-26 23:34:52,999 EPOCH 2519
2020-02-26 23:34:53,173 Epoch 2519 Step:   279500 Batch Loss:     0.000005 Tokens per Sec:  8398530, Lr: 0.000490
2020-02-26 23:35:03,101 Epoch 2519: total training loss 0.00058
2020-02-26 23:35:03,101 EPOCH 2520
2020-02-26 23:35:13,196 Epoch 2520: total training loss 0.00059
2020-02-26 23:35:13,197 EPOCH 2521
2020-02-26 23:35:15,833 Epoch 2521 Step:   279750 Batch Loss:     0.000005 Tokens per Sec: 10295046, Lr: 0.000490
2020-02-26 23:35:23,226 Epoch 2521: total training loss 0.00060
2020-02-26 23:35:23,227 EPOCH 2522
2020-02-26 23:35:33,371 Epoch 2522: total training loss 0.00058
2020-02-26 23:35:33,371 EPOCH 2523
2020-02-26 23:35:38,780 Epoch 2523 Step:   280000 Batch Loss:     0.000006 Tokens per Sec: 10285136, Lr: 0.000490
2020-02-26 23:35:38,781 Model noise rate: 5
2020-02-26 23:36:41,249 Validation result at epoch 2523, step   280000: Val DTW Score:  10.74, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0421, GT DTW Score:      nan, duration: 62.4675s
2020-02-26 23:36:45,993 Epoch 2523: total training loss 0.00059
2020-02-26 23:36:45,994 EPOCH 2524
2020-02-26 23:36:56,383 Epoch 2524: total training loss 0.00059
2020-02-26 23:36:56,384 EPOCH 2525
2020-02-26 23:37:04,274 Epoch 2525 Step:   280250 Batch Loss:     0.000005 Tokens per Sec:  9907772, Lr: 0.000490
2020-02-26 23:37:06,731 Epoch 2525: total training loss 0.00059
2020-02-26 23:37:06,731 EPOCH 2526
2020-02-26 23:37:17,349 Epoch 2526: total training loss 0.00060
2020-02-26 23:37:17,349 EPOCH 2527
2020-02-26 23:37:27,535 Epoch 2527: total training loss 0.00061
2020-02-26 23:37:27,536 EPOCH 2528
2020-02-26 23:37:27,787 Epoch 2528 Step:   280500 Batch Loss:     0.000006 Tokens per Sec:  8570736, Lr: 0.000490
2020-02-26 23:37:37,776 Epoch 2528: total training loss 0.00059
2020-02-26 23:37:37,776 EPOCH 2529
2020-02-26 23:37:47,899 Epoch 2529: total training loss 0.00059
2020-02-26 23:37:47,899 EPOCH 2530
2020-02-26 23:37:50,872 Epoch 2530 Step:   280750 Batch Loss:     0.000003 Tokens per Sec: 10443450, Lr: 0.000490
2020-02-26 23:37:57,949 Epoch 2530: total training loss 0.00058
2020-02-26 23:37:57,950 EPOCH 2531
2020-02-26 23:38:07,943 Epoch 2531: total training loss 0.00060
2020-02-26 23:38:07,944 EPOCH 2532
2020-02-26 23:38:13,203 Epoch 2532 Step:   281000 Batch Loss:     0.000005 Tokens per Sec: 10345755, Lr: 0.000490
2020-02-26 23:38:17,834 Epoch 2532: total training loss 0.00060
2020-02-26 23:38:17,835 EPOCH 2533
2020-02-26 23:38:27,806 Epoch 2533: total training loss 0.00059
2020-02-26 23:38:27,806 EPOCH 2534
2020-02-26 23:38:35,459 Epoch 2534 Step:   281250 Batch Loss:     0.000003 Tokens per Sec: 10221133, Lr: 0.000490
2020-02-26 23:38:37,837 Epoch 2534: total training loss 0.00059
2020-02-26 23:38:37,837 EPOCH 2535
2020-02-26 23:38:47,873 Epoch 2535: total training loss 0.00059
2020-02-26 23:38:47,873 EPOCH 2536
2020-02-26 23:38:57,854 Epoch 2536: total training loss 0.00059
2020-02-26 23:38:57,855 EPOCH 2537
2020-02-26 23:38:58,301 Epoch 2537 Step:   281500 Batch Loss:     0.000007 Tokens per Sec:  9963879, Lr: 0.000490
2020-02-26 23:39:07,893 Epoch 2537: total training loss 0.00064
2020-02-26 23:39:07,894 EPOCH 2538
2020-02-26 23:39:18,324 Epoch 2538: total training loss 0.00066
2020-02-26 23:39:18,325 EPOCH 2539
2020-02-26 23:39:21,397 Epoch 2539 Step:   281750 Batch Loss:     0.000003 Tokens per Sec:  9177196, Lr: 0.000490
2020-02-26 23:39:28,891 Epoch 2539: total training loss 0.00060
2020-02-26 23:39:28,891 EPOCH 2540
2020-02-26 23:39:39,233 Epoch 2540: total training loss 0.00060
2020-02-26 23:39:39,234 EPOCH 2541
2020-02-26 23:39:44,732 Epoch 2541 Step:   282000 Batch Loss:     0.000004 Tokens per Sec:  9975584, Lr: 0.000490
2020-02-26 23:39:49,569 Epoch 2541: total training loss 0.00059
2020-02-26 23:39:49,570 EPOCH 2542
2020-02-26 23:39:59,863 Epoch 2542: total training loss 0.00059
2020-02-26 23:39:59,864 EPOCH 2543
2020-02-26 23:40:07,724 Epoch 2543 Step:   282250 Batch Loss:     0.000004 Tokens per Sec: 10305983, Lr: 0.000490
2020-02-26 23:40:09,847 Epoch 2543: total training loss 0.00059
2020-02-26 23:40:09,847 EPOCH 2544
2020-02-26 23:40:19,841 Epoch 2544: total training loss 0.00064
2020-02-26 23:40:19,841 EPOCH 2545
2020-02-26 23:40:30,084 Epoch 2545: total training loss 0.00062
2020-02-26 23:40:30,085 EPOCH 2546
2020-02-26 23:40:30,673 Epoch 2546 Step:   282500 Batch Loss:     0.000007 Tokens per Sec:  9714911, Lr: 0.000490
2020-02-26 23:40:40,232 Epoch 2546: total training loss 0.00060
2020-02-26 23:40:40,232 EPOCH 2547
2020-02-26 23:40:50,241 Epoch 2547: total training loss 0.00059
2020-02-26 23:40:50,242 EPOCH 2548
2020-02-26 23:40:53,339 Epoch 2548 Step:   282750 Batch Loss:     0.000004 Tokens per Sec: 10517611, Lr: 0.000490
2020-02-26 23:41:00,134 Epoch 2548: total training loss 0.00058
2020-02-26 23:41:00,135 EPOCH 2549
2020-02-26 23:41:10,114 Epoch 2549: total training loss 0.00058
2020-02-26 23:41:10,115 EPOCH 2550
2020-02-26 23:41:15,729 Epoch 2550 Step:   283000 Batch Loss:     0.000005 Tokens per Sec: 10362790, Lr: 0.000490
2020-02-26 23:41:20,026 Epoch 2550: total training loss 0.00058
2020-02-26 23:41:20,027 EPOCH 2551
2020-02-26 23:41:29,956 Epoch 2551: total training loss 0.00058
2020-02-26 23:41:29,957 EPOCH 2552
2020-02-26 23:41:37,885 Epoch 2552 Step:   283250 Batch Loss:     0.000004 Tokens per Sec: 10375700, Lr: 0.000490
2020-02-26 23:41:39,873 Epoch 2552: total training loss 0.00058
2020-02-26 23:41:39,874 EPOCH 2553
2020-02-26 23:41:49,796 Epoch 2553: total training loss 0.00058
2020-02-26 23:41:49,797 EPOCH 2554
2020-02-26 23:41:59,864 Epoch 2554: total training loss 0.00059
2020-02-26 23:41:59,865 EPOCH 2555
2020-02-26 23:42:00,480 Epoch 2555 Step:   283500 Batch Loss:     0.000006 Tokens per Sec:  9403184, Lr: 0.000490
2020-02-26 23:42:10,542 Epoch 2555: total training loss 0.00059
2020-02-26 23:42:10,542 EPOCH 2556
2020-02-26 23:42:21,090 Epoch 2556: total training loss 0.00058
2020-02-26 23:42:21,091 EPOCH 2557
2020-02-26 23:42:24,186 Epoch 2557 Step:   283750 Batch Loss:     0.000007 Tokens per Sec:  9663773, Lr: 0.000490
2020-02-26 23:42:31,614 Epoch 2557: total training loss 0.00059
2020-02-26 23:42:31,615 EPOCH 2558
2020-02-26 23:42:42,179 Epoch 2558: total training loss 0.00062
2020-02-26 23:42:42,180 EPOCH 2559
2020-02-26 23:42:47,531 Epoch 2559 Step:   284000 Batch Loss:     0.000006 Tokens per Sec: 10302696, Lr: 0.000490
2020-02-26 23:42:52,138 Epoch 2559: total training loss 0.00061
2020-02-26 23:42:52,139 EPOCH 2560
2020-02-26 23:43:02,136 Epoch 2560: total training loss 0.00059
2020-02-26 23:43:02,137 EPOCH 2561
2020-02-26 23:43:10,284 Epoch 2561 Step:   284250 Batch Loss:     0.000006 Tokens per Sec: 10295633, Lr: 0.000490
2020-02-26 23:43:12,191 Epoch 2561: total training loss 0.00059
2020-02-26 23:43:12,191 EPOCH 2562
2020-02-26 23:43:22,231 Epoch 2562: total training loss 0.00059
2020-02-26 23:43:22,232 EPOCH 2563
2020-02-26 23:43:32,246 Epoch 2563: total training loss 0.00058
2020-02-26 23:43:32,247 EPOCH 2564
2020-02-26 23:43:32,878 Epoch 2564 Step:   284500 Batch Loss:     0.000004 Tokens per Sec: 10016033, Lr: 0.000490
2020-02-26 23:43:42,302 Epoch 2564: total training loss 0.00064
2020-02-26 23:43:42,303 EPOCH 2565
2020-02-26 23:43:52,232 Epoch 2565: total training loss 0.00061
2020-02-26 23:43:52,232 EPOCH 2566
2020-02-26 23:43:55,437 Epoch 2566 Step:   284750 Batch Loss:     0.000008 Tokens per Sec: 10556479, Lr: 0.000490
2020-02-26 23:44:02,330 Epoch 2566: total training loss 0.00057
2020-02-26 23:44:02,330 EPOCH 2567
2020-02-26 23:44:12,371 Epoch 2567: total training loss 0.00058
2020-02-26 23:44:12,371 EPOCH 2568
2020-02-26 23:44:18,093 Epoch 2568 Step:   285000 Batch Loss:     0.000006 Tokens per Sec: 10304706, Lr: 0.000490
2020-02-26 23:44:22,344 Epoch 2568: total training loss 0.00059
2020-02-26 23:44:22,344 EPOCH 2569
2020-02-26 23:44:32,257 Epoch 2569: total training loss 0.00059
2020-02-26 23:44:32,258 EPOCH 2570
2020-02-26 23:44:40,511 Epoch 2570 Step:   285250 Batch Loss:     0.000007 Tokens per Sec: 10195235, Lr: 0.000490
2020-02-26 23:44:42,398 Epoch 2570: total training loss 0.00059
2020-02-26 23:44:42,399 EPOCH 2571
2020-02-26 23:44:52,933 Epoch 2571: total training loss 0.00058
2020-02-26 23:44:52,934 EPOCH 2572
2020-02-26 23:45:03,376 Epoch 2572: total training loss 0.00058
2020-02-26 23:45:03,377 EPOCH 2573
2020-02-26 23:45:04,190 Epoch 2573 Step:   285500 Batch Loss:     0.000007 Tokens per Sec: 10083091, Lr: 0.000490
2020-02-26 23:45:13,791 Epoch 2573: total training loss 0.00058
2020-02-26 23:45:13,791 EPOCH 2574
2020-02-26 23:45:24,173 Epoch 2574: total training loss 0.00060
2020-02-26 23:45:24,174 EPOCH 2575
2020-02-26 23:45:27,750 Epoch 2575 Step:   285750 Batch Loss:     0.000005 Tokens per Sec: 10055571, Lr: 0.000490
2020-02-26 23:45:34,377 Epoch 2575: total training loss 0.00058
2020-02-26 23:45:34,378 EPOCH 2576
2020-02-26 23:45:44,531 Epoch 2576: total training loss 0.00059
2020-02-26 23:45:44,532 EPOCH 2577
2020-02-26 23:45:50,463 Epoch 2577 Step:   286000 Batch Loss:     0.000006 Tokens per Sec: 10053504, Lr: 0.000490
2020-02-26 23:45:54,753 Epoch 2577: total training loss 0.00062
2020-02-26 23:45:54,753 EPOCH 2578
2020-02-26 23:46:04,847 Epoch 2578: total training loss 0.00059
2020-02-26 23:46:04,847 EPOCH 2579
2020-02-26 23:46:13,099 Epoch 2579 Step:   286250 Batch Loss:     0.000008 Tokens per Sec: 10419771, Lr: 0.000490
2020-02-26 23:46:14,801 Epoch 2579: total training loss 0.00058
2020-02-26 23:46:14,801 EPOCH 2580
2020-02-26 23:46:24,813 Epoch 2580: total training loss 0.00060
2020-02-26 23:46:24,814 EPOCH 2581
2020-02-26 23:46:34,744 Epoch 2581: total training loss 0.00061
2020-02-26 23:46:34,745 EPOCH 2582
2020-02-26 23:46:35,607 Epoch 2582 Step:   286500 Batch Loss:     0.000006 Tokens per Sec: 10172564, Lr: 0.000490
2020-02-26 23:46:44,695 Epoch 2582: total training loss 0.00059
2020-02-26 23:46:44,695 EPOCH 2583
2020-02-26 23:46:54,719 Epoch 2583: total training loss 0.00061
2020-02-26 23:46:54,720 EPOCH 2584
2020-02-26 23:46:58,105 Epoch 2584 Step:   286750 Batch Loss:     0.000006 Tokens per Sec: 10357600, Lr: 0.000490
2020-02-26 23:47:04,635 Epoch 2584: total training loss 0.00061
2020-02-26 23:47:04,635 EPOCH 2585
2020-02-26 23:47:14,652 Epoch 2585: total training loss 0.00058
2020-02-26 23:47:14,652 EPOCH 2586
2020-02-26 23:47:20,850 Epoch 2586 Step:   287000 Batch Loss:     0.000008 Tokens per Sec: 10081249, Lr: 0.000490
2020-02-26 23:47:25,100 Epoch 2586: total training loss 0.00059
2020-02-26 23:47:25,101 EPOCH 2587
2020-02-26 23:47:35,859 Epoch 2587: total training loss 0.00060
2020-02-26 23:47:35,859 EPOCH 2588
2020-02-26 23:47:44,558 Epoch 2588 Step:   287250 Batch Loss:     0.000006 Tokens per Sec:  9838915, Lr: 0.000490
2020-02-26 23:47:46,339 Epoch 2588: total training loss 0.00059
2020-02-26 23:47:46,340 EPOCH 2589
2020-02-26 23:47:56,787 Epoch 2589: total training loss 0.00057
2020-02-26 23:47:56,788 EPOCH 2590
2020-02-26 23:48:07,185 Epoch 2590: total training loss 0.00058
2020-02-26 23:48:07,186 EPOCH 2591
2020-02-26 23:48:08,145 Epoch 2591 Step:   287500 Batch Loss:     0.000007 Tokens per Sec:  9235484, Lr: 0.000490
2020-02-26 23:48:17,329 Epoch 2591: total training loss 0.00058
2020-02-26 23:48:17,330 EPOCH 2592
2020-02-26 23:48:27,394 Epoch 2592: total training loss 0.00058
2020-02-26 23:48:27,395 EPOCH 2593
2020-02-26 23:48:30,939 Epoch 2593 Step:   287750 Batch Loss:     0.000005 Tokens per Sec: 10106466, Lr: 0.000490
2020-02-26 23:48:37,494 Epoch 2593: total training loss 0.00059
2020-02-26 23:48:37,494 EPOCH 2594
2020-02-26 23:48:47,632 Epoch 2594: total training loss 0.00061
2020-02-26 23:48:47,633 EPOCH 2595
2020-02-26 23:48:53,729 Epoch 2595 Step:   288000 Batch Loss:     0.000008 Tokens per Sec: 10276439, Lr: 0.000490
2020-02-26 23:48:57,588 Epoch 2595: total training loss 0.00064
2020-02-26 23:48:57,589 EPOCH 2596
2020-02-26 23:49:07,607 Epoch 2596: total training loss 0.00060
2020-02-26 23:49:07,607 EPOCH 2597
2020-02-26 23:49:15,982 Epoch 2597 Step:   288250 Batch Loss:     0.000005 Tokens per Sec: 10256637, Lr: 0.000490
2020-02-26 23:49:17,627 Epoch 2597: total training loss 0.00058
2020-02-26 23:49:17,628 EPOCH 2598
2020-02-26 23:49:27,628 Epoch 2598: total training loss 0.00057
2020-02-26 23:49:27,629 EPOCH 2599
2020-02-26 23:49:37,529 Epoch 2599: total training loss 0.00058
2020-02-26 23:49:37,530 EPOCH 2600
2020-02-26 23:49:38,515 Epoch 2600 Step:   288500 Batch Loss:     0.000004 Tokens per Sec: 10549544, Lr: 0.000490
2020-02-26 23:49:47,519 Epoch 2600: total training loss 0.00057
2020-02-26 23:49:47,520 EPOCH 2601
2020-02-26 23:49:57,570 Epoch 2601: total training loss 0.00058
2020-02-26 23:49:57,570 EPOCH 2602
2020-02-26 23:50:01,083 Epoch 2602 Step:   288750 Batch Loss:     0.000004 Tokens per Sec: 10273113, Lr: 0.000490
2020-02-26 23:50:07,608 Epoch 2602: total training loss 0.00058
2020-02-26 23:50:07,609 EPOCH 2603
2020-02-26 23:50:17,879 Epoch 2603: total training loss 0.00059
2020-02-26 23:50:17,879 EPOCH 2604
2020-02-26 23:50:24,153 Epoch 2604 Step:   289000 Batch Loss:     0.000008 Tokens per Sec:  9888514, Lr: 0.000490
2020-02-26 23:50:28,337 Epoch 2604: total training loss 0.00059
2020-02-26 23:50:28,338 EPOCH 2605
2020-02-26 23:50:38,888 Epoch 2605: total training loss 0.00059
2020-02-26 23:50:38,889 EPOCH 2606
2020-02-26 23:50:47,987 Epoch 2606 Step:   289250 Batch Loss:     0.000007 Tokens per Sec:  9709346, Lr: 0.000490
2020-02-26 23:50:49,520 Epoch 2606: total training loss 0.00058
2020-02-26 23:50:49,520 EPOCH 2607
2020-02-26 23:50:59,954 Epoch 2607: total training loss 0.00059
2020-02-26 23:50:59,955 EPOCH 2608
2020-02-26 23:51:10,097 Epoch 2608: total training loss 0.00059
2020-02-26 23:51:10,097 EPOCH 2609
2020-02-26 23:51:11,324 Epoch 2609 Step:   289500 Batch Loss:     0.000006 Tokens per Sec:  9361799, Lr: 0.000490
2020-02-26 23:51:20,355 Epoch 2609: total training loss 0.00058
2020-02-26 23:51:20,356 EPOCH 2610
2020-02-26 23:51:30,548 Epoch 2610: total training loss 0.00062
2020-02-26 23:51:30,548 EPOCH 2611
2020-02-26 23:51:34,055 Epoch 2611 Step:   289750 Batch Loss:     0.000004 Tokens per Sec:  9968158, Lr: 0.000490
2020-02-26 23:51:40,665 Epoch 2611: total training loss 0.00065
2020-02-26 23:51:40,665 EPOCH 2612
2020-02-26 23:51:50,668 Epoch 2612: total training loss 0.00062
2020-02-26 23:51:50,668 EPOCH 2613
2020-02-26 23:51:56,643 Epoch 2613 Step:   290000 Batch Loss:     0.000007 Tokens per Sec: 10193563, Lr: 0.000490
2020-02-26 23:51:56,643 Model noise rate: 5
2020-02-26 23:52:58,032 Validation result at epoch 2613, step   290000: Val DTW Score:  10.71, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0418, GT DTW Score:      nan, duration: 61.3884s
2020-02-26 23:53:02,432 Epoch 2613: total training loss 0.00059
2020-02-26 23:53:02,432 EPOCH 2614
2020-02-26 23:53:13,004 Epoch 2614: total training loss 0.00059
2020-02-26 23:53:13,004 EPOCH 2615
2020-02-26 23:53:21,943 Epoch 2615 Step:   290250 Batch Loss:     0.000005 Tokens per Sec:  9921233, Lr: 0.000490
2020-02-26 23:53:23,300 Epoch 2615: total training loss 0.00059
2020-02-26 23:53:23,301 EPOCH 2616
2020-02-26 23:53:33,513 Epoch 2616: total training loss 0.00061
2020-02-26 23:53:33,513 EPOCH 2617
2020-02-26 23:53:43,582 Epoch 2617: total training loss 0.00060
2020-02-26 23:53:43,583 EPOCH 2618
2020-02-26 23:53:44,772 Epoch 2618 Step:   290500 Batch Loss:     0.000004 Tokens per Sec: 10190972, Lr: 0.000490
2020-02-26 23:53:53,512 Epoch 2618: total training loss 0.00059
2020-02-26 23:53:53,513 EPOCH 2619
2020-02-26 23:54:03,411 Epoch 2619: total training loss 0.00058
2020-02-26 23:54:03,412 EPOCH 2620
2020-02-26 23:54:06,931 Epoch 2620 Step:   290750 Batch Loss:     0.000006 Tokens per Sec: 10355257, Lr: 0.000490
2020-02-26 23:54:13,303 Epoch 2620: total training loss 0.00058
2020-02-26 23:54:13,303 EPOCH 2621
2020-02-26 23:54:23,340 Epoch 2621: total training loss 0.00058
2020-02-26 23:54:23,340 EPOCH 2622
2020-02-26 23:54:29,635 Epoch 2622 Step:   291000 Batch Loss:     0.000004 Tokens per Sec: 10353067, Lr: 0.000490
2020-02-26 23:54:33,311 Epoch 2622: total training loss 0.00057
2020-02-26 23:54:33,312 EPOCH 2623
2020-02-26 23:54:43,374 Epoch 2623: total training loss 0.00058
2020-02-26 23:54:43,375 EPOCH 2624
2020-02-26 23:54:52,041 Epoch 2624 Step:   291250 Batch Loss:     0.000004 Tokens per Sec: 10329166, Lr: 0.000490
2020-02-26 23:54:53,227 Epoch 2624: total training loss 0.00059
2020-02-26 23:54:53,227 EPOCH 2625
2020-02-26 23:55:03,169 Epoch 2625: total training loss 0.00058
2020-02-26 23:55:03,169 EPOCH 2626
2020-02-26 23:55:13,134 Epoch 2626: total training loss 0.00057
2020-02-26 23:55:13,135 EPOCH 2627
2020-02-26 23:55:14,327 Epoch 2627 Step:   291500 Batch Loss:     0.000006 Tokens per Sec: 10323048, Lr: 0.000490
2020-02-26 23:55:23,136 Epoch 2627: total training loss 0.00057
2020-02-26 23:55:23,225 EPOCH 2628
2020-02-26 23:55:33,450 Epoch 2628: total training loss 0.00058
2020-02-26 23:55:33,451 EPOCH 2629
2020-02-26 23:55:37,719 Epoch 2629 Step:   291750 Batch Loss:     0.000008 Tokens per Sec:  9847745, Lr: 0.000490
2020-02-26 23:55:43,872 Epoch 2629: total training loss 0.00058
2020-02-26 23:55:43,872 EPOCH 2630
2020-02-26 23:55:54,365 Epoch 2630: total training loss 0.00058
2020-02-26 23:55:54,366 EPOCH 2631
2020-02-26 23:56:00,922 Epoch 2631 Step:   292000 Batch Loss:     0.000004 Tokens per Sec:  9745566, Lr: 0.000490
2020-02-26 23:56:05,002 Epoch 2631: total training loss 0.00059
2020-02-26 23:56:05,003 EPOCH 2632
2020-02-26 23:56:15,188 Epoch 2632: total training loss 0.00058
2020-02-26 23:56:15,189 EPOCH 2633
2020-02-26 23:56:24,272 Epoch 2633 Step:   292250 Batch Loss:     0.000006 Tokens per Sec: 10075456, Lr: 0.000490
2020-02-26 23:56:25,439 Epoch 2633: total training loss 0.00059
2020-02-26 23:56:25,439 EPOCH 2634
2020-02-26 23:56:35,528 Epoch 2634: total training loss 0.00062
2020-02-26 23:56:35,528 EPOCH 2635
2020-02-26 23:56:45,655 Epoch 2635: total training loss 0.00073
2020-02-26 23:56:45,656 EPOCH 2636
2020-02-26 23:56:47,020 Epoch 2636 Step:   292500 Batch Loss:     0.000005 Tokens per Sec: 10202140, Lr: 0.000490
2020-02-26 23:56:55,652 Epoch 2636: total training loss 0.00065
2020-02-26 23:56:55,652 EPOCH 2637
2020-02-26 23:57:05,644 Epoch 2637: total training loss 0.00060
2020-02-26 23:57:05,644 EPOCH 2638
2020-02-26 23:57:09,481 Epoch 2638 Step:   292750 Batch Loss:     0.000004 Tokens per Sec: 10446392, Lr: 0.000490
2020-02-26 23:57:15,532 Epoch 2638: total training loss 0.00059
2020-02-26 23:57:15,533 EPOCH 2639
2020-02-26 23:57:25,431 Epoch 2639: total training loss 0.00058
2020-02-26 23:57:25,431 EPOCH 2640
2020-02-26 23:57:31,896 Epoch 2640 Step:   293000 Batch Loss:     0.000007 Tokens per Sec: 10206753, Lr: 0.000490
2020-02-26 23:57:35,404 Epoch 2640: total training loss 0.00058
2020-02-26 23:57:35,404 EPOCH 2641
2020-02-26 23:57:45,440 Epoch 2641: total training loss 0.00058
2020-02-26 23:57:45,440 EPOCH 2642
2020-02-26 23:57:54,327 Epoch 2642 Step:   293250 Batch Loss:     0.000006 Tokens per Sec: 10228561, Lr: 0.000490
2020-02-26 23:57:55,445 Epoch 2642: total training loss 0.00058
2020-02-26 23:57:55,445 EPOCH 2643
2020-02-26 23:58:05,810 Epoch 2643: total training loss 0.00058
2020-02-26 23:58:05,811 EPOCH 2644
2020-02-26 23:58:16,237 Epoch 2644: total training loss 0.00058
2020-02-26 23:58:16,238 EPOCH 2645
2020-02-26 23:58:17,629 Epoch 2645 Step:   293500 Batch Loss:     0.000005 Tokens per Sec:  9054858, Lr: 0.000490
2020-02-26 23:58:26,606 Epoch 2645: total training loss 0.00058
2020-02-26 23:58:26,607 EPOCH 2646
2020-02-26 23:58:36,964 Epoch 2646: total training loss 0.00058
2020-02-26 23:58:36,965 EPOCH 2647
2020-02-26 23:58:41,109 Epoch 2647 Step:   293750 Batch Loss:     0.000006 Tokens per Sec:  9774808, Lr: 0.000490
2020-02-26 23:58:47,375 Epoch 2647: total training loss 0.00058
2020-02-26 23:58:47,375 EPOCH 2648
2020-02-26 23:58:57,558 Epoch 2648: total training loss 0.00063
2020-02-26 23:58:57,558 EPOCH 2649
2020-02-26 23:59:04,062 Epoch 2649 Step:   294000 Batch Loss:     0.000004 Tokens per Sec: 10064027, Lr: 0.000490
2020-02-26 23:59:07,725 Epoch 2649: total training loss 0.00063
2020-02-26 23:59:07,726 EPOCH 2650
2020-02-26 23:59:17,965 Epoch 2650: total training loss 0.00058
2020-02-26 23:59:17,967 EPOCH 2651
2020-02-26 23:59:27,115 Epoch 2651 Step:   294250 Batch Loss:     0.000005 Tokens per Sec: 10138031, Lr: 0.000490
2020-02-26 23:59:28,107 Epoch 2651: total training loss 0.00057
2020-02-26 23:59:28,107 EPOCH 2652
2020-02-26 23:59:38,130 Epoch 2652: total training loss 0.00058
2020-02-26 23:59:38,131 EPOCH 2653
2020-02-26 23:59:48,126 Epoch 2653: total training loss 0.00058
2020-02-26 23:59:48,127 EPOCH 2654
2020-02-26 23:59:49,587 Epoch 2654 Step:   294500 Batch Loss:     0.000006 Tokens per Sec:  9837126, Lr: 0.000490
2020-02-26 23:59:58,070 Epoch 2654: total training loss 0.00057
2020-02-26 23:59:58,070 EPOCH 2655
2020-02-27 00:00:08,072 Epoch 2655: total training loss 0.00057
2020-02-27 00:00:08,073 EPOCH 2656
2020-02-27 00:00:12,163 Epoch 2656 Step:   294750 Batch Loss:     0.000004 Tokens per Sec: 10345372, Lr: 0.000490
2020-02-27 00:00:17,936 Epoch 2656: total training loss 0.00058
2020-02-27 00:00:17,936 EPOCH 2657
2020-02-27 00:00:27,979 Epoch 2657: total training loss 0.00057
2020-02-27 00:00:27,980 EPOCH 2658
2020-02-27 00:00:34,715 Epoch 2658 Step:   295000 Batch Loss:     0.000005 Tokens per Sec: 10469755, Lr: 0.000490
2020-02-27 00:00:37,807 Epoch 2658: total training loss 0.00058
2020-02-27 00:00:37,808 EPOCH 2659
2020-02-27 00:00:47,883 Epoch 2659: total training loss 0.00057
2020-02-27 00:00:47,884 EPOCH 2660
2020-02-27 00:00:57,250 Epoch 2660 Step:   295250 Batch Loss:     0.000004 Tokens per Sec:  9980111, Lr: 0.000490
2020-02-27 00:00:58,168 Epoch 2660: total training loss 0.00060
2020-02-27 00:00:58,169 EPOCH 2661
2020-02-27 00:01:08,628 Epoch 2661: total training loss 0.00063
2020-02-27 00:01:08,629 EPOCH 2662
2020-02-27 00:01:19,245 Epoch 2662: total training loss 0.00058
2020-02-27 00:01:19,245 EPOCH 2663
2020-02-27 00:01:20,907 Epoch 2663 Step:   295500 Batch Loss:     0.000003 Tokens per Sec:  9259807, Lr: 0.000490
2020-02-27 00:01:29,885 Epoch 2663: total training loss 0.00058
2020-02-27 00:01:29,886 EPOCH 2664
2020-02-27 00:01:40,084 Epoch 2664: total training loss 0.00058
2020-02-27 00:01:40,084 EPOCH 2665
2020-02-27 00:01:44,216 Epoch 2665 Step:   295750 Batch Loss:     0.000008 Tokens per Sec:  9950469, Lr: 0.000490
2020-02-27 00:01:50,252 Epoch 2665: total training loss 0.00058
2020-02-27 00:01:50,253 EPOCH 2666
2020-02-27 00:02:00,334 Epoch 2666: total training loss 0.00058
2020-02-27 00:02:00,334 EPOCH 2667
2020-02-27 00:02:06,889 Epoch 2667 Step:   296000 Batch Loss:     0.000006 Tokens per Sec: 10336155, Lr: 0.000490
2020-02-27 00:02:10,331 Epoch 2667: total training loss 0.00058
2020-02-27 00:02:10,331 EPOCH 2668
2020-02-27 00:02:20,226 Epoch 2668: total training loss 0.00058
2020-02-27 00:02:20,227 EPOCH 2669
2020-02-27 00:02:29,469 Epoch 2669 Step:   296250 Batch Loss:     0.000003 Tokens per Sec: 10247158, Lr: 0.000490
2020-02-27 00:02:30,278 Epoch 2669: total training loss 0.00058
2020-02-27 00:02:30,278 EPOCH 2670
2020-02-27 00:02:40,276 Epoch 2670: total training loss 0.00058
2020-02-27 00:02:40,277 EPOCH 2671
2020-02-27 00:02:50,157 Epoch 2671: total training loss 0.00058
2020-02-27 00:02:50,157 EPOCH 2672
2020-02-27 00:02:51,886 Epoch 2672 Step:   296500 Batch Loss:     0.000006 Tokens per Sec: 10560852, Lr: 0.000490
2020-02-27 00:02:59,960 Epoch 2672: total training loss 0.00058
2020-02-27 00:02:59,960 EPOCH 2673
2020-02-27 00:03:09,843 Epoch 2673: total training loss 0.00058
2020-02-27 00:03:09,844 EPOCH 2674
2020-02-27 00:03:14,287 Epoch 2674 Step:   296750 Batch Loss:     0.000006 Tokens per Sec: 10550899, Lr: 0.000490
2020-02-27 00:03:19,655 Epoch 2674: total training loss 0.00058
2020-02-27 00:03:19,656 EPOCH 2675
2020-02-27 00:03:29,576 Epoch 2675: total training loss 0.00058
2020-02-27 00:03:29,577 EPOCH 2676
2020-02-27 00:03:36,342 Epoch 2676 Step:   297000 Batch Loss:     0.000005 Tokens per Sec: 10002670, Lr: 0.000490
2020-02-27 00:03:39,725 Epoch 2676: total training loss 0.00059
2020-02-27 00:03:39,725 EPOCH 2677
2020-02-27 00:03:50,035 Epoch 2677: total training loss 0.00059
2020-02-27 00:03:50,036 EPOCH 2678
2020-02-27 00:03:59,667 Epoch 2678 Step:   297250 Batch Loss:     0.000004 Tokens per Sec:  9810468, Lr: 0.000490
2020-02-27 00:04:00,425 Epoch 2678: total training loss 0.00059
2020-02-27 00:04:00,426 EPOCH 2679
2020-02-27 00:04:10,822 Epoch 2679: total training loss 0.00057
2020-02-27 00:04:10,823 EPOCH 2680
2020-02-27 00:04:20,830 Epoch 2680: total training loss 0.00058
2020-02-27 00:04:20,833 EPOCH 2681
2020-02-27 00:04:22,705 Epoch 2681 Step:   297500 Batch Loss:     0.000004 Tokens per Sec: 10313579, Lr: 0.000490
2020-02-27 00:04:30,825 Epoch 2681: total training loss 0.00058
2020-02-27 00:04:30,826 EPOCH 2682
2020-02-27 00:04:40,891 Epoch 2682: total training loss 0.00063
2020-02-27 00:04:40,892 EPOCH 2683
2020-02-27 00:04:45,214 Epoch 2683 Step:   297750 Batch Loss:     0.000004 Tokens per Sec: 10569587, Lr: 0.000490
2020-02-27 00:04:50,818 Epoch 2683: total training loss 0.00060
2020-02-27 00:04:50,818 EPOCH 2684
2020-02-27 00:05:00,715 Epoch 2684: total training loss 0.00059
2020-02-27 00:05:00,716 EPOCH 2685
2020-02-27 00:05:07,462 Epoch 2685 Step:   298000 Batch Loss:     0.000004 Tokens per Sec: 10211011, Lr: 0.000490
2020-02-27 00:05:10,696 Epoch 2685: total training loss 0.00058
2020-02-27 00:05:10,697 EPOCH 2686
2020-02-27 00:05:20,692 Epoch 2686: total training loss 0.00057
2020-02-27 00:05:20,692 EPOCH 2687
2020-02-27 00:05:30,071 Epoch 2687 Step:   298250 Batch Loss:     0.000004 Tokens per Sec: 10374619, Lr: 0.000490
2020-02-27 00:05:30,646 Epoch 2687: total training loss 0.00057
2020-02-27 00:05:30,647 EPOCH 2688
2020-02-27 00:05:40,652 Epoch 2688: total training loss 0.00059
2020-02-27 00:05:40,653 EPOCH 2689
2020-02-27 00:05:50,441 Epoch 2689: total training loss 0.00058
2020-02-27 00:05:50,441 EPOCH 2690
2020-02-27 00:05:52,465 Epoch 2690 Step:   298500 Batch Loss:     0.000007 Tokens per Sec: 10744032, Lr: 0.000490
2020-02-27 00:06:00,244 Epoch 2690: total training loss 0.00059
2020-02-27 00:06:00,244 EPOCH 2691
2020-02-27 00:06:10,164 Epoch 2691: total training loss 0.00060
2020-02-27 00:06:10,166 EPOCH 2692
2020-02-27 00:06:14,599 Epoch 2692 Step:   298750 Batch Loss:     0.000008 Tokens per Sec: 10131299, Lr: 0.000490
2020-02-27 00:06:20,284 Epoch 2692: total training loss 0.00065
2020-02-27 00:06:20,284 EPOCH 2693
2020-02-27 00:06:30,335 Epoch 2693: total training loss 0.00068
2020-02-27 00:06:30,336 EPOCH 2694
2020-02-27 00:06:37,928 Epoch 2694 Step:   299000 Batch Loss:     0.000004 Tokens per Sec:  9597125, Lr: 0.000490
2020-02-27 00:06:41,012 Epoch 2694: total training loss 0.00059
2020-02-27 00:06:41,012 EPOCH 2695
2020-02-27 00:06:51,609 Epoch 2695: total training loss 0.00057
2020-02-27 00:06:51,609 EPOCH 2696
2020-02-27 00:07:01,540 Epoch 2696 Step:   299250 Batch Loss:     0.000005 Tokens per Sec:  9731073, Lr: 0.000490
2020-02-27 00:07:02,100 Epoch 2696: total training loss 0.00057
2020-02-27 00:07:02,100 EPOCH 2697
2020-02-27 00:07:12,440 Epoch 2697: total training loss 0.00057
2020-02-27 00:07:12,440 EPOCH 2698
2020-02-27 00:07:22,762 Epoch 2698: total training loss 0.00057
2020-02-27 00:07:22,763 EPOCH 2699
2020-02-27 00:07:24,712 Epoch 2699 Step:   299500 Batch Loss:     0.000004 Tokens per Sec: 10229675, Lr: 0.000490
2020-02-27 00:07:32,803 Epoch 2699: total training loss 0.00058
2020-02-27 00:07:32,803 EPOCH 2700
2020-02-27 00:07:43,093 Epoch 2700: total training loss 0.00057
2020-02-27 00:07:43,094 EPOCH 2701
2020-02-27 00:07:47,618 Epoch 2701 Step:   299750 Batch Loss:     0.000010 Tokens per Sec: 10190062, Lr: 0.000490
2020-02-27 00:07:53,291 Epoch 2701: total training loss 0.00058
2020-02-27 00:07:53,292 EPOCH 2702
2020-02-27 00:08:03,370 Epoch 2702: total training loss 0.00060
2020-02-27 00:08:03,370 EPOCH 2703
2020-02-27 00:08:10,382 Epoch 2703 Step:   300000 Batch Loss:     0.000004 Tokens per Sec: 10329887, Lr: 0.000490
2020-02-27 00:08:10,383 Model noise rate: 5
2020-02-27 00:09:11,959 Validation result at epoch 2703, step   300000: Val DTW Score:  10.73, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0420, GT DTW Score:      nan, duration: 61.5754s
2020-02-27 00:09:14,975 Epoch 2703: total training loss 0.00060
2020-02-27 00:09:14,976 EPOCH 2704
2020-02-27 00:09:25,463 Epoch 2704: total training loss 0.00062
2020-02-27 00:09:25,464 EPOCH 2705
2020-02-27 00:09:35,616 Epoch 2705 Step:   300250 Batch Loss:     0.000007 Tokens per Sec:  9619295, Lr: 0.000490
2020-02-27 00:09:36,052 Epoch 2705: total training loss 0.00059
2020-02-27 00:09:36,052 EPOCH 2706
2020-02-27 00:09:46,615 Epoch 2706: total training loss 0.00058
2020-02-27 00:09:46,616 EPOCH 2707
2020-02-27 00:09:57,331 Epoch 2707: total training loss 0.00057
2020-02-27 00:09:57,332 EPOCH 2708
2020-02-27 00:09:59,408 Epoch 2708 Step:   300500 Batch Loss:     0.000007 Tokens per Sec:  9722227, Lr: 0.000490
2020-02-27 00:10:07,570 Epoch 2708: total training loss 0.00057
2020-02-27 00:10:07,571 EPOCH 2709
2020-02-27 00:10:17,507 Epoch 2709: total training loss 0.00057
2020-02-27 00:10:17,508 EPOCH 2710
2020-02-27 00:10:22,255 Epoch 2710 Step:   300750 Batch Loss:     0.000006 Tokens per Sec: 10153262, Lr: 0.000490
2020-02-27 00:10:27,614 Epoch 2710: total training loss 0.00060
2020-02-27 00:10:27,615 EPOCH 2711
2020-02-27 00:10:37,744 Epoch 2711: total training loss 0.00058
2020-02-27 00:10:37,744 EPOCH 2712
2020-02-27 00:10:44,958 Epoch 2712 Step:   301000 Batch Loss:     0.000003 Tokens per Sec: 10282293, Lr: 0.000490
2020-02-27 00:10:47,813 Epoch 2712: total training loss 0.00059
2020-02-27 00:10:47,813 EPOCH 2713
2020-02-27 00:10:57,784 Epoch 2713: total training loss 0.00060
2020-02-27 00:10:57,785 EPOCH 2714
2020-02-27 00:11:07,400 Epoch 2714 Step:   301250 Batch Loss:     0.000010 Tokens per Sec: 10336555, Lr: 0.000490
2020-02-27 00:11:07,751 Epoch 2714: total training loss 0.00063
2020-02-27 00:11:07,751 EPOCH 2715
2020-02-27 00:11:17,844 Epoch 2715: total training loss 0.00060
2020-02-27 00:11:17,845 EPOCH 2716
2020-02-27 00:11:27,868 Epoch 2716: total training loss 0.00058
2020-02-27 00:11:27,869 EPOCH 2717
2020-02-27 00:11:30,096 Epoch 2717 Step:   301500 Batch Loss:     0.000006 Tokens per Sec: 10442997, Lr: 0.000490
2020-02-27 00:11:37,867 Epoch 2717: total training loss 0.00057
2020-02-27 00:11:37,868 EPOCH 2718
2020-02-27 00:11:47,838 Epoch 2718: total training loss 0.00057
2020-02-27 00:11:47,839 EPOCH 2719
2020-02-27 00:11:52,475 Epoch 2719 Step:   301750 Batch Loss:     0.000004 Tokens per Sec: 10450023, Lr: 0.000490
2020-02-27 00:11:57,688 Epoch 2719: total training loss 0.00058
2020-02-27 00:11:57,689 EPOCH 2720
2020-02-27 00:12:07,850 Epoch 2720: total training loss 0.00057
2020-02-27 00:12:07,852 EPOCH 2721
2020-02-27 00:12:15,192 Epoch 2721 Step:   302000 Batch Loss:     0.000004 Tokens per Sec: 10008150, Lr: 0.000490
2020-02-27 00:12:18,090 Epoch 2721: total training loss 0.00057
2020-02-27 00:12:18,090 EPOCH 2722
2020-02-27 00:12:28,303 Epoch 2722: total training loss 0.00065
2020-02-27 00:12:28,303 EPOCH 2723
2020-02-27 00:12:38,327 Epoch 2723 Step:   302250 Batch Loss:     0.000006 Tokens per Sec:  9959853, Lr: 0.000490
2020-02-27 00:12:38,619 Epoch 2723: total training loss 0.00068
2020-02-27 00:12:38,619 EPOCH 2724
2020-02-27 00:12:49,047 Epoch 2724: total training loss 0.00060
2020-02-27 00:12:49,049 EPOCH 2725
2020-02-27 00:12:59,396 Epoch 2725: total training loss 0.00060
2020-02-27 00:12:59,396 EPOCH 2726
2020-02-27 00:13:01,635 Epoch 2726 Step:   302500 Batch Loss:     0.000007 Tokens per Sec: 10365356, Lr: 0.000490
2020-02-27 00:13:09,380 Epoch 2726: total training loss 0.00058
2020-02-27 00:13:09,381 EPOCH 2727
2020-02-27 00:13:19,387 Epoch 2727: total training loss 0.00057
2020-02-27 00:13:19,388 EPOCH 2728
2020-02-27 00:13:24,105 Epoch 2728 Step:   302750 Batch Loss:     0.000005 Tokens per Sec: 10341855, Lr: 0.000490
2020-02-27 00:13:29,241 Epoch 2728: total training loss 0.00057
2020-02-27 00:13:29,241 EPOCH 2729
2020-02-27 00:13:39,090 Epoch 2729: total training loss 0.00057
2020-02-27 00:13:39,091 EPOCH 2730
2020-02-27 00:13:46,548 Epoch 2730 Step:   303000 Batch Loss:     0.000007 Tokens per Sec: 10170579, Lr: 0.000490
2020-02-27 00:13:49,134 Epoch 2730: total training loss 0.00057
2020-02-27 00:13:49,138 EPOCH 2731
2020-02-27 00:13:59,102 Epoch 2731: total training loss 0.00057
2020-02-27 00:13:59,104 EPOCH 2732
2020-02-27 00:14:08,980 Epoch 2732 Step:   303250 Batch Loss:     0.000004 Tokens per Sec: 10270033, Lr: 0.000490
2020-02-27 00:14:09,132 Epoch 2732: total training loss 0.00056
2020-02-27 00:14:09,132 EPOCH 2733
2020-02-27 00:14:19,059 Epoch 2733: total training loss 0.00057
2020-02-27 00:14:19,059 EPOCH 2734
2020-02-27 00:14:29,014 Epoch 2734: total training loss 0.00058
2020-02-27 00:14:29,015 EPOCH 2735
2020-02-27 00:14:31,234 Epoch 2735 Step:   303500 Batch Loss:     0.000004 Tokens per Sec: 10125724, Lr: 0.000490
2020-02-27 00:14:38,973 Epoch 2735: total training loss 0.00061
2020-02-27 00:14:38,974 EPOCH 2736
2020-02-27 00:14:48,959 Epoch 2736: total training loss 0.00059
2020-02-27 00:14:48,960 EPOCH 2737
2020-02-27 00:14:54,005 Epoch 2737 Step:   303750 Batch Loss:     0.000003 Tokens per Sec: 10242472, Lr: 0.000490
2020-02-27 00:14:59,102 Epoch 2737: total training loss 0.00061
2020-02-27 00:14:59,103 EPOCH 2738
2020-02-27 00:15:09,707 Epoch 2738: total training loss 0.00059
2020-02-27 00:15:09,709 EPOCH 2739
2020-02-27 00:15:17,930 Epoch 2739 Step:   304000 Batch Loss:     0.000005 Tokens per Sec:  9086396, Lr: 0.000490
2020-02-27 00:15:20,855 Epoch 2739: total training loss 0.00057
2020-02-27 00:15:20,855 EPOCH 2740
2020-02-27 00:15:31,697 Epoch 2740: total training loss 0.00057
2020-02-27 00:15:31,698 EPOCH 2741
2020-02-27 00:15:42,279 Epoch 2741 Step:   304250 Batch Loss:     0.000005 Tokens per Sec:  9593038, Lr: 0.000490
2020-02-27 00:15:42,390 Epoch 2741: total training loss 0.00058
2020-02-27 00:15:42,391 EPOCH 2742
2020-02-27 00:15:52,764 Epoch 2742: total training loss 0.00057
2020-02-27 00:15:52,765 EPOCH 2743
2020-02-27 00:16:02,663 Epoch 2743: total training loss 0.00058
2020-02-27 00:16:02,663 EPOCH 2744
2020-02-27 00:16:05,243 Epoch 2744 Step:   304500 Batch Loss:     0.000004 Tokens per Sec: 10259146, Lr: 0.000490
2020-02-27 00:16:12,712 Epoch 2744: total training loss 0.00058
2020-02-27 00:16:12,712 EPOCH 2745
2020-02-27 00:16:22,823 Epoch 2745: total training loss 0.00058
2020-02-27 00:16:22,824 EPOCH 2746
2020-02-27 00:16:27,805 Epoch 2746 Step:   304750 Batch Loss:     0.000002 Tokens per Sec: 10007983, Lr: 0.000490
2020-02-27 00:16:32,983 Epoch 2746: total training loss 0.00059
2020-02-27 00:16:32,985 EPOCH 2747
2020-02-27 00:16:43,293 Epoch 2747: total training loss 0.00059
2020-02-27 00:16:43,294 EPOCH 2748
2020-02-27 00:16:51,016 Epoch 2748 Step:   305000 Batch Loss:     0.000006 Tokens per Sec: 10044740, Lr: 0.000490
2020-02-27 00:16:53,591 Epoch 2748: total training loss 0.00057
2020-02-27 00:16:53,592 EPOCH 2749
2020-02-27 00:17:03,788 Epoch 2749: total training loss 0.00057
2020-02-27 00:17:03,788 EPOCH 2750
2020-02-27 00:17:13,913 Epoch 2750 Step:   305250 Batch Loss:     0.000005 Tokens per Sec: 10104768, Lr: 0.000490
2020-02-27 00:17:13,914 Epoch 2750: total training loss 0.00058
2020-02-27 00:17:13,914 EPOCH 2751
2020-02-27 00:17:24,161 Epoch 2751: total training loss 0.00067
2020-02-27 00:17:24,162 EPOCH 2752
2020-02-27 00:17:34,382 Epoch 2752: total training loss 0.00060
2020-02-27 00:17:34,384 EPOCH 2753
2020-02-27 00:17:37,058 Epoch 2753 Step:   305500 Batch Loss:     0.000005 Tokens per Sec:  9857592, Lr: 0.000490
2020-02-27 00:17:45,070 Epoch 2753: total training loss 0.00059
2020-02-27 00:17:45,071 EPOCH 2754
2020-02-27 00:17:55,663 Epoch 2754: total training loss 0.00059
2020-02-27 00:17:55,664 EPOCH 2755
2020-02-27 00:18:01,147 Epoch 2755 Step:   305750 Batch Loss:     0.000003 Tokens per Sec:  9671762, Lr: 0.000490
2020-02-27 00:18:06,156 Epoch 2755: total training loss 0.00060
2020-02-27 00:18:06,156 EPOCH 2756
2020-02-27 00:18:16,780 Epoch 2756: total training loss 0.00058
2020-02-27 00:18:16,782 EPOCH 2757
2020-02-27 00:18:24,916 Epoch 2757 Step:   306000 Batch Loss:     0.000007 Tokens per Sec:  9556900, Lr: 0.000490
2020-02-27 00:18:27,516 Epoch 2757: total training loss 0.00057
2020-02-27 00:18:27,517 EPOCH 2758
2020-02-27 00:18:37,787 Epoch 2758: total training loss 0.00057
2020-02-27 00:18:37,787 EPOCH 2759
2020-02-27 00:18:48,160 Epoch 2759: total training loss 0.00057
2020-02-27 00:18:48,161 EPOCH 2760
2020-02-27 00:18:48,290 Epoch 2760 Step:   306250 Batch Loss:     0.000006 Tokens per Sec:  8566231, Lr: 0.000490
2020-02-27 00:18:58,519 Epoch 2760: total training loss 0.00057
2020-02-27 00:18:58,520 EPOCH 2761
2020-02-27 00:19:08,727 Epoch 2761: total training loss 0.00058
2020-02-27 00:19:08,728 EPOCH 2762
2020-02-27 00:19:11,266 Epoch 2762 Step:   306500 Batch Loss:     0.000006 Tokens per Sec: 10056203, Lr: 0.000490
2020-02-27 00:19:18,914 Epoch 2762: total training loss 0.00057
2020-02-27 00:19:18,915 EPOCH 2763
2020-02-27 00:19:28,941 Epoch 2763: total training loss 0.00057
2020-02-27 00:19:28,942 EPOCH 2764
2020-02-27 00:19:34,138 Epoch 2764 Step:   306750 Batch Loss:     0.000007 Tokens per Sec: 10236425, Lr: 0.000490
2020-02-27 00:19:39,022 Epoch 2764: total training loss 0.00057
2020-02-27 00:19:39,023 EPOCH 2765
2020-02-27 00:19:49,027 Epoch 2765: total training loss 0.00058
2020-02-27 00:19:49,028 EPOCH 2766
2020-02-27 00:19:56,748 Epoch 2766 Step:   307000 Batch Loss:     0.000005 Tokens per Sec: 10077584, Lr: 0.000490
2020-02-27 00:19:59,166 Epoch 2766: total training loss 0.00057
2020-02-27 00:19:59,166 EPOCH 2767
2020-02-27 00:20:09,261 Epoch 2767: total training loss 0.00058
2020-02-27 00:20:09,263 EPOCH 2768
2020-02-27 00:20:19,769 Epoch 2768: total training loss 0.00059
2020-02-27 00:20:19,770 EPOCH 2769
2020-02-27 00:20:19,966 Epoch 2769 Step:   307250 Batch Loss:     0.000004 Tokens per Sec:  8879278, Lr: 0.000490
2020-02-27 00:20:30,497 Epoch 2769: total training loss 0.00057
2020-02-27 00:20:30,497 EPOCH 2770
2020-02-27 00:20:41,219 Epoch 2770: total training loss 0.00058
2020-02-27 00:20:41,220 EPOCH 2771
2020-02-27 00:20:44,088 Epoch 2771 Step:   307500 Batch Loss:     0.000005 Tokens per Sec:  9293085, Lr: 0.000490
2020-02-27 00:20:52,021 Epoch 2771: total training loss 0.00058
2020-02-27 00:20:52,022 EPOCH 2772
2020-02-27 00:21:02,474 Epoch 2772: total training loss 0.00058
2020-02-27 00:21:02,474 EPOCH 2773
2020-02-27 00:21:07,918 Epoch 2773 Step:   307750 Batch Loss:     0.000008 Tokens per Sec:  9622911, Lr: 0.000490
2020-02-27 00:21:12,824 Epoch 2773: total training loss 0.00059
2020-02-27 00:21:12,825 EPOCH 2774
2020-02-27 00:21:23,060 Epoch 2774: total training loss 0.00061
2020-02-27 00:21:23,060 EPOCH 2775
2020-02-27 00:21:31,061 Epoch 2775 Step:   308000 Batch Loss:     0.000005 Tokens per Sec: 10033214, Lr: 0.000490
2020-02-27 00:21:33,379 Epoch 2775: total training loss 0.00057
2020-02-27 00:21:33,379 EPOCH 2776
2020-02-27 00:21:43,727 Epoch 2776: total training loss 0.00057
2020-02-27 00:21:43,729 EPOCH 2777
2020-02-27 00:21:54,037 Epoch 2777: total training loss 0.00058
2020-02-27 00:21:54,037 EPOCH 2778
2020-02-27 00:21:54,307 Epoch 2778 Step:   308250 Batch Loss:     0.000005 Tokens per Sec:  9742632, Lr: 0.000490
2020-02-27 00:22:04,321 Epoch 2778: total training loss 0.00057
2020-02-27 00:22:04,322 EPOCH 2779
2020-02-27 00:22:14,552 Epoch 2779: total training loss 0.00057
2020-02-27 00:22:14,553 EPOCH 2780
2020-02-27 00:22:17,553 Epoch 2780 Step:   308500 Batch Loss:     0.000009 Tokens per Sec: 10132098, Lr: 0.000490
2020-02-27 00:22:24,679 Epoch 2780: total training loss 0.00057
2020-02-27 00:22:24,679 EPOCH 2781
2020-02-27 00:22:34,822 Epoch 2781: total training loss 0.00057
2020-02-27 00:22:34,823 EPOCH 2782
2020-02-27 00:22:40,203 Epoch 2782 Step:   308750 Batch Loss:     0.000005 Tokens per Sec: 10199671, Lr: 0.000490
2020-02-27 00:22:44,900 Epoch 2782: total training loss 0.00057
2020-02-27 00:22:44,900 EPOCH 2783
2020-02-27 00:22:55,046 Epoch 2783: total training loss 0.00057
2020-02-27 00:22:55,047 EPOCH 2784
2020-02-27 00:23:03,046 Epoch 2784 Step:   309000 Batch Loss:     0.000003 Tokens per Sec:  9945245, Lr: 0.000490
2020-02-27 00:23:05,395 Epoch 2784: total training loss 0.00066
2020-02-27 00:23:05,395 EPOCH 2785
2020-02-27 00:23:15,746 Epoch 2785: total training loss 0.00058
2020-02-27 00:23:15,746 EPOCH 2786
2020-02-27 00:23:25,947 Epoch 2786: total training loss 0.00059
2020-02-27 00:23:25,949 EPOCH 2787
2020-02-27 00:23:26,368 Epoch 2787 Step:   309250 Batch Loss:     0.000005 Tokens per Sec:  9831260, Lr: 0.000490
2020-02-27 00:23:36,636 Epoch 2787: total training loss 0.00061
2020-02-27 00:23:36,637 EPOCH 2788
2020-02-27 00:23:47,293 Epoch 2788: total training loss 0.00061
2020-02-27 00:23:47,294 EPOCH 2789
2020-02-27 00:23:50,369 Epoch 2789 Step:   309500 Batch Loss:     0.000003 Tokens per Sec:  9917013, Lr: 0.000490
2020-02-27 00:23:57,591 Epoch 2789: total training loss 0.00058
2020-02-27 00:23:57,591 EPOCH 2790
2020-02-27 00:24:07,733 Epoch 2790: total training loss 0.00058
2020-02-27 00:24:07,733 EPOCH 2791
2020-02-27 00:24:13,219 Epoch 2791 Step:   309750 Batch Loss:     0.000006 Tokens per Sec: 10297903, Lr: 0.000490
2020-02-27 00:24:17,751 Epoch 2791: total training loss 0.00056
2020-02-27 00:24:17,752 EPOCH 2792
2020-02-27 00:24:27,809 Epoch 2792: total training loss 0.00057
2020-02-27 00:24:27,832 EPOCH 2793
2020-02-27 00:24:35,684 Epoch 2793 Step:   310000 Batch Loss:     0.000004 Tokens per Sec: 10182559, Lr: 0.000490
2020-02-27 00:24:35,685 Model noise rate: 5
2020-02-27 00:25:39,435 Validation result at epoch 2793, step   310000: Val DTW Score:  10.74, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0424, GT DTW Score:      nan, duration: 63.7504s
2020-02-27 00:25:41,682 Epoch 2793: total training loss 0.00057
2020-02-27 00:25:41,683 EPOCH 2794
2020-02-27 00:25:52,185 Epoch 2794: total training loss 0.00056
2020-02-27 00:25:52,188 EPOCH 2795
2020-02-27 00:26:02,685 Epoch 2795: total training loss 0.00057
2020-02-27 00:26:02,686 EPOCH 2796
2020-02-27 00:26:03,089 Epoch 2796 Step:   310250 Batch Loss:     0.000003 Tokens per Sec:  8413757, Lr: 0.000490
2020-02-27 00:26:13,106 Epoch 2796: total training loss 0.00057
2020-02-27 00:26:13,107 EPOCH 2797
2020-02-27 00:26:23,682 Epoch 2797: total training loss 0.00058
2020-02-27 00:26:23,682 EPOCH 2798
2020-02-27 00:26:26,791 Epoch 2798 Step:   310500 Batch Loss:     0.000005 Tokens per Sec:  9725818, Lr: 0.000490
2020-02-27 00:26:34,057 Epoch 2798: total training loss 0.00058
2020-02-27 00:26:34,058 EPOCH 2799
2020-02-27 00:26:44,268 Epoch 2799: total training loss 0.00058
2020-02-27 00:26:44,270 EPOCH 2800
2020-02-27 00:26:49,983 Epoch 2800 Step:   310750 Batch Loss:     0.000004 Tokens per Sec:  9955857, Lr: 0.000490
2020-02-27 00:26:54,583 Epoch 2800: total training loss 0.00058
2020-02-27 00:26:54,583 EPOCH 2801
2020-02-27 00:27:04,688 Epoch 2801: total training loss 0.00057
2020-02-27 00:27:04,688 EPOCH 2802
2020-02-27 00:27:12,974 Epoch 2802 Step:   311000 Batch Loss:     0.000007 Tokens per Sec:  9906235, Lr: 0.000490
2020-02-27 00:27:15,003 Epoch 2802: total training loss 0.00057
2020-02-27 00:27:15,003 EPOCH 2803
2020-02-27 00:27:25,135 Epoch 2803: total training loss 0.00057
2020-02-27 00:27:25,136 EPOCH 2804
2020-02-27 00:27:35,024 Epoch 2804: total training loss 0.00058
2020-02-27 00:27:35,025 EPOCH 2805
2020-02-27 00:27:35,539 Epoch 2805 Step:   311250 Batch Loss:     0.000004 Tokens per Sec: 10296431, Lr: 0.000490
2020-02-27 00:27:44,846 Epoch 2805: total training loss 0.00057
2020-02-27 00:27:44,847 EPOCH 2806
2020-02-27 00:27:54,948 Epoch 2806: total training loss 0.00057
2020-02-27 00:27:54,949 EPOCH 2807
2020-02-27 00:27:57,953 Epoch 2807 Step:   311500 Batch Loss:     0.000005 Tokens per Sec: 10332993, Lr: 0.000490
2020-02-27 00:28:05,028 Epoch 2807: total training loss 0.00057
2020-02-27 00:28:05,029 EPOCH 2808
2020-02-27 00:28:14,965 Epoch 2808: total training loss 0.00059
2020-02-27 00:28:14,966 EPOCH 2809
2020-02-27 00:28:20,459 Epoch 2809 Step:   311750 Batch Loss:     0.000006 Tokens per Sec: 10454394, Lr: 0.000490
2020-02-27 00:28:24,938 Epoch 2809: total training loss 0.00058
2020-02-27 00:28:24,939 EPOCH 2810
2020-02-27 00:28:36,089 Epoch 2810: total training loss 0.00061
2020-02-27 00:28:36,090 EPOCH 2811
2020-02-27 00:28:45,022 Epoch 2811 Step:   312000 Batch Loss:     0.000009 Tokens per Sec:  9333086, Lr: 0.000490
2020-02-27 00:28:47,079 Epoch 2811: total training loss 0.00059
2020-02-27 00:28:47,079 EPOCH 2812
2020-02-27 00:28:58,260 Epoch 2812: total training loss 0.00061
2020-02-27 00:28:58,261 EPOCH 2813
2020-02-27 00:29:09,070 Epoch 2813: total training loss 0.00058
2020-02-27 00:29:09,071 EPOCH 2814
2020-02-27 00:29:09,768 Epoch 2814 Step:   312250 Batch Loss:     0.000004 Tokens per Sec: 10089501, Lr: 0.000490
2020-02-27 00:29:19,586 Epoch 2814: total training loss 0.00057
2020-02-27 00:29:19,586 EPOCH 2815
2020-02-27 00:29:29,775 Epoch 2815: total training loss 0.00057
2020-02-27 00:29:29,775 EPOCH 2816
2020-02-27 00:29:32,979 Epoch 2816 Step:   312500 Batch Loss:     0.000007 Tokens per Sec:  9885395, Lr: 0.000490
2020-02-27 00:29:40,155 Epoch 2816: total training loss 0.00057
2020-02-27 00:29:40,156 EPOCH 2817
2020-02-27 00:29:50,453 Epoch 2817: total training loss 0.00058
2020-02-27 00:29:50,454 EPOCH 2818
2020-02-27 00:29:56,262 Epoch 2818 Step:   312750 Batch Loss:     0.000004 Tokens per Sec: 10014962, Lr: 0.000490
2020-02-27 00:30:00,720 Epoch 2818: total training loss 0.00057
2020-02-27 00:30:00,721 EPOCH 2819
2020-02-27 00:30:10,731 Epoch 2819: total training loss 0.00058
2020-02-27 00:30:10,731 EPOCH 2820
2020-02-27 00:30:19,274 Epoch 2820 Step:   313000 Batch Loss:     0.000004 Tokens per Sec:  9987173, Lr: 0.000490
2020-02-27 00:30:21,095 Epoch 2820: total training loss 0.00056
2020-02-27 00:30:21,095 EPOCH 2821
2020-02-27 00:30:31,364 Epoch 2821: total training loss 0.00056
2020-02-27 00:30:31,364 EPOCH 2822
2020-02-27 00:30:41,628 Epoch 2822: total training loss 0.00057
2020-02-27 00:30:41,628 EPOCH 2823
2020-02-27 00:30:42,339 Epoch 2823 Step:   313250 Batch Loss:     0.000006 Tokens per Sec:  9610813, Lr: 0.000490
2020-02-27 00:30:51,698 Epoch 2823: total training loss 0.00057
2020-02-27 00:30:51,699 EPOCH 2824
2020-02-27 00:31:01,879 Epoch 2824: total training loss 0.00056
2020-02-27 00:31:01,880 EPOCH 2825
2020-02-27 00:31:05,229 Epoch 2825 Step:   313500 Batch Loss:     0.000006 Tokens per Sec: 10116847, Lr: 0.000490
2020-02-27 00:31:12,217 Epoch 2825: total training loss 0.00057
2020-02-27 00:31:12,218 EPOCH 2826
2020-02-27 00:31:22,462 Epoch 2826: total training loss 0.00062
2020-02-27 00:31:22,462 EPOCH 2827
2020-02-27 00:31:28,418 Epoch 2827 Step:   313750 Batch Loss:     0.000009 Tokens per Sec: 10020271, Lr: 0.000490
2020-02-27 00:31:32,806 Epoch 2827: total training loss 0.00060
2020-02-27 00:31:32,807 EPOCH 2828
2020-02-27 00:31:43,166 Epoch 2828: total training loss 0.00058
2020-02-27 00:31:43,167 EPOCH 2829
2020-02-27 00:31:52,105 Epoch 2829 Step:   314000 Batch Loss:     0.000004 Tokens per Sec:  9610446, Lr: 0.000490
2020-02-27 00:31:53,929 Epoch 2829: total training loss 0.00057
2020-02-27 00:31:53,929 EPOCH 2830
2020-02-27 00:32:04,312 Epoch 2830: total training loss 0.00059
2020-02-27 00:32:04,312 EPOCH 2831
2020-02-27 00:32:14,804 Epoch 2831: total training loss 0.00057
2020-02-27 00:32:14,805 EPOCH 2832
2020-02-27 00:32:15,676 Epoch 2832 Step:   314250 Batch Loss:     0.000004 Tokens per Sec: 10087835, Lr: 0.000490
2020-02-27 00:32:25,154 Epoch 2832: total training loss 0.00057
2020-02-27 00:32:25,154 EPOCH 2833
2020-02-27 00:32:35,671 Epoch 2833: total training loss 0.00056
2020-02-27 00:32:35,671 EPOCH 2834
2020-02-27 00:32:39,086 Epoch 2834 Step:   314500 Batch Loss:     0.000004 Tokens per Sec: 10020778, Lr: 0.000490
2020-02-27 00:32:45,885 Epoch 2834: total training loss 0.00056
2020-02-27 00:32:45,886 EPOCH 2835
2020-02-27 00:32:55,984 Epoch 2835: total training loss 0.00058
2020-02-27 00:32:55,984 EPOCH 2836
2020-02-27 00:33:01,873 Epoch 2836 Step:   314750 Batch Loss:     0.000005 Tokens per Sec: 10193354, Lr: 0.000490
2020-02-27 00:33:06,110 Epoch 2836: total training loss 0.00057
2020-02-27 00:33:06,111 EPOCH 2837
2020-02-27 00:33:16,412 Epoch 2837: total training loss 0.00056
2020-02-27 00:33:16,412 EPOCH 2838
2020-02-27 00:33:24,711 Epoch 2838 Step:   315000 Batch Loss:     0.000004 Tokens per Sec: 10110941, Lr: 0.000490
2020-02-27 00:33:26,499 Epoch 2838: total training loss 0.00056
2020-02-27 00:33:26,500 EPOCH 2839
2020-02-27 00:33:36,596 Epoch 2839: total training loss 0.00057
2020-02-27 00:33:36,596 EPOCH 2840
2020-02-27 00:33:46,849 Epoch 2840: total training loss 0.00059
2020-02-27 00:33:46,851 EPOCH 2841
2020-02-27 00:33:47,853 Epoch 2841 Step:   315250 Batch Loss:     0.000006 Tokens per Sec:  9623339, Lr: 0.000490
2020-02-27 00:33:57,308 Epoch 2841: total training loss 0.00058
2020-02-27 00:33:57,309 EPOCH 2842
2020-02-27 00:34:08,249 Epoch 2842: total training loss 0.00059
2020-02-27 00:34:08,249 EPOCH 2843
2020-02-27 00:34:11,871 Epoch 2843 Step:   315500 Batch Loss:     0.000004 Tokens per Sec: 10111793, Lr: 0.000490
2020-02-27 00:34:18,667 Epoch 2843: total training loss 0.00058
2020-02-27 00:34:18,668 EPOCH 2844
2020-02-27 00:34:29,135 Epoch 2844: total training loss 0.00058
2020-02-27 00:34:29,136 EPOCH 2845
2020-02-27 00:34:35,398 Epoch 2845 Step:   315750 Batch Loss:     0.000006 Tokens per Sec:  9740394, Lr: 0.000490
2020-02-27 00:34:39,584 Epoch 2845: total training loss 0.00056
2020-02-27 00:34:39,585 EPOCH 2846
2020-02-27 00:34:49,912 Epoch 2846: total training loss 0.00057
2020-02-27 00:34:49,928 EPOCH 2847
2020-02-27 00:34:58,556 Epoch 2847 Step:   316000 Batch Loss:     0.000007 Tokens per Sec:  9893462, Lr: 0.000490
2020-02-27 00:35:00,223 Epoch 2847: total training loss 0.00058
2020-02-27 00:35:00,224 EPOCH 2848
2020-02-27 00:35:10,310 Epoch 2848: total training loss 0.00058
2020-02-27 00:35:10,311 EPOCH 2849
2020-02-27 00:35:20,826 Epoch 2849: total training loss 0.00057
2020-02-27 00:35:20,827 EPOCH 2850
2020-02-27 00:35:21,831 Epoch 2850 Step:   316250 Batch Loss:     0.000004 Tokens per Sec:  9913702, Lr: 0.000490
2020-02-27 00:35:31,015 Epoch 2850: total training loss 0.00058
2020-02-27 00:35:31,015 EPOCH 2851
2020-02-27 00:35:41,092 Epoch 2851: total training loss 0.00059
2020-02-27 00:35:41,092 EPOCH 2852
2020-02-27 00:35:44,755 Epoch 2852 Step:   316500 Batch Loss:     0.000003 Tokens per Sec: 10271143, Lr: 0.000490
2020-02-27 00:35:51,140 Epoch 2852: total training loss 0.00057
2020-02-27 00:35:51,140 EPOCH 2853
2020-02-27 00:36:01,460 Epoch 2853: total training loss 0.00056
2020-02-27 00:36:01,461 EPOCH 2854
2020-02-27 00:36:07,556 Epoch 2854 Step:   316750 Batch Loss:     0.000006 Tokens per Sec:  9719236, Lr: 0.000490
2020-02-27 00:36:11,940 Epoch 2854: total training loss 0.00058
2020-02-27 00:36:11,941 EPOCH 2855
2020-02-27 00:36:22,520 Epoch 2855: total training loss 0.00061
2020-02-27 00:36:22,521 EPOCH 2856
2020-02-27 00:36:31,580 Epoch 2856 Step:   317000 Batch Loss:     0.000004 Tokens per Sec:  9675900, Lr: 0.000490
2020-02-27 00:36:33,210 Epoch 2856: total training loss 0.00062
2020-02-27 00:36:33,211 EPOCH 2857
2020-02-27 00:36:44,457 Epoch 2857: total training loss 0.00060
2020-02-27 00:36:44,458 EPOCH 2858
2020-02-27 00:36:55,244 Epoch 2858: total training loss 0.00056
2020-02-27 00:36:55,245 EPOCH 2859
2020-02-27 00:36:56,468 Epoch 2859 Step:   317250 Batch Loss:     0.000005 Tokens per Sec:  8787889, Lr: 0.000490
2020-02-27 00:37:06,573 Epoch 2859: total training loss 0.00056
2020-02-27 00:37:06,573 EPOCH 2860
2020-02-27 00:37:17,439 Epoch 2860: total training loss 0.00057
2020-02-27 00:37:17,440 EPOCH 2861
2020-02-27 00:37:21,302 Epoch 2861 Step:   317500 Batch Loss:     0.000005 Tokens per Sec:  9662661, Lr: 0.000490
2020-02-27 00:37:27,951 Epoch 2861: total training loss 0.00057
2020-02-27 00:37:27,951 EPOCH 2862
2020-02-27 00:37:38,348 Epoch 2862: total training loss 0.00058
2020-02-27 00:37:38,348 EPOCH 2863
2020-02-27 00:37:44,649 Epoch 2863 Step:   317750 Batch Loss:     0.000004 Tokens per Sec: 10062716, Lr: 0.000490
2020-02-27 00:37:48,724 Epoch 2863: total training loss 0.00056
2020-02-27 00:37:48,725 EPOCH 2864
2020-02-27 00:37:58,947 Epoch 2864: total training loss 0.00059
2020-02-27 00:37:58,947 EPOCH 2865
2020-02-27 00:38:07,894 Epoch 2865 Step:   318000 Batch Loss:     0.000004 Tokens per Sec:  9824128, Lr: 0.000490
2020-02-27 00:38:09,365 Epoch 2865: total training loss 0.00059
2020-02-27 00:38:09,365 EPOCH 2866
2020-02-27 00:38:19,682 Epoch 2866: total training loss 0.00059
2020-02-27 00:38:19,683 EPOCH 2867
2020-02-27 00:38:30,473 Epoch 2867: total training loss 0.00058
2020-02-27 00:38:30,474 EPOCH 2868
2020-02-27 00:38:31,805 Epoch 2868 Step:   318250 Batch Loss:     0.000007 Tokens per Sec:  8652248, Lr: 0.000490
2020-02-27 00:38:41,096 Epoch 2868: total training loss 0.00058
2020-02-27 00:38:41,096 EPOCH 2869
2020-02-27 00:38:51,232 Epoch 2869: total training loss 0.00056
2020-02-27 00:38:51,233 EPOCH 2870
2020-02-27 00:38:55,038 Epoch 2870 Step:   318500 Batch Loss:     0.000005 Tokens per Sec: 10128684, Lr: 0.000490
2020-02-27 00:39:01,400 Epoch 2870: total training loss 0.00056
2020-02-27 00:39:01,400 EPOCH 2871
2020-02-27 00:39:11,277 Epoch 2871: total training loss 0.00056
2020-02-27 00:39:11,278 EPOCH 2872
2020-02-27 00:39:17,660 Epoch 2872 Step:   318750 Batch Loss:     0.000006 Tokens per Sec: 10161859, Lr: 0.000490
2020-02-27 00:39:21,381 Epoch 2872: total training loss 0.00057
2020-02-27 00:39:21,381 EPOCH 2873
2020-02-27 00:39:31,815 Epoch 2873: total training loss 0.00058
2020-02-27 00:39:31,816 EPOCH 2874
2020-02-27 00:39:41,059 Epoch 2874 Step:   319000 Batch Loss:     0.000004 Tokens per Sec:  9726414, Lr: 0.000490
2020-02-27 00:39:42,401 Epoch 2874: total training loss 0.00058
2020-02-27 00:39:42,401 EPOCH 2875
2020-02-27 00:39:53,063 Epoch 2875: total training loss 0.00064
2020-02-27 00:39:53,063 EPOCH 2876
2020-02-27 00:40:03,816 Epoch 2876: total training loss 0.00062
2020-02-27 00:40:03,818 EPOCH 2877
2020-02-27 00:40:05,151 Epoch 2877 Step:   319250 Batch Loss:     0.000007 Tokens per Sec:  9759031, Lr: 0.000490
2020-02-27 00:40:14,672 Epoch 2877: total training loss 0.00059
2020-02-27 00:40:14,673 EPOCH 2878
2020-02-27 00:40:25,454 Epoch 2878: total training loss 0.00062
2020-02-27 00:40:25,455 EPOCH 2879
2020-02-27 00:40:29,614 Epoch 2879 Step:   319500 Batch Loss:     0.000005 Tokens per Sec:  9822387, Lr: 0.000490
2020-02-27 00:40:35,953 Epoch 2879: total training loss 0.00057
2020-02-27 00:40:35,954 EPOCH 2880
2020-02-27 00:40:46,871 Epoch 2880: total training loss 0.00056
2020-02-27 00:40:46,872 EPOCH 2881
2020-02-27 00:40:53,695 Epoch 2881 Step:   319750 Batch Loss:     0.000004 Tokens per Sec:  9827850, Lr: 0.000490
2020-02-27 00:40:57,268 Epoch 2881: total training loss 0.00056
2020-02-27 00:40:57,268 EPOCH 2882
2020-02-27 00:41:07,558 Epoch 2882: total training loss 0.00056
2020-02-27 00:41:07,559 EPOCH 2883
2020-02-27 00:41:16,530 Epoch 2883 Step:   320000 Batch Loss:     0.000004 Tokens per Sec: 10095042, Lr: 0.000490
2020-02-27 00:41:16,531 Model noise rate: 5
2020-02-27 00:42:19,839 Validation result at epoch 2883, step   320000: Val DTW Score:  10.73, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0423, GT DTW Score:      nan, duration: 63.3076s
2020-02-27 00:42:21,260 Epoch 2883: total training loss 0.00056
2020-02-27 00:42:21,261 EPOCH 2884
2020-02-27 00:42:31,642 Epoch 2884: total training loss 0.00057
2020-02-27 00:42:31,643 EPOCH 2885
2020-02-27 00:42:42,444 Epoch 2885: total training loss 0.00055
2020-02-27 00:42:42,444 EPOCH 2886
2020-02-27 00:42:43,922 Epoch 2886 Step:   320250 Batch Loss:     0.000005 Tokens per Sec:  9215321, Lr: 0.000490
2020-02-27 00:42:53,218 Epoch 2886: total training loss 0.00057
2020-02-27 00:42:53,219 EPOCH 2887
2020-02-27 00:43:03,614 Epoch 2887: total training loss 0.00062
2020-02-27 00:43:03,614 EPOCH 2888
2020-02-27 00:43:07,684 Epoch 2888 Step:   320500 Batch Loss:     0.000005 Tokens per Sec:  9987808, Lr: 0.000490
2020-02-27 00:43:14,060 Epoch 2888: total training loss 0.00064
2020-02-27 00:43:14,061 EPOCH 2889
2020-02-27 00:43:24,868 Epoch 2889: total training loss 0.00058
2020-02-27 00:43:24,868 EPOCH 2890
2020-02-27 00:43:31,689 Epoch 2890 Step:   320750 Batch Loss:     0.000004 Tokens per Sec:  9553678, Lr: 0.000490
2020-02-27 00:43:35,626 Epoch 2890: total training loss 0.00058
2020-02-27 00:43:35,627 EPOCH 2891
2020-02-27 00:43:46,253 Epoch 2891: total training loss 0.00057
2020-02-27 00:43:46,254 EPOCH 2892
2020-02-27 00:43:55,908 Epoch 2892 Step:   321000 Batch Loss:     0.000004 Tokens per Sec:  9431757, Lr: 0.000490
2020-02-27 00:43:57,085 Epoch 2892: total training loss 0.00058
2020-02-27 00:43:57,086 EPOCH 2893
2020-02-27 00:44:07,176 Epoch 2893: total training loss 0.00059
2020-02-27 00:44:07,177 EPOCH 2894
2020-02-27 00:44:17,153 Epoch 2894: total training loss 0.00060
2020-02-27 00:44:17,154 EPOCH 2895
2020-02-27 00:44:18,535 Epoch 2895 Step:   321250 Batch Loss:     0.000003 Tokens per Sec: 10010087, Lr: 0.000490
2020-02-27 00:44:27,261 Epoch 2895: total training loss 0.00063
2020-02-27 00:44:27,261 EPOCH 2896
2020-02-27 00:44:37,174 Epoch 2896: total training loss 0.00060
2020-02-27 00:44:37,175 EPOCH 2897
2020-02-27 00:44:41,295 Epoch 2897 Step:   321500 Batch Loss:     0.000007 Tokens per Sec: 10515023, Lr: 0.000490
2020-02-27 00:44:47,083 Epoch 2897: total training loss 0.00057
2020-02-27 00:44:47,084 EPOCH 2898
2020-02-27 00:44:57,331 Epoch 2898: total training loss 0.00056
2020-02-27 00:44:57,333 EPOCH 2899
2020-02-27 00:45:04,256 Epoch 2899 Step:   321750 Batch Loss:     0.000006 Tokens per Sec:  9774622, Lr: 0.000490
2020-02-27 00:45:07,879 Epoch 2899: total training loss 0.00056
2020-02-27 00:45:07,879 EPOCH 2900
2020-02-27 00:45:18,532 Epoch 2900: total training loss 0.00055
2020-02-27 00:45:18,537 EPOCH 2901
2020-02-27 00:45:28,330 Epoch 2901 Step:   322000 Batch Loss:     0.000004 Tokens per Sec:  9530531, Lr: 0.000490
2020-02-27 00:45:29,516 Epoch 2901: total training loss 0.00055
2020-02-27 00:45:29,516 EPOCH 2902
2020-02-27 00:45:40,338 Epoch 2902: total training loss 0.00056
2020-02-27 00:45:40,339 EPOCH 2903
2020-02-27 00:45:50,569 Epoch 2903: total training loss 0.00057
2020-02-27 00:45:50,569 EPOCH 2904
2020-02-27 00:45:52,109 Epoch 2904 Step:   322250 Batch Loss:     0.000004 Tokens per Sec: 10481892, Lr: 0.000490
2020-02-27 00:46:00,713 Epoch 2904: total training loss 0.00056
2020-02-27 00:46:00,714 EPOCH 2905
2020-02-27 00:46:11,049 Epoch 2905: total training loss 0.00056
2020-02-27 00:46:11,050 EPOCH 2906
2020-02-27 00:46:14,967 Epoch 2906 Step:   322500 Batch Loss:     0.000007 Tokens per Sec: 10139853, Lr: 0.000490
2020-02-27 00:46:21,016 Epoch 2906: total training loss 0.00064
2020-02-27 00:46:21,017 EPOCH 2907
2020-02-27 00:46:31,133 Epoch 2907: total training loss 0.00060
2020-02-27 00:46:31,135 EPOCH 2908
2020-02-27 00:46:37,957 Epoch 2908 Step:   322750 Batch Loss:     0.000004 Tokens per Sec:  9688081, Lr: 0.000490
2020-02-27 00:46:41,668 Epoch 2908: total training loss 0.00057
2020-02-27 00:46:41,668 EPOCH 2909
2020-02-27 00:46:52,585 Epoch 2909: total training loss 0.00056
2020-02-27 00:46:52,585 EPOCH 2910
2020-02-27 00:47:02,194 Epoch 2910 Step:   323000 Batch Loss:     0.000005 Tokens per Sec:  9701164, Lr: 0.000490
2020-02-27 00:47:03,195 Epoch 2910: total training loss 0.00056
2020-02-27 00:47:03,195 EPOCH 2911
2020-02-27 00:47:13,744 Epoch 2911: total training loss 0.00056
2020-02-27 00:47:13,745 EPOCH 2912
2020-02-27 00:47:23,917 Epoch 2912: total training loss 0.00056
2020-02-27 00:47:23,917 EPOCH 2913
2020-02-27 00:47:25,509 Epoch 2913 Step:   323250 Batch Loss:     0.000004 Tokens per Sec: 10667124, Lr: 0.000490
2020-02-27 00:47:33,821 Epoch 2913: total training loss 0.00060
2020-02-27 00:47:33,822 EPOCH 2914
2020-02-27 00:47:43,957 Epoch 2914: total training loss 0.00058
2020-02-27 00:47:43,957 EPOCH 2915
2020-02-27 00:47:48,292 Epoch 2915 Step:   323500 Batch Loss:     0.000005 Tokens per Sec: 10120838, Lr: 0.000490
2020-02-27 00:47:53,943 Epoch 2915: total training loss 0.00058
2020-02-27 00:47:53,943 EPOCH 2916
2020-02-27 00:48:04,006 Epoch 2916: total training loss 0.00057
2020-02-27 00:48:04,007 EPOCH 2917
2020-02-27 00:48:10,794 Epoch 2917 Step:   323750 Batch Loss:     0.000005 Tokens per Sec:  9979263, Lr: 0.000490
2020-02-27 00:48:14,287 Epoch 2917: total training loss 0.00057
2020-02-27 00:48:14,287 EPOCH 2918
2020-02-27 00:48:25,077 Epoch 2918: total training loss 0.00056
2020-02-27 00:48:25,077 EPOCH 2919
2020-02-27 00:48:35,030 Epoch 2919 Step:   324000 Batch Loss:     0.000004 Tokens per Sec:  9526834, Lr: 0.000490
2020-02-27 00:48:35,989 Epoch 2919: total training loss 0.00058
2020-02-27 00:48:35,989 EPOCH 2920
2020-02-27 00:48:46,586 Epoch 2920: total training loss 0.00057
2020-02-27 00:48:46,586 EPOCH 2921
2020-02-27 00:48:57,428 Epoch 2921: total training loss 0.00056
2020-02-27 00:48:57,429 EPOCH 2922
2020-02-27 00:48:59,070 Epoch 2922 Step:   324250 Batch Loss:     0.000008 Tokens per Sec: 10145360, Lr: 0.000490
2020-02-27 00:49:07,604 Epoch 2922: total training loss 0.00056
2020-02-27 00:49:07,604 EPOCH 2923
2020-02-27 00:49:17,555 Epoch 2923: total training loss 0.00056
2020-02-27 00:49:17,555 EPOCH 2924
2020-02-27 00:49:21,769 Epoch 2924 Step:   324500 Batch Loss:     0.000005 Tokens per Sec: 10245122, Lr: 0.000490
2020-02-27 00:49:27,495 Epoch 2924: total training loss 0.00056
2020-02-27 00:49:27,495 EPOCH 2925
2020-02-27 00:49:37,244 Epoch 2925: total training loss 0.00058
2020-02-27 00:49:37,245 EPOCH 2926
2020-02-27 00:49:43,708 Epoch 2926 Step:   324750 Batch Loss:     0.000005 Tokens per Sec: 10227602, Lr: 0.000490
2020-02-27 00:49:47,241 Epoch 2926: total training loss 0.00059
2020-02-27 00:49:47,241 EPOCH 2927
2020-02-27 00:49:57,444 Epoch 2927: total training loss 0.00058
2020-02-27 00:49:57,445 EPOCH 2928
2020-02-27 00:50:06,886 Epoch 2928 Step:   325000 Batch Loss:     0.000005 Tokens per Sec: 10029853, Lr: 0.000490
2020-02-27 00:50:07,637 Epoch 2928: total training loss 0.00058
2020-02-27 00:50:07,637 EPOCH 2929
2020-02-27 00:50:18,044 Epoch 2929: total training loss 0.00056
2020-02-27 00:50:18,044 EPOCH 2930
2020-02-27 00:50:28,392 Epoch 2930: total training loss 0.00056
2020-02-27 00:50:28,392 EPOCH 2931
2020-02-27 00:50:30,233 Epoch 2931 Step:   325250 Batch Loss:     0.000004 Tokens per Sec:  9600846, Lr: 0.000490
2020-02-27 00:50:38,547 Epoch 2931: total training loss 0.00056
2020-02-27 00:50:38,547 EPOCH 2932
2020-02-27 00:50:48,362 Epoch 2932: total training loss 0.00056
2020-02-27 00:50:48,362 EPOCH 2933
2020-02-27 00:50:52,933 Epoch 2933 Step:   325500 Batch Loss:     0.000005 Tokens per Sec: 10492519, Lr: 0.000490
2020-02-27 00:50:58,249 Epoch 2933: total training loss 0.00056
2020-02-27 00:50:58,249 EPOCH 2934
2020-02-27 00:51:08,234 Epoch 2934: total training loss 0.00056
2020-02-27 00:51:08,236 EPOCH 2935
2020-02-27 00:51:15,064 Epoch 2935 Step:   325750 Batch Loss:     0.000005 Tokens per Sec: 10020980, Lr: 0.000490
2020-02-27 00:51:18,466 Epoch 2935: total training loss 0.00062
2020-02-27 00:51:18,467 EPOCH 2936
2020-02-27 00:51:28,765 Epoch 2936: total training loss 0.00061
2020-02-27 00:51:28,766 EPOCH 2937
2020-02-27 00:51:38,969 Epoch 2937 Step:   326000 Batch Loss:     0.000009 Tokens per Sec:  9383064, Lr: 0.000490
2020-02-27 00:51:39,680 Epoch 2937: total training loss 0.00058
2020-02-27 00:51:39,681 EPOCH 2938
2020-02-27 00:51:50,382 Epoch 2938: total training loss 0.00065
2020-02-27 00:51:50,383 EPOCH 2939
2020-02-27 00:52:01,367 Epoch 2939: total training loss 0.00059
2020-02-27 00:52:01,368 EPOCH 2940
2020-02-27 00:52:03,553 Epoch 2940 Step:   326250 Batch Loss:     0.000006 Tokens per Sec:  9909808, Lr: 0.000490
2020-02-27 00:52:11,721 Epoch 2940: total training loss 0.00056
2020-02-27 00:52:11,721 EPOCH 2941
2020-02-27 00:52:22,093 Epoch 2941: total training loss 0.00058
2020-02-27 00:52:22,093 EPOCH 2942
2020-02-27 00:52:26,567 Epoch 2942 Step:   326500 Batch Loss:     0.000005 Tokens per Sec: 10047128, Lr: 0.000490
2020-02-27 00:52:32,282 Epoch 2942: total training loss 0.00056
2020-02-27 00:52:32,282 EPOCH 2943
2020-02-27 00:52:42,349 Epoch 2943: total training loss 0.00056
2020-02-27 00:52:42,350 EPOCH 2944
2020-02-27 00:52:49,264 Epoch 2944 Step:   326750 Batch Loss:     0.000005 Tokens per Sec: 10400056, Lr: 0.000490
2020-02-27 00:52:52,278 Epoch 2944: total training loss 0.00055
2020-02-27 00:52:52,279 EPOCH 2945
2020-02-27 00:53:02,143 Epoch 2945: total training loss 0.00056
2020-02-27 00:53:02,143 EPOCH 2946
2020-02-27 00:53:11,396 Epoch 2946 Step:   327000 Batch Loss:     0.000004 Tokens per Sec: 10428455, Lr: 0.000490
2020-02-27 00:53:11,959 Epoch 2946: total training loss 0.00056
2020-02-27 00:53:11,960 EPOCH 2947
2020-02-27 00:53:22,262 Epoch 2947: total training loss 0.00058
2020-02-27 00:53:22,262 EPOCH 2948
2020-02-27 00:53:32,922 Epoch 2948: total training loss 0.00057
2020-02-27 00:53:32,923 EPOCH 2949
2020-02-27 00:53:35,100 Epoch 2949 Step:   327250 Batch Loss:     0.000004 Tokens per Sec:  9453162, Lr: 0.000490
2020-02-27 00:53:43,640 Epoch 2949: total training loss 0.00058
2020-02-27 00:53:43,641 EPOCH 2950
2020-02-27 00:53:53,949 Epoch 2950: total training loss 0.00056
2020-02-27 00:53:53,950 EPOCH 2951
2020-02-27 00:53:58,721 Epoch 2951 Step:   327500 Batch Loss:     0.000004 Tokens per Sec:  9662759, Lr: 0.000490
2020-02-27 00:54:04,531 Epoch 2951: total training loss 0.00056
2020-02-27 00:54:04,532 EPOCH 2952
2020-02-27 00:54:14,604 Epoch 2952: total training loss 0.00056
2020-02-27 00:54:14,605 EPOCH 2953
2020-02-27 00:54:21,827 Epoch 2953 Step:   327750 Batch Loss:     0.000006 Tokens per Sec: 10064006, Lr: 0.000490
2020-02-27 00:54:24,791 Epoch 2953: total training loss 0.00057
2020-02-27 00:54:24,792 EPOCH 2954
2020-02-27 00:54:35,135 Epoch 2954: total training loss 0.00059
2020-02-27 00:54:35,135 EPOCH 2955
2020-02-27 00:54:44,736 Epoch 2955 Step:   328000 Batch Loss:     0.000004 Tokens per Sec: 10146594, Lr: 0.000490
2020-02-27 00:54:45,204 Epoch 2955: total training loss 0.00058
2020-02-27 00:54:45,205 EPOCH 2956
2020-02-27 00:54:55,203 Epoch 2956: total training loss 0.00056
2020-02-27 00:54:55,204 EPOCH 2957
2020-02-27 00:55:05,221 Epoch 2957: total training loss 0.00058
2020-02-27 00:55:05,221 EPOCH 2958
2020-02-27 00:55:07,533 Epoch 2958 Step:   328250 Batch Loss:     0.000005 Tokens per Sec:  9903832, Lr: 0.000490
2020-02-27 00:55:15,790 Epoch 2958: total training loss 0.00057
2020-02-27 00:55:15,790 EPOCH 2959
2020-02-27 00:55:25,801 Epoch 2959: total training loss 0.00066
2020-02-27 00:55:25,801 EPOCH 2960
2020-02-27 00:55:30,425 Epoch 2960 Step:   328500 Batch Loss:     0.000005 Tokens per Sec: 10474638, Lr: 0.000490
2020-02-27 00:55:35,681 Epoch 2960: total training loss 0.00059
2020-02-27 00:55:35,681 EPOCH 2961
2020-02-27 00:55:45,593 Epoch 2961: total training loss 0.00056
2020-02-27 00:55:45,594 EPOCH 2962
2020-02-27 00:55:53,153 Epoch 2962 Step:   328750 Batch Loss:     0.000004 Tokens per Sec:  9853763, Lr: 0.000490
2020-02-27 00:55:56,137 Epoch 2962: total training loss 0.00059
2020-02-27 00:55:56,137 EPOCH 2963
2020-02-27 00:56:06,524 Epoch 2963: total training loss 0.00065
2020-02-27 00:56:06,524 EPOCH 2964
2020-02-27 00:56:16,780 Epoch 2964 Step:   329000 Batch Loss:     0.000006 Tokens per Sec:  9678852, Lr: 0.000490
2020-02-27 00:56:17,102 Epoch 2964: total training loss 0.00057
2020-02-27 00:56:17,102 EPOCH 2965
2020-02-27 00:56:27,083 Epoch 2965: total training loss 0.00056
2020-02-27 00:56:27,083 EPOCH 2966
2020-02-27 00:56:36,823 Epoch 2966: total training loss 0.00056
2020-02-27 00:56:36,824 EPOCH 2967
2020-02-27 00:56:38,942 Epoch 2967 Step:   329250 Batch Loss:     0.000003 Tokens per Sec: 10087753, Lr: 0.000490
2020-02-27 00:56:46,924 Epoch 2967: total training loss 0.00055
2020-02-27 00:56:46,924 EPOCH 2968
2020-02-27 00:56:57,192 Epoch 2968: total training loss 0.00056
2020-02-27 00:56:57,192 EPOCH 2969
2020-02-27 00:57:01,948 Epoch 2969 Step:   329500 Batch Loss:     0.000004 Tokens per Sec: 10076410, Lr: 0.000490
2020-02-27 00:57:07,419 Epoch 2969: total training loss 0.00056
2020-02-27 00:57:07,420 EPOCH 2970
2020-02-27 00:57:17,654 Epoch 2970: total training loss 0.00058
2020-02-27 00:57:17,655 EPOCH 2971
2020-02-27 00:57:25,162 Epoch 2971 Step:   329750 Batch Loss:     0.000006 Tokens per Sec:  9594166, Lr: 0.000490
2020-02-27 00:57:28,308 Epoch 2971: total training loss 0.00056
2020-02-27 00:57:28,308 EPOCH 2972
2020-02-27 00:57:39,098 Epoch 2972: total training loss 0.00056
2020-02-27 00:57:39,098 EPOCH 2973
2020-02-27 00:57:49,130 Epoch 2973 Step:   330000 Batch Loss:     0.000006 Tokens per Sec:  9824155, Lr: 0.000490
2020-02-27 00:57:49,130 Model noise rate: 5
2020-02-27 00:58:50,089 Validation result at epoch 2973, step   330000: Val DTW Score:  10.73, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0424, GT DTW Score:      nan, duration: 60.9584s
2020-02-27 00:58:50,492 Epoch 2973: total training loss 0.00057
2020-02-27 00:58:50,493 EPOCH 2974
2020-02-27 00:59:00,666 Epoch 2974: total training loss 0.00056
2020-02-27 00:59:00,667 EPOCH 2975
2020-02-27 00:59:11,297 Epoch 2975: total training loss 0.00056
2020-02-27 00:59:11,298 EPOCH 2976
2020-02-27 00:59:13,628 Epoch 2976 Step:   330250 Batch Loss:     0.000006 Tokens per Sec:  9763601, Lr: 0.000490
2020-02-27 00:59:21,875 Epoch 2976: total training loss 0.00056
2020-02-27 00:59:21,875 EPOCH 2977
2020-02-27 00:59:32,748 Epoch 2977: total training loss 0.00060
2020-02-27 00:59:32,749 EPOCH 2978
2020-02-27 00:59:38,145 Epoch 2978 Step:   330500 Batch Loss:     0.000006 Tokens per Sec:  9041027, Lr: 0.000490
2020-02-27 00:59:43,869 Epoch 2978: total training loss 0.00064
2020-02-27 00:59:43,869 EPOCH 2979
2020-02-27 00:59:54,228 Epoch 2979: total training loss 0.00057
2020-02-27 00:59:54,229 EPOCH 2980
2020-02-27 01:00:01,589 Epoch 2980 Step:   330750 Batch Loss:     0.000004 Tokens per Sec: 10076788, Lr: 0.000490
2020-02-27 01:00:04,513 Epoch 2980: total training loss 0.00058
2020-02-27 01:00:04,513 EPOCH 2981
2020-02-27 01:00:14,884 Epoch 2981: total training loss 0.00056
2020-02-27 01:00:14,885 EPOCH 2982
2020-02-27 01:00:24,917 Epoch 2982 Step:   331000 Batch Loss:     0.000005 Tokens per Sec:  9988152, Lr: 0.000490
2020-02-27 01:00:25,238 Epoch 2982: total training loss 0.00055
2020-02-27 01:00:25,238 EPOCH 2983
2020-02-27 01:00:35,524 Epoch 2983: total training loss 0.00056
2020-02-27 01:00:35,524 EPOCH 2984
2020-02-27 01:00:46,032 Epoch 2984: total training loss 0.00056
2020-02-27 01:00:46,032 EPOCH 2985
2020-02-27 01:00:48,658 Epoch 2985 Step:   331250 Batch Loss:     0.000005 Tokens per Sec:  9685274, Lr: 0.000490
2020-02-27 01:00:56,734 Epoch 2985: total training loss 0.00055
2020-02-27 01:00:56,734 EPOCH 2986
2020-02-27 01:01:07,623 Epoch 2986: total training loss 0.00055
2020-02-27 01:01:07,623 EPOCH 2987
2020-02-27 01:01:12,775 Epoch 2987 Step:   331500 Batch Loss:     0.000003 Tokens per Sec:  9655222, Lr: 0.000490
2020-02-27 01:01:18,190 Epoch 2987: total training loss 0.00057
2020-02-27 01:01:18,190 EPOCH 2988
2020-02-27 01:01:29,182 Epoch 2988: total training loss 0.00056
2020-02-27 01:01:29,183 EPOCH 2989
2020-02-27 01:01:36,576 Epoch 2989 Step:   331750 Batch Loss:     0.000005 Tokens per Sec: 10224005, Lr: 0.000490
2020-02-27 01:01:39,138 Epoch 2989: total training loss 0.00056
2020-02-27 01:01:39,139 EPOCH 2990
2020-02-27 01:01:49,115 Epoch 2990: total training loss 0.00056
2020-02-27 01:01:49,116 EPOCH 2991
2020-02-27 01:01:59,002 Epoch 2991 Step:   332000 Batch Loss:     0.000004 Tokens per Sec: 10192550, Lr: 0.000490
2020-02-27 01:01:59,097 Epoch 2991: total training loss 0.00059
2020-02-27 01:01:59,097 EPOCH 2992
2020-02-27 01:02:09,068 Epoch 2992: total training loss 0.00065
2020-02-27 01:02:09,068 EPOCH 2993
2020-02-27 01:02:19,038 Epoch 2993: total training loss 0.00057
2020-02-27 01:02:19,039 EPOCH 2994
2020-02-27 01:02:21,635 Epoch 2994 Step:   332250 Batch Loss:     0.000005 Tokens per Sec:  9701030, Lr: 0.000490
2020-02-27 01:02:29,529 Epoch 2994: total training loss 0.00066
2020-02-27 01:02:29,530 EPOCH 2995
2020-02-27 01:02:40,316 Epoch 2995: total training loss 0.00058
2020-02-27 01:02:40,316 EPOCH 2996
2020-02-27 01:02:45,625 Epoch 2996 Step:   332500 Batch Loss:     0.000006 Tokens per Sec:  9626029, Lr: 0.000490
2020-02-27 01:02:50,974 Epoch 2996: total training loss 0.00056
2020-02-27 01:02:50,974 EPOCH 2997
2020-02-27 01:03:01,706 Epoch 2997: total training loss 0.00056
2020-02-27 01:03:01,706 EPOCH 2998
2020-02-27 01:03:09,721 Epoch 2998 Step:   332750 Batch Loss:     0.000004 Tokens per Sec:  9620946, Lr: 0.000490
2020-02-27 01:03:12,508 Epoch 2998: total training loss 0.00055
2020-02-27 01:03:12,508 EPOCH 2999
2020-02-27 01:03:22,775 Epoch 2999: total training loss 0.00056
2020-02-27 01:03:22,775 EPOCH 3000
2020-02-27 01:03:32,627 Epoch 3000 Step:   333000 Batch Loss:     0.000005 Tokens per Sec: 10385337, Lr: 0.000490
2020-02-27 01:03:32,628 Epoch 3000: total training loss 0.00058
2020-02-27 01:03:32,628 EPOCH 3001
2020-02-27 01:03:42,594 Epoch 3001: total training loss 0.00056
2020-02-27 01:03:42,595 EPOCH 3002
2020-02-27 01:03:52,578 Epoch 3002: total training loss 0.00056
2020-02-27 01:03:52,578 EPOCH 3003
2020-02-27 01:03:54,989 Epoch 3003 Step:   333250 Batch Loss:     0.000006 Tokens per Sec: 10173744, Lr: 0.000490
2020-02-27 01:04:02,429 Epoch 3003: total training loss 0.00056
2020-02-27 01:04:02,429 EPOCH 3004
2020-02-27 01:04:12,272 Epoch 3004: total training loss 0.00056
2020-02-27 01:04:12,273 EPOCH 3005
2020-02-27 01:04:17,576 Epoch 3005 Step:   333500 Batch Loss:     0.000006 Tokens per Sec:  9694768, Lr: 0.000490
2020-02-27 01:04:22,783 Epoch 3005: total training loss 0.00056
2020-02-27 01:04:22,783 EPOCH 3006
2020-02-27 01:04:33,448 Epoch 3006: total training loss 0.00058
2020-02-27 01:04:33,449 EPOCH 3007
2020-02-27 01:04:41,098 Epoch 3007 Step:   333750 Batch Loss:     0.000004 Tokens per Sec:  9802242, Lr: 0.000490
2020-02-27 01:04:43,985 Epoch 3007: total training loss 0.00057
2020-02-27 01:04:43,985 EPOCH 3008
2020-02-27 01:04:54,655 Epoch 3008: total training loss 0.00058
2020-02-27 01:04:54,656 EPOCH 3009
2020-02-27 01:05:04,956 Epoch 3009: total training loss 0.00056
2020-02-27 01:05:04,957 EPOCH 3010
2020-02-27 01:05:05,081 Epoch 3010 Step:   334000 Batch Loss:     0.000006 Tokens per Sec:  8713341, Lr: 0.000490
2020-02-27 01:05:15,222 Epoch 3010: total training loss 0.00055
2020-02-27 01:05:15,222 EPOCH 3011
2020-02-27 01:05:25,303 Epoch 3011: total training loss 0.00056
2020-02-27 01:05:25,304 EPOCH 3012
2020-02-27 01:05:28,071 Epoch 3012 Step:   334250 Batch Loss:     0.000005 Tokens per Sec: 10280960, Lr: 0.000490
2020-02-27 01:05:35,480 Epoch 3012: total training loss 0.00064
2020-02-27 01:05:35,480 EPOCH 3013
2020-02-27 01:05:45,369 Epoch 3013: total training loss 0.00059
2020-02-27 01:05:45,369 EPOCH 3014
2020-02-27 01:05:50,571 Epoch 3014 Step:   334500 Batch Loss:     0.000008 Tokens per Sec: 10323881, Lr: 0.000490
2020-02-27 01:05:55,597 Epoch 3014: total training loss 0.00056
2020-02-27 01:05:55,598 EPOCH 3015
2020-02-27 01:06:05,832 Epoch 3015: total training loss 0.00056
2020-02-27 01:06:05,833 EPOCH 3016
2020-02-27 01:06:13,604 Epoch 3016 Step:   334750 Batch Loss:     0.000005 Tokens per Sec:  9802202, Lr: 0.000490
2020-02-27 01:06:16,268 Epoch 3016: total training loss 0.00058
2020-02-27 01:06:16,269 EPOCH 3017
2020-02-27 01:06:26,620 Epoch 3017: total training loss 0.00056
2020-02-27 01:06:26,621 EPOCH 3018
2020-02-27 01:06:37,200 Epoch 3018: total training loss 0.00056
2020-02-27 01:06:37,200 EPOCH 3019
2020-02-27 01:06:37,377 Epoch 3019 Step:   335000 Batch Loss:     0.000005 Tokens per Sec:  8454460, Lr: 0.000490
2020-02-27 01:06:47,104 Epoch 3019: total training loss 0.00056
2020-02-27 01:06:47,105 EPOCH 3020
2020-02-27 01:06:57,175 Epoch 3020: total training loss 0.00055
2020-02-27 01:06:57,175 EPOCH 3021
2020-02-27 01:06:59,813 Epoch 3021 Step:   335250 Batch Loss:     0.000005 Tokens per Sec: 10486120, Lr: 0.000490
2020-02-27 01:07:07,043 Epoch 3021: total training loss 0.00055
2020-02-27 01:07:07,043 EPOCH 3022
2020-02-27 01:07:16,999 Epoch 3022: total training loss 0.00055
2020-02-27 01:07:17,000 EPOCH 3023
2020-02-27 01:07:22,390 Epoch 3023 Step:   335500 Batch Loss:     0.000003 Tokens per Sec:  9889491, Lr: 0.000490
2020-02-27 01:07:27,371 Epoch 3023: total training loss 0.00057
2020-02-27 01:07:27,372 EPOCH 3024
2020-02-27 01:07:38,060 Epoch 3024: total training loss 0.00059
2020-02-27 01:07:38,061 EPOCH 3025
2020-02-27 01:07:46,438 Epoch 3025 Step:   335750 Batch Loss:     0.000005 Tokens per Sec:  9390411, Lr: 0.000490
2020-02-27 01:07:48,982 Epoch 3025: total training loss 0.00056
2020-02-27 01:07:48,982 EPOCH 3026
2020-02-27 01:07:59,825 Epoch 3026: total training loss 0.00057
2020-02-27 01:07:59,825 EPOCH 3027
2020-02-27 01:08:10,344 Epoch 3027: total training loss 0.00057
2020-02-27 01:08:10,345 EPOCH 3028
2020-02-27 01:08:10,651 Epoch 3028 Step:   336000 Batch Loss:     0.000005 Tokens per Sec: 10130083, Lr: 0.000490
2020-02-27 01:08:20,579 Epoch 3028: total training loss 0.00059
2020-02-27 01:08:20,580 EPOCH 3029
2020-02-27 01:08:30,483 Epoch 3029: total training loss 0.00059
2020-02-27 01:08:30,483 EPOCH 3030
2020-02-27 01:08:33,295 Epoch 3030 Step:   336250 Batch Loss:     0.000006 Tokens per Sec: 10278166, Lr: 0.000490
2020-02-27 01:08:40,425 Epoch 3030: total training loss 0.00057
2020-02-27 01:08:40,426 EPOCH 3031
2020-02-27 01:08:50,242 Epoch 3031: total training loss 0.00056
2020-02-27 01:08:50,242 EPOCH 3032
2020-02-27 01:08:55,566 Epoch 3032 Step:   336500 Batch Loss:     0.000004 Tokens per Sec: 10140491, Lr: 0.000490
2020-02-27 01:09:00,350 Epoch 3032: total training loss 0.00055
2020-02-27 01:09:00,351 EPOCH 3033
2020-02-27 01:09:10,779 Epoch 3033: total training loss 0.00055
2020-02-27 01:09:10,779 EPOCH 3034
2020-02-27 01:09:19,215 Epoch 3034 Step:   336750 Batch Loss:     0.000006 Tokens per Sec:  9684515, Lr: 0.000490
2020-02-27 01:09:21,276 Epoch 3034: total training loss 0.00056
2020-02-27 01:09:21,277 EPOCH 3035
2020-02-27 01:09:31,978 Epoch 3035: total training loss 0.00055
2020-02-27 01:09:31,978 EPOCH 3036
2020-02-27 01:09:42,206 Epoch 3036: total training loss 0.00056
2020-02-27 01:09:42,207 EPOCH 3037
2020-02-27 01:09:42,663 Epoch 3037 Step:   337000 Batch Loss:     0.000005 Tokens per Sec:  9327676, Lr: 0.000490
2020-02-27 01:09:52,370 Epoch 3037: total training loss 0.00059
2020-02-27 01:09:52,371 EPOCH 3038
2020-02-27 01:10:02,345 Epoch 3038: total training loss 0.00057
2020-02-27 01:10:02,346 EPOCH 3039
2020-02-27 01:10:05,279 Epoch 3039 Step:   337250 Batch Loss:     0.000005 Tokens per Sec: 10166937, Lr: 0.000490
2020-02-27 01:10:12,461 Epoch 3039: total training loss 0.00056
2020-02-27 01:10:12,463 EPOCH 3040
2020-02-27 01:10:22,782 Epoch 3040: total training loss 0.00055
2020-02-27 01:10:22,783 EPOCH 3041
2020-02-27 01:10:28,225 Epoch 3041 Step:   337500 Batch Loss:     0.000008 Tokens per Sec:  9763847, Lr: 0.000490
2020-02-27 01:10:33,059 Epoch 3041: total training loss 0.00056
2020-02-27 01:10:33,059 EPOCH 3042
2020-02-27 01:10:43,280 Epoch 3042: total training loss 0.00055
2020-02-27 01:10:43,281 EPOCH 3043
2020-02-27 01:10:52,102 Epoch 3043 Step:   337750 Batch Loss:     0.000005 Tokens per Sec:  9379611, Lr: 0.000490
2020-02-27 01:10:54,245 Epoch 3043: total training loss 0.00056
2020-02-27 01:10:54,245 EPOCH 3044
2020-02-27 01:11:05,028 Epoch 3044: total training loss 0.00056
2020-02-27 01:11:05,029 EPOCH 3045
2020-02-27 01:11:15,357 Epoch 3045: total training loss 0.00056
2020-02-27 01:11:15,358 EPOCH 3046
2020-02-27 01:11:15,788 Epoch 3046 Step:   338000 Batch Loss:     0.000003 Tokens per Sec:  9311223, Lr: 0.000490
2020-02-27 01:11:25,996 Epoch 3046: total training loss 0.00055
2020-02-27 01:11:25,996 EPOCH 3047
2020-02-27 01:11:36,221 Epoch 3047: total training loss 0.00055
2020-02-27 01:11:36,221 EPOCH 3048
2020-02-27 01:11:39,010 Epoch 3048 Step:   338250 Batch Loss:     0.000004 Tokens per Sec: 10115011, Lr: 0.000490
2020-02-27 01:11:46,107 Epoch 3048: total training loss 0.00055
2020-02-27 01:11:46,107 EPOCH 3049
2020-02-27 01:11:55,864 Epoch 3049: total training loss 0.00055
2020-02-27 01:11:55,865 EPOCH 3050
2020-02-27 01:12:01,310 Epoch 3050 Step:   338500 Batch Loss:     0.000004 Tokens per Sec: 10432548, Lr: 0.000490
2020-02-27 01:12:05,846 Epoch 3050: total training loss 0.00055
2020-02-27 01:12:05,847 EPOCH 3051
2020-02-27 01:12:15,765 Epoch 3051: total training loss 0.00055
2020-02-27 01:12:15,766 EPOCH 3052
2020-02-27 01:12:23,381 Epoch 3052 Step:   338750 Batch Loss:     0.000004 Tokens per Sec: 10629494, Lr: 0.000490
2020-02-27 01:12:25,350 Epoch 3052: total training loss 0.00057
2020-02-27 01:12:25,351 EPOCH 3053
2020-02-27 01:12:35,323 Epoch 3053: total training loss 0.00056
2020-02-27 01:12:35,324 EPOCH 3054
2020-02-27 01:12:45,780 Epoch 3054: total training loss 0.00056
2020-02-27 01:12:45,781 EPOCH 3055
2020-02-27 01:12:46,472 Epoch 3055 Step:   339000 Batch Loss:     0.000005 Tokens per Sec: 10105959, Lr: 0.000490
2020-02-27 01:12:56,055 Epoch 3055: total training loss 0.00056
2020-02-27 01:12:56,056 EPOCH 3056
2020-02-27 01:13:06,268 Epoch 3056: total training loss 0.00057
2020-02-27 01:13:06,268 EPOCH 3057
2020-02-27 01:13:09,116 Epoch 3057 Step:   339250 Batch Loss:     0.000006 Tokens per Sec:  9524782, Lr: 0.000490
2020-02-27 01:13:16,485 Epoch 3057: total training loss 0.00059
2020-02-27 01:13:16,486 EPOCH 3058
2020-02-27 01:13:26,598 Epoch 3058: total training loss 0.00062
2020-02-27 01:13:26,598 EPOCH 3059
2020-02-27 01:13:32,328 Epoch 3059 Step:   339500 Batch Loss:     0.000006 Tokens per Sec: 10427566, Lr: 0.000490
2020-02-27 01:13:36,498 Epoch 3059: total training loss 0.00073
2020-02-27 01:13:36,499 EPOCH 3060
2020-02-27 01:13:46,581 Epoch 3060: total training loss 0.00058
2020-02-27 01:13:46,582 EPOCH 3061
2020-02-27 01:13:54,866 Epoch 3061 Step:   339750 Batch Loss:     0.000006 Tokens per Sec:  9933431, Lr: 0.000490
2020-02-27 01:13:56,775 Epoch 3061: total training loss 0.00058
2020-02-27 01:13:56,775 EPOCH 3062
2020-02-27 01:14:06,789 Epoch 3062: total training loss 0.00056
2020-02-27 01:14:06,789 EPOCH 3063
2020-02-27 01:14:17,015 Epoch 3063: total training loss 0.00055
2020-02-27 01:14:17,016 EPOCH 3064
2020-02-27 01:14:17,595 Epoch 3064 Step:   340000 Batch Loss:     0.000003 Tokens per Sec:  9937978, Lr: 0.000490
2020-02-27 01:14:17,595 Model noise rate: 5
2020-02-27 01:15:23,450 Validation result at epoch 3064, step   340000: Val DTW Score:  10.76, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0423, GT DTW Score:      nan, duration: 65.8542s
2020-02-27 01:15:32,803 Epoch 3064: total training loss 0.00055
2020-02-27 01:15:32,804 EPOCH 3065
2020-02-27 01:15:42,742 Epoch 3065: total training loss 0.00054
2020-02-27 01:15:42,742 EPOCH 3066
2020-02-27 01:15:45,960 Epoch 3066 Step:   340250 Batch Loss:     0.000005 Tokens per Sec: 10225230, Lr: 0.000343
2020-02-27 01:15:52,636 Epoch 3066: total training loss 0.00054
2020-02-27 01:15:52,636 EPOCH 3067
2020-02-27 01:16:02,544 Epoch 3067: total training loss 0.00053
2020-02-27 01:16:02,574 EPOCH 3068
2020-02-27 01:16:08,148 Epoch 3068 Step:   340500 Batch Loss:     0.000006 Tokens per Sec: 10390176, Lr: 0.000343
2020-02-27 01:16:12,469 Epoch 3068: total training loss 0.00054
2020-02-27 01:16:12,470 EPOCH 3069
2020-02-27 01:16:22,829 Epoch 3069: total training loss 0.00054
2020-02-27 01:16:22,829 EPOCH 3070
2020-02-27 01:16:30,950 Epoch 3070 Step:   340750 Batch Loss:     0.000005 Tokens per Sec: 10211503, Lr: 0.000343
2020-02-27 01:16:32,952 Epoch 3070: total training loss 0.00054
2020-02-27 01:16:32,952 EPOCH 3071
2020-02-27 01:16:43,228 Epoch 3071: total training loss 0.00054
2020-02-27 01:16:43,228 EPOCH 3072
2020-02-27 01:16:53,652 Epoch 3072: total training loss 0.00054
2020-02-27 01:16:53,653 EPOCH 3073
2020-02-27 01:16:54,492 Epoch 3073 Step:   341000 Batch Loss:     0.000003 Tokens per Sec:  9666483, Lr: 0.000343
2020-02-27 01:17:03,875 Epoch 3073: total training loss 0.00053
2020-02-27 01:17:03,875 EPOCH 3074
2020-02-27 01:17:14,771 Epoch 3074: total training loss 0.00053
2020-02-27 01:17:14,771 EPOCH 3075
2020-02-27 01:17:18,214 Epoch 3075 Step:   341250 Batch Loss:     0.000004 Tokens per Sec:  9865736, Lr: 0.000343
2020-02-27 01:17:25,244 Epoch 3075: total training loss 0.00054
2020-02-27 01:17:25,244 EPOCH 3076
2020-02-27 01:17:35,559 Epoch 3076: total training loss 0.00054
2020-02-27 01:17:35,559 EPOCH 3077
2020-02-27 01:17:41,289 Epoch 3077 Step:   341500 Batch Loss:     0.000005 Tokens per Sec: 10121472, Lr: 0.000343
2020-02-27 01:17:45,575 Epoch 3077: total training loss 0.00054
2020-02-27 01:17:45,575 EPOCH 3078
2020-02-27 01:17:55,655 Epoch 3078: total training loss 0.00053
2020-02-27 01:17:55,656 EPOCH 3079
2020-02-27 01:18:03,951 Epoch 3079 Step:   341750 Batch Loss:     0.000005 Tokens per Sec: 10154000, Lr: 0.000343
2020-02-27 01:18:05,725 Epoch 3079: total training loss 0.00054
2020-02-27 01:18:05,725 EPOCH 3080
2020-02-27 01:18:15,622 Epoch 3080: total training loss 0.00053
2020-02-27 01:18:15,623 EPOCH 3081
2020-02-27 01:18:25,562 Epoch 3081: total training loss 0.00054
2020-02-27 01:18:25,563 EPOCH 3082
2020-02-27 01:18:26,393 Epoch 3082 Step:   342000 Batch Loss:     0.000003 Tokens per Sec: 10014395, Lr: 0.000343
2020-02-27 01:18:35,455 Epoch 3082: total training loss 0.00053
2020-02-27 01:18:35,456 EPOCH 3083
2020-02-27 01:18:45,346 Epoch 3083: total training loss 0.00054
2020-02-27 01:18:45,346 EPOCH 3084
2020-02-27 01:18:48,851 Epoch 3084 Step:   342250 Batch Loss:     0.000005 Tokens per Sec: 10297937, Lr: 0.000343
2020-02-27 01:18:55,545 Epoch 3084: total training loss 0.00053
2020-02-27 01:18:55,545 EPOCH 3085
2020-02-27 01:19:06,014 Epoch 3085: total training loss 0.00054
2020-02-27 01:19:06,015 EPOCH 3086
2020-02-27 01:19:12,333 Epoch 3086 Step:   342500 Batch Loss:     0.000006 Tokens per Sec:  9712970, Lr: 0.000343
2020-02-27 01:19:16,679 Epoch 3086: total training loss 0.00053
2020-02-27 01:19:16,679 EPOCH 3087
2020-02-27 01:19:27,711 Epoch 3087: total training loss 0.00054
2020-02-27 01:19:27,712 EPOCH 3088
2020-02-27 01:19:36,691 Epoch 3088 Step:   342750 Batch Loss:     0.000004 Tokens per Sec:  9577325, Lr: 0.000343
2020-02-27 01:19:38,486 Epoch 3088: total training loss 0.00053
2020-02-27 01:19:38,486 EPOCH 3089
2020-02-27 01:19:48,838 Epoch 3089: total training loss 0.00055
2020-02-27 01:19:48,838 EPOCH 3090
2020-02-27 01:19:58,907 Epoch 3090: total training loss 0.00055
2020-02-27 01:19:58,908 EPOCH 3091
2020-02-27 01:19:59,793 Epoch 3091 Step:   343000 Batch Loss:     0.000005 Tokens per Sec:  9936113, Lr: 0.000343
2020-02-27 01:20:09,022 Epoch 3091: total training loss 0.00054
2020-02-27 01:20:09,022 EPOCH 3092
2020-02-27 01:20:19,015 Epoch 3092: total training loss 0.00055
2020-02-27 01:20:19,015 EPOCH 3093
2020-02-27 01:20:22,393 Epoch 3093 Step:   343250 Batch Loss:     0.000009 Tokens per Sec: 10432529, Lr: 0.000343
2020-02-27 01:20:29,096 Epoch 3093: total training loss 0.00054
2020-02-27 01:20:29,097 EPOCH 3094
2020-02-27 01:20:39,561 Epoch 3094: total training loss 0.00054
2020-02-27 01:20:39,562 EPOCH 3095
2020-02-27 01:20:45,916 Epoch 3095 Step:   343500 Batch Loss:     0.000006 Tokens per Sec:  9644589, Lr: 0.000343
2020-02-27 01:20:50,170 Epoch 3095: total training loss 0.00054
2020-02-27 01:20:50,171 EPOCH 3096
2020-02-27 01:21:00,623 Epoch 3096: total training loss 0.00053
2020-02-27 01:21:00,624 EPOCH 3097
2020-02-27 01:21:09,821 Epoch 3097 Step:   343750 Batch Loss:     0.000003 Tokens per Sec:  9588277, Lr: 0.000343
2020-02-27 01:21:11,280 Epoch 3097: total training loss 0.00053
2020-02-27 01:21:11,280 EPOCH 3098
2020-02-27 01:21:21,216 Epoch 3098: total training loss 0.00053
2020-02-27 01:21:21,217 EPOCH 3099
2020-02-27 01:21:31,264 Epoch 3099: total training loss 0.00053
2020-02-27 01:21:31,264 EPOCH 3100
2020-02-27 01:21:32,136 Epoch 3100 Step:   344000 Batch Loss:     0.000004 Tokens per Sec:  9841621, Lr: 0.000343
2020-02-27 01:21:41,254 Epoch 3100: total training loss 0.00053
2020-02-27 01:21:41,254 EPOCH 3101
2020-02-27 01:21:51,071 Epoch 3101: total training loss 0.00055
2020-02-27 01:21:51,071 EPOCH 3102
2020-02-27 01:21:54,575 Epoch 3102 Step:   344250 Batch Loss:     0.000004 Tokens per Sec: 10447092, Lr: 0.000343
2020-02-27 01:22:00,880 Epoch 3102: total training loss 0.00056
2020-02-27 01:22:00,881 EPOCH 3103
2020-02-27 01:22:10,909 Epoch 3103: total training loss 0.00055
2020-02-27 01:22:10,909 EPOCH 3104
2020-02-27 01:22:17,293 Epoch 3104 Step:   344500 Batch Loss:     0.000004 Tokens per Sec:  9349984, Lr: 0.000343
2020-02-27 01:22:21,766 Epoch 3104: total training loss 0.00054
2020-02-27 01:22:21,766 EPOCH 3105
2020-02-27 01:22:32,719 Epoch 3105: total training loss 0.00054
2020-02-27 01:22:32,720 EPOCH 3106
2020-02-27 01:22:42,015 Epoch 3106 Step:   344750 Batch Loss:     0.000007 Tokens per Sec:  9438912, Lr: 0.000343
2020-02-27 01:22:43,603 Epoch 3106: total training loss 0.00053
2020-02-27 01:22:43,603 EPOCH 3107
2020-02-27 01:22:54,905 Epoch 3107: total training loss 0.00053
2020-02-27 01:22:54,905 EPOCH 3108
2020-02-27 01:23:05,258 Epoch 3108: total training loss 0.00054
2020-02-27 01:23:05,258 EPOCH 3109
2020-02-27 01:23:06,268 Epoch 3109 Step:   345000 Batch Loss:     0.000003 Tokens per Sec: 10002497, Lr: 0.000343
2020-02-27 01:23:15,128 Epoch 3109: total training loss 0.00053
2020-02-27 01:23:15,129 EPOCH 3110
2020-02-27 01:23:25,128 Epoch 3110: total training loss 0.00053
2020-02-27 01:23:25,128 EPOCH 3111
2020-02-27 01:23:28,689 Epoch 3111 Step:   345250 Batch Loss:     0.000005 Tokens per Sec: 10362824, Lr: 0.000343
2020-02-27 01:23:34,856 Epoch 3111: total training loss 0.00054
2020-02-27 01:23:34,857 EPOCH 3112
2020-02-27 01:23:44,704 Epoch 3112: total training loss 0.00053
2020-02-27 01:23:44,705 EPOCH 3113
2020-02-27 01:23:50,722 Epoch 3113 Step:   345500 Batch Loss:     0.000003 Tokens per Sec: 10392040, Lr: 0.000343
2020-02-27 01:23:54,654 Epoch 3113: total training loss 0.00054
2020-02-27 01:23:54,655 EPOCH 3114
2020-02-27 01:24:05,207 Epoch 3114: total training loss 0.00054
2020-02-27 01:24:05,208 EPOCH 3115
2020-02-27 01:24:14,246 Epoch 3115 Step:   345750 Batch Loss:     0.000003 Tokens per Sec:  9760761, Lr: 0.000343
2020-02-27 01:24:15,718 Epoch 3115: total training loss 0.00055
2020-02-27 01:24:15,718 EPOCH 3116
2020-02-27 01:24:26,263 Epoch 3116: total training loss 0.00054
2020-02-27 01:24:26,263 EPOCH 3117
2020-02-27 01:24:36,751 Epoch 3117: total training loss 0.00053
2020-02-27 01:24:36,752 EPOCH 3118
2020-02-27 01:24:38,265 Epoch 3118 Step:   346000 Batch Loss:     0.000006 Tokens per Sec:  9708015, Lr: 0.000343
2020-02-27 01:24:47,157 Epoch 3118: total training loss 0.00053
2020-02-27 01:24:47,159 EPOCH 3119
2020-02-27 01:24:57,282 Epoch 3119: total training loss 0.00053
2020-02-27 01:24:57,283 EPOCH 3120
2020-02-27 01:25:00,966 Epoch 3120 Step:   346250 Batch Loss:     0.000005 Tokens per Sec:  9839650, Lr: 0.000343
2020-02-27 01:25:07,463 Epoch 3120: total training loss 0.00053
2020-02-27 01:25:07,463 EPOCH 3121
2020-02-27 01:25:17,806 Epoch 3121: total training loss 0.00053
2020-02-27 01:25:17,806 EPOCH 3122
2020-02-27 01:25:24,117 Epoch 3122 Step:   346500 Batch Loss:     0.000004 Tokens per Sec: 10025203, Lr: 0.000343
2020-02-27 01:25:28,004 Epoch 3122: total training loss 0.00053
2020-02-27 01:25:28,005 EPOCH 3123
2020-02-27 01:25:37,920 Epoch 3123: total training loss 0.00053
2020-02-27 01:25:37,921 EPOCH 3124
2020-02-27 01:25:46,955 Epoch 3124 Step:   346750 Batch Loss:     0.000003 Tokens per Sec:  9869502, Lr: 0.000343
2020-02-27 01:25:48,359 Epoch 3124: total training loss 0.00054
2020-02-27 01:25:48,359 EPOCH 3125
2020-02-27 01:25:58,842 Epoch 3125: total training loss 0.00053
2020-02-27 01:25:58,843 EPOCH 3126
2020-02-27 01:26:09,116 Epoch 3126: total training loss 0.00055
2020-02-27 01:26:09,116 EPOCH 3127
2020-02-27 01:26:10,583 Epoch 3127 Step:   347000 Batch Loss:     0.000006 Tokens per Sec:  9772205, Lr: 0.000343
2020-02-27 01:26:19,542 Epoch 3127: total training loss 0.00054
2020-02-27 01:26:19,543 EPOCH 3128
2020-02-27 01:26:29,467 Epoch 3128: total training loss 0.00053
2020-02-27 01:26:29,467 EPOCH 3129
2020-02-27 01:26:33,300 Epoch 3129 Step:   347250 Batch Loss:     0.000006 Tokens per Sec: 10190317, Lr: 0.000343
2020-02-27 01:26:39,554 Epoch 3129: total training loss 0.00053
2020-02-27 01:26:39,555 EPOCH 3130
2020-02-27 01:26:49,679 Epoch 3130: total training loss 0.00052
2020-02-27 01:26:49,679 EPOCH 3131
2020-02-27 01:26:55,824 Epoch 3131 Step:   347500 Batch Loss:     0.000005 Tokens per Sec: 10523872, Lr: 0.000343
2020-02-27 01:26:59,407 Epoch 3131: total training loss 0.00054
2020-02-27 01:26:59,407 EPOCH 3132
2020-02-27 01:27:09,310 Epoch 3132: total training loss 0.00055
2020-02-27 01:27:09,310 EPOCH 3133
2020-02-27 01:27:18,116 Epoch 3133 Step:   347750 Batch Loss:     0.000003 Tokens per Sec: 10440233, Lr: 0.000343
2020-02-27 01:27:19,208 Epoch 3133: total training loss 0.00054
2020-02-27 01:27:19,208 EPOCH 3134
2020-02-27 01:27:29,337 Epoch 3134: total training loss 0.00054
2020-02-27 01:27:29,338 EPOCH 3135
2020-02-27 01:27:39,814 Epoch 3135: total training loss 0.00054
2020-02-27 01:27:39,815 EPOCH 3136
2020-02-27 01:27:41,137 Epoch 3136 Step:   348000 Batch Loss:     0.000005 Tokens per Sec:  9436948, Lr: 0.000343
2020-02-27 01:27:50,721 Epoch 3136: total training loss 0.00054
2020-02-27 01:27:50,721 EPOCH 3137
2020-02-27 01:28:01,372 Epoch 3137: total training loss 0.00053
2020-02-27 01:28:01,372 EPOCH 3138
2020-02-27 01:28:05,363 Epoch 3138 Step:   348250 Batch Loss:     0.000004 Tokens per Sec:  9790735, Lr: 0.000343
2020-02-27 01:28:11,992 Epoch 3138: total training loss 0.00053
2020-02-27 01:28:11,992 EPOCH 3139
2020-02-27 01:28:22,441 Epoch 3139: total training loss 0.00053
2020-02-27 01:28:22,442 EPOCH 3140
2020-02-27 01:28:28,872 Epoch 3140 Step:   348500 Batch Loss:     0.000003 Tokens per Sec: 10424388, Lr: 0.000343
2020-02-27 01:28:32,414 Epoch 3140: total training loss 0.00053
2020-02-27 01:28:32,414 EPOCH 3141
2020-02-27 01:28:42,374 Epoch 3141: total training loss 0.00053
2020-02-27 01:28:42,375 EPOCH 3142
2020-02-27 01:28:51,194 Epoch 3142 Step:   348750 Batch Loss:     0.000007 Tokens per Sec: 10403904, Lr: 0.000343
2020-02-27 01:28:52,238 Epoch 3142: total training loss 0.00053
2020-02-27 01:28:52,239 EPOCH 3143
2020-02-27 01:29:02,692 Epoch 3143: total training loss 0.00054
2020-02-27 01:29:02,692 EPOCH 3144
2020-02-27 01:29:13,153 Epoch 3144: total training loss 0.00054
2020-02-27 01:29:13,153 EPOCH 3145
2020-02-27 01:29:14,852 Epoch 3145 Step:   349000 Batch Loss:     0.000006 Tokens per Sec:  9983051, Lr: 0.000343
2020-02-27 01:29:23,696 Epoch 3145: total training loss 0.00057
2020-02-27 01:29:23,697 EPOCH 3146
2020-02-27 01:29:34,159 Epoch 3146: total training loss 0.00054
2020-02-27 01:29:34,160 EPOCH 3147
2020-02-27 01:29:38,208 Epoch 3147 Step:   349250 Batch Loss:     0.000009 Tokens per Sec:  9748033, Lr: 0.000343
2020-02-27 01:29:44,316 Epoch 3147: total training loss 0.00053
2020-02-27 01:29:44,317 EPOCH 3148
2020-02-27 01:29:54,340 Epoch 3148: total training loss 0.00053
2020-02-27 01:29:54,340 EPOCH 3149
2020-02-27 01:30:00,393 Epoch 3149 Step:   349500 Batch Loss:     0.000003 Tokens per Sec: 10261528, Lr: 0.000343
2020-02-27 01:30:04,325 Epoch 3149: total training loss 0.00053
2020-02-27 01:30:04,325 EPOCH 3150
2020-02-27 01:30:14,446 Epoch 3150: total training loss 0.00054
2020-02-27 01:30:14,446 EPOCH 3151
2020-02-27 01:30:23,505 Epoch 3151 Step:   349750 Batch Loss:     0.000004 Tokens per Sec: 10356261, Lr: 0.000343
2020-02-27 01:30:24,503 Epoch 3151: total training loss 0.00054
2020-02-27 01:30:24,503 EPOCH 3152
2020-02-27 01:30:34,370 Epoch 3152: total training loss 0.00055
2020-02-27 01:30:34,371 EPOCH 3153
2020-02-27 01:30:44,517 Epoch 3153: total training loss 0.00054
2020-02-27 01:30:44,518 EPOCH 3154
2020-02-27 01:30:46,073 Epoch 3154 Step:   350000 Batch Loss:     0.000004 Tokens per Sec:  9876138, Lr: 0.000343
2020-02-27 01:30:46,074 Model noise rate: 5
2020-02-27 01:31:51,657 Validation result at epoch 3154, step   350000: Val DTW Score:  10.72, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0427, GT DTW Score:      nan, duration: 65.5831s
2020-02-27 01:32:00,263 Epoch 3154: total training loss 0.00054
2020-02-27 01:32:00,264 EPOCH 3155
2020-02-27 01:32:10,198 Epoch 3155: total training loss 0.00054
2020-02-27 01:32:10,199 EPOCH 3156
2020-02-27 01:32:14,098 Epoch 3156 Step:   350250 Batch Loss:     0.000004 Tokens per Sec: 10429529, Lr: 0.000343
2020-02-27 01:32:19,992 Epoch 3156: total training loss 0.00053
2020-02-27 01:32:19,993 EPOCH 3157
2020-02-27 01:32:30,521 Epoch 3157: total training loss 0.00053
2020-02-27 01:32:30,521 EPOCH 3158
2020-02-27 01:32:37,654 Epoch 3158 Step:   350500 Batch Loss:     0.000002 Tokens per Sec:  9591645, Lr: 0.000343
2020-02-27 01:32:41,101 Epoch 3158: total training loss 0.00053
2020-02-27 01:32:41,101 EPOCH 3159
2020-02-27 01:32:51,584 Epoch 3159: total training loss 0.00053
2020-02-27 01:32:51,585 EPOCH 3160
2020-02-27 01:33:01,202 Epoch 3160 Step:   350750 Batch Loss:     0.000003 Tokens per Sec:  9628803, Lr: 0.000343
2020-02-27 01:33:02,200 Epoch 3160: total training loss 0.00053
2020-02-27 01:33:02,200 EPOCH 3161
2020-02-27 01:33:12,326 Epoch 3161: total training loss 0.00053
2020-02-27 01:33:12,326 EPOCH 3162
2020-02-27 01:33:22,356 Epoch 3162: total training loss 0.00053
2020-02-27 01:33:22,356 EPOCH 3163
2020-02-27 01:33:24,049 Epoch 3163 Step:   351000 Batch Loss:     0.000003 Tokens per Sec: 10162590, Lr: 0.000343
2020-02-27 01:33:32,460 Epoch 3163: total training loss 0.00053
2020-02-27 01:33:32,461 EPOCH 3164
2020-02-27 01:33:42,725 Epoch 3164: total training loss 0.00054
2020-02-27 01:33:42,725 EPOCH 3165
2020-02-27 01:33:46,825 Epoch 3165 Step:   351250 Batch Loss:     0.000003 Tokens per Sec: 10353002, Lr: 0.000343
2020-02-27 01:33:52,744 Epoch 3165: total training loss 0.00055
2020-02-27 01:33:52,744 EPOCH 3166
2020-02-27 01:34:03,030 Epoch 3166: total training loss 0.00054
2020-02-27 01:34:03,030 EPOCH 3167
2020-02-27 01:34:09,682 Epoch 3167 Step:   351500 Batch Loss:     0.000004 Tokens per Sec: 10080660, Lr: 0.000343
2020-02-27 01:34:13,230 Epoch 3167: total training loss 0.00061
2020-02-27 01:34:13,230 EPOCH 3168
2020-02-27 01:34:23,828 Epoch 3168: total training loss 0.00054
2020-02-27 01:34:23,829 EPOCH 3169
2020-02-27 01:34:33,503 Epoch 3169 Step:   351750 Batch Loss:     0.000003 Tokens per Sec:  9653982, Lr: 0.000343
2020-02-27 01:34:34,388 Epoch 3169: total training loss 0.00053
2020-02-27 01:34:34,388 EPOCH 3170
2020-02-27 01:34:44,873 Epoch 3170: total training loss 0.00053
2020-02-27 01:34:44,874 EPOCH 3171
2020-02-27 01:34:55,819 Epoch 3171: total training loss 0.00053
2020-02-27 01:34:55,819 EPOCH 3172
2020-02-27 01:34:57,674 Epoch 3172 Step:   352000 Batch Loss:     0.000004 Tokens per Sec:  9815766, Lr: 0.000343
2020-02-27 01:35:05,924 Epoch 3172: total training loss 0.00053
2020-02-27 01:35:05,925 EPOCH 3173
2020-02-27 01:35:15,913 Epoch 3173: total training loss 0.00053
2020-02-27 01:35:15,914 EPOCH 3174
2020-02-27 01:35:20,095 Epoch 3174 Step:   352250 Batch Loss:     0.000004 Tokens per Sec: 10356646, Lr: 0.000343
2020-02-27 01:35:25,879 Epoch 3174: total training loss 0.00052
2020-02-27 01:35:25,880 EPOCH 3175
2020-02-27 01:35:35,804 Epoch 3175: total training loss 0.00053
2020-02-27 01:35:35,804 EPOCH 3176
2020-02-27 01:35:42,375 Epoch 3176 Step:   352500 Batch Loss:     0.000004 Tokens per Sec: 10279412, Lr: 0.000343
2020-02-27 01:35:45,907 Epoch 3176: total training loss 0.00052
2020-02-27 01:35:45,907 EPOCH 3177
2020-02-27 01:35:56,289 Epoch 3177: total training loss 0.00053
2020-02-27 01:35:56,291 EPOCH 3178
2020-02-27 01:36:06,014 Epoch 3178 Step:   352750 Batch Loss:     0.000003 Tokens per Sec:  9885816, Lr: 0.000343
2020-02-27 01:36:06,752 Epoch 3178: total training loss 0.00053
2020-02-27 01:36:06,752 EPOCH 3179
2020-02-27 01:36:17,000 Epoch 3179: total training loss 0.00053
2020-02-27 01:36:17,000 EPOCH 3180
2020-02-27 01:36:27,458 Epoch 3180: total training loss 0.00053
2020-02-27 01:36:27,459 EPOCH 3181
2020-02-27 01:36:29,380 Epoch 3181 Step:   353000 Batch Loss:     0.000006 Tokens per Sec: 10137662, Lr: 0.000343
2020-02-27 01:36:37,801 Epoch 3181: total training loss 0.00054
2020-02-27 01:36:37,814 EPOCH 3182
2020-02-27 01:36:47,938 Epoch 3182: total training loss 0.00053
2020-02-27 01:36:47,939 EPOCH 3183
2020-02-27 01:36:52,468 Epoch 3183 Step:   353250 Batch Loss:     0.000007 Tokens per Sec:  9678678, Lr: 0.000343
2020-02-27 01:36:58,301 Epoch 3183: total training loss 0.00053
2020-02-27 01:36:58,302 EPOCH 3184
2020-02-27 01:37:08,598 Epoch 3184: total training loss 0.00053
2020-02-27 01:37:08,599 EPOCH 3185
2020-02-27 01:37:15,432 Epoch 3185 Step:   353500 Batch Loss:     0.000005 Tokens per Sec: 10234628, Lr: 0.000343
2020-02-27 01:37:18,596 Epoch 3185: total training loss 0.00056
2020-02-27 01:37:18,596 EPOCH 3186
2020-02-27 01:37:28,452 Epoch 3186: total training loss 0.00053
2020-02-27 01:37:28,494 EPOCH 3187
2020-02-27 01:37:37,984 Epoch 3187 Step:   353750 Batch Loss:     0.000005 Tokens per Sec: 10201601, Lr: 0.000343
2020-02-27 01:37:38,569 Epoch 3187: total training loss 0.00052
2020-02-27 01:37:38,569 EPOCH 3188
2020-02-27 01:37:48,746 Epoch 3188: total training loss 0.00053
2020-02-27 01:37:48,747 EPOCH 3189
2020-02-27 01:37:58,481 Epoch 3189: total training loss 0.00054
2020-02-27 01:37:58,482 EPOCH 3190
2020-02-27 01:38:00,372 Epoch 3190 Step:   354000 Batch Loss:     0.000005 Tokens per Sec:  9789451, Lr: 0.000343
2020-02-27 01:38:08,497 Epoch 3190: total training loss 0.00053
2020-02-27 01:38:08,497 EPOCH 3191
2020-02-27 01:38:18,636 Epoch 3191: total training loss 0.00054
2020-02-27 01:38:18,637 EPOCH 3192
2020-02-27 01:38:23,399 Epoch 3192 Step:   354250 Batch Loss:     0.000003 Tokens per Sec:  9799191, Lr: 0.000343
2020-02-27 01:38:29,323 Epoch 3192: total training loss 0.00055
2020-02-27 01:38:29,323 EPOCH 3193
2020-02-27 01:38:39,849 Epoch 3193: total training loss 0.00053
2020-02-27 01:38:39,850 EPOCH 3194
2020-02-27 01:38:47,051 Epoch 3194 Step:   354500 Batch Loss:     0.000004 Tokens per Sec:  9930317, Lr: 0.000343
2020-02-27 01:38:50,179 Epoch 3194: total training loss 0.00053
2020-02-27 01:38:50,180 EPOCH 3195
2020-02-27 01:39:00,537 Epoch 3195: total training loss 0.00053
2020-02-27 01:39:00,658 EPOCH 3196
2020-02-27 01:39:10,347 Epoch 3196 Step:   354750 Batch Loss:     0.000008 Tokens per Sec: 10081579, Lr: 0.000343
2020-02-27 01:39:10,841 Epoch 3196: total training loss 0.00053
2020-02-27 01:39:10,842 EPOCH 3197
2020-02-27 01:39:20,818 Epoch 3197: total training loss 0.00053
2020-02-27 01:39:20,819 EPOCH 3198
2020-02-27 01:39:30,826 Epoch 3198: total training loss 0.00053
2020-02-27 01:39:30,826 EPOCH 3199
2020-02-27 01:39:32,839 Epoch 3199 Step:   355000 Batch Loss:     0.000005 Tokens per Sec: 10285518, Lr: 0.000343
2020-02-27 01:39:40,933 Epoch 3199: total training loss 0.00053
2020-02-27 01:39:40,933 EPOCH 3200
2020-02-27 01:39:50,745 Epoch 3200: total training loss 0.00053
2020-02-27 01:39:50,745 EPOCH 3201
2020-02-27 01:39:55,310 Epoch 3201 Step:   355250 Batch Loss:     0.000005 Tokens per Sec: 10190355, Lr: 0.000343
2020-02-27 01:40:00,721 Epoch 3201: total training loss 0.00055
2020-02-27 01:40:00,723 EPOCH 3202
2020-02-27 01:40:11,145 Epoch 3202: total training loss 0.00054
2020-02-27 01:40:11,145 EPOCH 3203
2020-02-27 01:40:18,788 Epoch 3203 Step:   355500 Batch Loss:     0.000007 Tokens per Sec:  9682671, Lr: 0.000343
2020-02-27 01:40:21,700 Epoch 3203: total training loss 0.00054
2020-02-27 01:40:21,700 EPOCH 3204
2020-02-27 01:40:32,164 Epoch 3204: total training loss 0.00054
2020-02-27 01:40:32,165 EPOCH 3205
2020-02-27 01:40:41,987 Epoch 3205 Step:   355750 Batch Loss:     0.000004 Tokens per Sec:  9950571, Lr: 0.000343
2020-02-27 01:40:42,486 Epoch 3205: total training loss 0.00053
2020-02-27 01:40:42,487 EPOCH 3206
2020-02-27 01:40:52,504 Epoch 3206: total training loss 0.00053
2020-02-27 01:40:52,504 EPOCH 3207
2020-02-27 01:41:02,432 Epoch 3207: total training loss 0.00053
2020-02-27 01:41:02,433 EPOCH 3208
2020-02-27 01:41:04,553 Epoch 3208 Step:   356000 Batch Loss:     0.000003 Tokens per Sec: 10278573, Lr: 0.000343
2020-02-27 01:41:12,357 Epoch 3208: total training loss 0.00053
2020-02-27 01:41:12,357 EPOCH 3209
2020-02-27 01:41:22,183 Epoch 3209: total training loss 0.00054
2020-02-27 01:41:22,183 EPOCH 3210
2020-02-27 01:41:26,491 Epoch 3210 Step:   356250 Batch Loss:     0.000003 Tokens per Sec: 10541861, Lr: 0.000343
2020-02-27 01:41:31,808 Epoch 3210: total training loss 0.00054
2020-02-27 01:41:31,809 EPOCH 3211
2020-02-27 01:41:41,740 Epoch 3211: total training loss 0.00053
2020-02-27 01:41:41,742 EPOCH 3212
2020-02-27 01:41:49,005 Epoch 3212 Step:   356500 Batch Loss:     0.000004 Tokens per Sec:  9741891, Lr: 0.000343
2020-02-27 01:41:52,320 Epoch 3212: total training loss 0.00053
2020-02-27 01:41:52,320 EPOCH 3213
2020-02-27 01:42:02,989 Epoch 3213: total training loss 0.00053
2020-02-27 01:42:02,989 EPOCH 3214
2020-02-27 01:42:13,296 Epoch 3214 Step:   356750 Batch Loss:     0.000004 Tokens per Sec:  9685291, Lr: 0.000343
2020-02-27 01:42:13,621 Epoch 3214: total training loss 0.00053
2020-02-27 01:42:13,621 EPOCH 3215
2020-02-27 01:42:24,376 Epoch 3215: total training loss 0.00053
2020-02-27 01:42:24,376 EPOCH 3216
2020-02-27 01:42:34,674 Epoch 3216: total training loss 0.00054
2020-02-27 01:42:34,674 EPOCH 3217
2020-02-27 01:42:36,921 Epoch 3217 Step:   357000 Batch Loss:     0.000004 Tokens per Sec: 10008663, Lr: 0.000343
2020-02-27 01:42:44,831 Epoch 3217: total training loss 0.00054
2020-02-27 01:42:44,831 EPOCH 3218
2020-02-27 01:42:54,769 Epoch 3218: total training loss 0.00052
2020-02-27 01:42:54,770 EPOCH 3219
2020-02-27 01:42:59,463 Epoch 3219 Step:   357250 Batch Loss:     0.000003 Tokens per Sec: 10623360, Lr: 0.000343
2020-02-27 01:43:04,595 Epoch 3219: total training loss 0.00054
2020-02-27 01:43:04,596 EPOCH 3220
2020-02-27 01:43:14,658 Epoch 3220: total training loss 0.00054
2020-02-27 01:43:14,659 EPOCH 3221
2020-02-27 01:43:21,822 Epoch 3221 Step:   357500 Batch Loss:     0.000005 Tokens per Sec: 10207311, Lr: 0.000343
2020-02-27 01:43:24,729 Epoch 3221: total training loss 0.00054
2020-02-27 01:43:24,730 EPOCH 3222
2020-02-27 01:43:35,001 Epoch 3222: total training loss 0.00053
2020-02-27 01:43:35,001 EPOCH 3223
2020-02-27 01:43:44,918 Epoch 3223 Step:   357750 Batch Loss:     0.000007 Tokens per Sec:  9941035, Lr: 0.000343
2020-02-27 01:43:45,248 Epoch 3223: total training loss 0.00053
2020-02-27 01:43:45,248 EPOCH 3224
2020-02-27 01:43:55,722 Epoch 3224: total training loss 0.00053
2020-02-27 01:43:55,723 EPOCH 3225
2020-02-27 01:44:05,960 Epoch 3225: total training loss 0.00053
2020-02-27 01:44:05,960 EPOCH 3226
2020-02-27 01:44:08,062 Epoch 3226 Step:   358000 Batch Loss:     0.000008 Tokens per Sec: 10249306, Lr: 0.000343
2020-02-27 01:44:15,707 Epoch 3226: total training loss 0.00053
2020-02-27 01:44:15,707 EPOCH 3227
2020-02-27 01:44:25,671 Epoch 3227: total training loss 0.00054
2020-02-27 01:44:25,671 EPOCH 3228
2020-02-27 01:44:30,358 Epoch 3228 Step:   358250 Batch Loss:     0.000005 Tokens per Sec: 10418293, Lr: 0.000343
2020-02-27 01:44:35,475 Epoch 3228: total training loss 0.00058
2020-02-27 01:44:35,475 EPOCH 3229
2020-02-27 01:44:45,508 Epoch 3229: total training loss 0.00056
2020-02-27 01:44:45,508 EPOCH 3230
2020-02-27 01:44:52,829 Epoch 3230 Step:   358500 Batch Loss:     0.000003 Tokens per Sec: 10118433, Lr: 0.000343
2020-02-27 01:44:55,765 Epoch 3230: total training loss 0.00053
2020-02-27 01:44:55,765 EPOCH 3231
2020-02-27 01:45:06,523 Epoch 3231: total training loss 0.00053
2020-02-27 01:45:06,523 EPOCH 3232
2020-02-27 01:45:16,750 Epoch 3232 Step:   358750 Batch Loss:     0.000003 Tokens per Sec:  9905611, Lr: 0.000343
2020-02-27 01:45:16,937 Epoch 3232: total training loss 0.00052
2020-02-27 01:45:16,938 EPOCH 3233
2020-02-27 01:45:27,561 Epoch 3233: total training loss 0.00053
2020-02-27 01:45:27,561 EPOCH 3234
2020-02-27 01:45:38,192 Epoch 3234: total training loss 0.00052
2020-02-27 01:45:38,193 EPOCH 3235
2020-02-27 01:45:40,943 Epoch 3235 Step:   359000 Batch Loss:     0.000006 Tokens per Sec:  9862704, Lr: 0.000343
2020-02-27 01:45:48,283 Epoch 3235: total training loss 0.00053
2020-02-27 01:45:48,284 EPOCH 3236
2020-02-27 01:45:58,284 Epoch 3236: total training loss 0.00052
2020-02-27 01:45:58,284 EPOCH 3237
2020-02-27 01:46:03,117 Epoch 3237 Step:   359250 Batch Loss:     0.000004 Tokens per Sec: 10250574, Lr: 0.000343
2020-02-27 01:46:08,375 Epoch 3237: total training loss 0.00052
2020-02-27 01:46:08,375 EPOCH 3238
2020-02-27 01:46:18,372 Epoch 3238: total training loss 0.00052
2020-02-27 01:46:18,373 EPOCH 3239
2020-02-27 01:46:25,497 Epoch 3239 Step:   359500 Batch Loss:     0.000007 Tokens per Sec: 10406077, Lr: 0.000343
2020-02-27 01:46:28,240 Epoch 3239: total training loss 0.00053
2020-02-27 01:46:28,241 EPOCH 3240
2020-02-27 01:46:38,143 Epoch 3240: total training loss 0.00053
2020-02-27 01:46:38,144 EPOCH 3241
2020-02-27 01:46:48,424 Epoch 3241 Step:   359750 Batch Loss:     0.000006 Tokens per Sec:  9985603, Lr: 0.000343
2020-02-27 01:46:48,504 Epoch 3241: total training loss 0.00052
2020-02-27 01:46:48,504 EPOCH 3242
2020-02-27 01:46:58,763 Epoch 3242: total training loss 0.00052
2020-02-27 01:46:58,765 EPOCH 3243
2020-02-27 01:47:09,282 Epoch 3243: total training loss 0.00053
2020-02-27 01:47:09,282 EPOCH 3244
2020-02-27 01:47:11,766 Epoch 3244 Step:   360000 Batch Loss:     0.000006 Tokens per Sec:  9967707, Lr: 0.000343
2020-02-27 01:47:11,767 Model noise rate: 5
2020-02-27 01:48:13,645 Validation result at epoch 3244, step   360000: Val DTW Score:  10.76, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0430, GT DTW Score:      nan, duration: 61.8778s
2020-02-27 01:48:21,352 Epoch 3244: total training loss 0.00053
2020-02-27 01:48:21,353 EPOCH 3245
2020-02-27 01:48:32,333 Epoch 3245: total training loss 0.00053
2020-02-27 01:48:32,334 EPOCH 3246
2020-02-27 01:48:37,989 Epoch 3246 Step:   360250 Batch Loss:     0.000007 Tokens per Sec:  9319219, Lr: 0.000343
2020-02-27 01:48:43,479 Epoch 3246: total training loss 0.00053
2020-02-27 01:48:43,480 EPOCH 3247
2020-02-27 01:48:54,365 Epoch 3247: total training loss 0.00058
2020-02-27 01:48:54,365 EPOCH 3248
2020-02-27 01:49:02,562 Epoch 3248 Step:   360500 Batch Loss:     0.000006 Tokens per Sec:  9468593, Lr: 0.000343
2020-02-27 01:49:05,314 Epoch 3248: total training loss 0.00056
2020-02-27 01:49:05,315 EPOCH 3249
2020-02-27 01:49:16,004 Epoch 3249: total training loss 0.00054
2020-02-27 01:49:16,005 EPOCH 3250
2020-02-27 01:49:25,864 Epoch 3250 Step:   360750 Batch Loss:     0.000007 Tokens per Sec: 10359584, Lr: 0.000343
2020-02-27 01:49:25,866 Epoch 3250: total training loss 0.00053
2020-02-27 01:49:25,866 EPOCH 3251
2020-02-27 01:49:36,013 Epoch 3251: total training loss 0.00053
2020-02-27 01:49:36,014 EPOCH 3252
2020-02-27 01:49:46,138 Epoch 3252: total training loss 0.00053
2020-02-27 01:49:46,138 EPOCH 3253
2020-02-27 01:49:48,571 Epoch 3253 Step:   361000 Batch Loss:     0.000005 Tokens per Sec: 10306479, Lr: 0.000343
2020-02-27 01:49:55,969 Epoch 3253: total training loss 0.00053
2020-02-27 01:49:55,969 EPOCH 3254
2020-02-27 01:50:05,824 Epoch 3254: total training loss 0.00052
2020-02-27 01:50:05,825 EPOCH 3255
2020-02-27 01:50:10,854 Epoch 3255 Step:   361250 Batch Loss:     0.000006 Tokens per Sec:  9870571, Lr: 0.000343
2020-02-27 01:50:16,032 Epoch 3255: total training loss 0.00053
2020-02-27 01:50:16,032 EPOCH 3256
2020-02-27 01:50:26,360 Epoch 3256: total training loss 0.00053
2020-02-27 01:50:26,361 EPOCH 3257
2020-02-27 01:50:34,380 Epoch 3257 Step:   361500 Batch Loss:     0.000005 Tokens per Sec:  9892903, Lr: 0.000343
2020-02-27 01:50:36,749 Epoch 3257: total training loss 0.00053
2020-02-27 01:50:36,749 EPOCH 3258
2020-02-27 01:50:47,177 Epoch 3258: total training loss 0.00054
2020-02-27 01:50:47,177 EPOCH 3259
2020-02-27 01:50:57,319 Epoch 3259: total training loss 0.00053
2020-02-27 01:50:57,320 EPOCH 3260
2020-02-27 01:50:57,404 Epoch 3260 Step:   361750 Batch Loss:     0.000003 Tokens per Sec:  7207489, Lr: 0.000343
2020-02-27 01:51:07,056 Epoch 3260: total training loss 0.00054
2020-02-27 01:51:07,057 EPOCH 3261
2020-02-27 01:51:17,203 Epoch 3261: total training loss 0.00054
2020-02-27 01:51:17,204 EPOCH 3262
2020-02-27 01:51:19,769 Epoch 3262 Step:   362000 Batch Loss:     0.000007 Tokens per Sec: 10132817, Lr: 0.000343
2020-02-27 01:51:27,444 Epoch 3262: total training loss 0.00053
2020-02-27 01:51:27,445 EPOCH 3263
2020-02-27 01:51:38,157 Epoch 3263: total training loss 0.00052
2020-02-27 01:51:38,157 EPOCH 3264
2020-02-27 01:51:44,000 Epoch 3264 Step:   362250 Batch Loss:     0.000006 Tokens per Sec:  9419600, Lr: 0.000343
2020-02-27 01:51:49,093 Epoch 3264: total training loss 0.00053
2020-02-27 01:51:49,093 EPOCH 3265
2020-02-27 01:51:59,841 Epoch 3265: total training loss 0.00053
2020-02-27 01:51:59,841 EPOCH 3266
2020-02-27 01:52:07,945 Epoch 3266 Step:   362500 Batch Loss:     0.000008 Tokens per Sec:  9542418, Lr: 0.000343
2020-02-27 01:52:10,421 Epoch 3266: total training loss 0.00062
2020-02-27 01:52:10,421 EPOCH 3267
2020-02-27 01:52:20,838 Epoch 3267: total training loss 0.00056
2020-02-27 01:52:20,839 EPOCH 3268
2020-02-27 01:52:30,836 Epoch 3268: total training loss 0.00053
2020-02-27 01:52:30,837 EPOCH 3269
2020-02-27 01:52:30,989 Epoch 3269 Step:   362750 Batch Loss:     0.000004 Tokens per Sec:  8481879, Lr: 0.000343
2020-02-27 01:52:40,615 Epoch 3269: total training loss 0.00053
2020-02-27 01:52:40,615 EPOCH 3270
2020-02-27 01:52:50,566 Epoch 3270: total training loss 0.00053
2020-02-27 01:52:50,566 EPOCH 3271
2020-02-27 01:52:53,272 Epoch 3271 Step:   363000 Batch Loss:     0.000005 Tokens per Sec: 10204712, Lr: 0.000343
2020-02-27 01:53:00,482 Epoch 3271: total training loss 0.00053
2020-02-27 01:53:00,483 EPOCH 3272
2020-02-27 01:53:10,425 Epoch 3272: total training loss 0.00052
2020-02-27 01:53:10,425 EPOCH 3273
2020-02-27 01:53:15,704 Epoch 3273 Step:   363250 Batch Loss:     0.000003 Tokens per Sec: 10329059, Lr: 0.000343
2020-02-27 01:53:20,557 Epoch 3273: total training loss 0.00052
2020-02-27 01:53:20,557 EPOCH 3274
2020-02-27 01:53:30,817 Epoch 3274: total training loss 0.00052
2020-02-27 01:53:30,817 EPOCH 3275
2020-02-27 01:53:38,843 Epoch 3275 Step:   363500 Batch Loss:     0.000005 Tokens per Sec:  9870915, Lr: 0.000343
2020-02-27 01:53:41,235 Epoch 3275: total training loss 0.00053
2020-02-27 01:53:41,235 EPOCH 3276
2020-02-27 01:53:51,776 Epoch 3276: total training loss 0.00053
2020-02-27 01:53:51,776 EPOCH 3277
2020-02-27 01:54:02,072 Epoch 3277: total training loss 0.00054
2020-02-27 01:54:02,073 EPOCH 3278
2020-02-27 01:54:02,358 Epoch 3278 Step:   363750 Batch Loss:     0.000004 Tokens per Sec:  9486678, Lr: 0.000343
2020-02-27 01:54:12,166 Epoch 3278: total training loss 0.00052
2020-02-27 01:54:12,168 EPOCH 3279
2020-02-27 01:54:22,192 Epoch 3279: total training loss 0.00053
2020-02-27 01:54:22,193 EPOCH 3280
2020-02-27 01:54:25,135 Epoch 3280 Step:   364000 Batch Loss:     0.000004 Tokens per Sec: 10137251, Lr: 0.000343
2020-02-27 01:54:32,307 Epoch 3280: total training loss 0.00053
2020-02-27 01:54:32,308 EPOCH 3281
2020-02-27 01:54:42,379 Epoch 3281: total training loss 0.00052
2020-02-27 01:54:42,379 EPOCH 3282
2020-02-27 01:54:47,568 Epoch 3282 Step:   364250 Batch Loss:     0.000003 Tokens per Sec: 10148387, Lr: 0.000343
2020-02-27 01:54:52,401 Epoch 3282: total training loss 0.00052
2020-02-27 01:54:52,402 EPOCH 3283
2020-02-27 01:55:02,161 Epoch 3283: total training loss 0.00053
2020-02-27 01:55:02,161 EPOCH 3284
2020-02-27 01:55:09,674 Epoch 3284 Step:   364500 Batch Loss:     0.000007 Tokens per Sec: 10741690, Lr: 0.000343
2020-02-27 01:55:11,726 Epoch 3284: total training loss 0.00054
2020-02-27 01:55:11,727 EPOCH 3285
2020-02-27 01:55:21,715 Epoch 3285: total training loss 0.00052
2020-02-27 01:55:21,715 EPOCH 3286
2020-02-27 01:55:31,499 Epoch 3286: total training loss 0.00053
2020-02-27 01:55:31,500 EPOCH 3287
2020-02-27 01:55:31,876 Epoch 3287 Step:   364750 Batch Loss:     0.000003 Tokens per Sec: 10016301, Lr: 0.000343
2020-02-27 01:55:41,383 Epoch 3287: total training loss 0.00052
2020-02-27 01:55:41,384 EPOCH 3288
2020-02-27 01:55:51,146 Epoch 3288: total training loss 0.00053
2020-02-27 01:55:51,146 EPOCH 3289
2020-02-27 01:55:53,942 Epoch 3289 Step:   365000 Batch Loss:     0.000003 Tokens per Sec: 10718806, Lr: 0.000343
2020-02-27 01:56:00,853 Epoch 3289: total training loss 0.00052
2020-02-27 01:56:00,854 EPOCH 3290
2020-02-27 01:56:10,696 Epoch 3290: total training loss 0.00053
2020-02-27 01:56:10,696 EPOCH 3291
2020-02-27 01:56:16,203 Epoch 3291 Step:   365250 Batch Loss:     0.000006 Tokens per Sec: 10472206, Lr: 0.000343
2020-02-27 01:56:20,640 Epoch 3291: total training loss 0.00053
2020-02-27 01:56:20,641 EPOCH 3292
2020-02-27 01:56:30,852 Epoch 3292: total training loss 0.00053
2020-02-27 01:56:30,853 EPOCH 3293
2020-02-27 01:56:39,113 Epoch 3293 Step:   365500 Batch Loss:     0.000007 Tokens per Sec:  9913158, Lr: 0.000343
2020-02-27 01:56:41,267 Epoch 3293: total training loss 0.00052
2020-02-27 01:56:41,267 EPOCH 3294
2020-02-27 01:56:52,217 Epoch 3294: total training loss 0.00054
2020-02-27 01:56:52,217 EPOCH 3295
2020-02-27 01:57:02,913 Epoch 3295: total training loss 0.00055
2020-02-27 01:57:02,915 EPOCH 3296
2020-02-27 01:57:03,497 Epoch 3296 Step:   365750 Batch Loss:     0.000003 Tokens per Sec:  8921608, Lr: 0.000343
2020-02-27 01:57:13,611 Epoch 3296: total training loss 0.00053
2020-02-27 01:57:13,611 EPOCH 3297
2020-02-27 01:57:23,669 Epoch 3297: total training loss 0.00054
2020-02-27 01:57:23,669 EPOCH 3298
2020-02-27 01:57:26,773 Epoch 3298 Step:   366000 Batch Loss:     0.000005 Tokens per Sec: 10210272, Lr: 0.000343
2020-02-27 01:57:33,788 Epoch 3298: total training loss 0.00053
2020-02-27 01:57:33,789 EPOCH 3299
2020-02-27 01:57:43,922 Epoch 3299: total training loss 0.00053
2020-02-27 01:57:43,922 EPOCH 3300
2020-02-27 01:57:49,386 Epoch 3300 Step:   366250 Batch Loss:     0.000005 Tokens per Sec: 10370808, Lr: 0.000343
2020-02-27 01:57:53,926 Epoch 3300: total training loss 0.00053
2020-02-27 01:57:53,927 EPOCH 3301
2020-02-27 01:58:03,772 Epoch 3301: total training loss 0.00053
2020-02-27 01:58:03,773 EPOCH 3302
2020-02-27 01:58:11,796 Epoch 3302 Step:   366500 Batch Loss:     0.000006 Tokens per Sec: 10408494, Lr: 0.000343
2020-02-27 01:58:13,761 Epoch 3302: total training loss 0.00053
2020-02-27 01:58:13,762 EPOCH 3303
2020-02-27 01:58:24,011 Epoch 3303: total training loss 0.00053
2020-02-27 01:58:24,011 EPOCH 3304
2020-02-27 01:58:34,409 Epoch 3304: total training loss 0.00053
2020-02-27 01:58:34,409 EPOCH 3305
2020-02-27 01:58:34,964 Epoch 3305 Step:   366750 Batch Loss:     0.000005 Tokens per Sec:  9284400, Lr: 0.000343
2020-02-27 01:58:44,853 Epoch 3305: total training loss 0.00052
2020-02-27 01:58:44,854 EPOCH 3306
2020-02-27 01:58:55,223 Epoch 3306: total training loss 0.00052
2020-02-27 01:58:55,224 EPOCH 3307
2020-02-27 01:58:58,334 Epoch 3307 Step:   367000 Batch Loss:     0.000004 Tokens per Sec: 10188358, Lr: 0.000343
2020-02-27 01:59:05,334 Epoch 3307: total training loss 0.00053
2020-02-27 01:59:05,335 EPOCH 3308
2020-02-27 01:59:15,299 Epoch 3308: total training loss 0.00055
2020-02-27 01:59:15,299 EPOCH 3309
2020-02-27 01:59:20,919 Epoch 3309 Step:   367250 Batch Loss:     0.000003 Tokens per Sec: 10239400, Lr: 0.000343
2020-02-27 01:59:25,363 Epoch 3309: total training loss 0.00056
2020-02-27 01:59:25,364 EPOCH 3310
2020-02-27 01:59:35,445 Epoch 3310: total training loss 0.00053
2020-02-27 01:59:35,445 EPOCH 3311
2020-02-27 01:59:43,458 Epoch 3311 Step:   367500 Batch Loss:     0.000004 Tokens per Sec: 10374945, Lr: 0.000343
2020-02-27 01:59:45,402 Epoch 3311: total training loss 0.00055
2020-02-27 01:59:45,402 EPOCH 3312
2020-02-27 01:59:55,277 Epoch 3312: total training loss 0.00053
2020-02-27 01:59:55,278 EPOCH 3313
2020-02-27 02:00:05,768 Epoch 3313: total training loss 0.00052
2020-02-27 02:00:05,770 EPOCH 3314
2020-02-27 02:00:06,502 Epoch 3314 Step:   367750 Batch Loss:     0.000003 Tokens per Sec:  9318417, Lr: 0.000343
2020-02-27 02:00:16,307 Epoch 3314: total training loss 0.00052
2020-02-27 02:00:16,309 EPOCH 3315
2020-02-27 02:00:26,839 Epoch 3315: total training loss 0.00053
2020-02-27 02:00:26,840 EPOCH 3316
2020-02-27 02:00:30,351 Epoch 3316 Step:   368000 Batch Loss:     0.000006 Tokens per Sec:  9647252, Lr: 0.000343
2020-02-27 02:00:37,246 Epoch 3316: total training loss 0.00053
2020-02-27 02:00:37,246 EPOCH 3317
2020-02-27 02:00:47,541 Epoch 3317: total training loss 0.00052
2020-02-27 02:00:47,542 EPOCH 3318
2020-02-27 02:00:53,071 Epoch 3318 Step:   368250 Batch Loss:     0.000004 Tokens per Sec: 10485341, Lr: 0.000343
2020-02-27 02:00:57,379 Epoch 3318: total training loss 0.00053
2020-02-27 02:00:57,379 EPOCH 3319
2020-02-27 02:01:07,313 Epoch 3319: total training loss 0.00052
2020-02-27 02:01:07,313 EPOCH 3320
2020-02-27 02:01:15,371 Epoch 3320 Step:   368500 Batch Loss:     0.000003 Tokens per Sec: 10482541, Lr: 0.000343
2020-02-27 02:01:17,073 Epoch 3320: total training loss 0.00052
2020-02-27 02:01:17,073 EPOCH 3321
2020-02-27 02:01:26,819 Epoch 3321: total training loss 0.00052
2020-02-27 02:01:26,820 EPOCH 3322
2020-02-27 02:01:36,685 Epoch 3322: total training loss 0.00053
2020-02-27 02:01:36,688 EPOCH 3323
2020-02-27 02:01:37,439 Epoch 3323 Step:   368750 Batch Loss:     0.000007 Tokens per Sec:  9713593, Lr: 0.000343
2020-02-27 02:01:47,030 Epoch 3323: total training loss 0.00052
2020-02-27 02:01:47,030 EPOCH 3324
2020-02-27 02:01:57,372 Epoch 3324: total training loss 0.00052
2020-02-27 02:01:57,373 EPOCH 3325
2020-02-27 02:02:00,573 Epoch 3325 Step:   369000 Batch Loss:     0.000007 Tokens per Sec:  9596295, Lr: 0.000343
2020-02-27 02:02:07,765 Epoch 3325: total training loss 0.00053
2020-02-27 02:02:07,765 EPOCH 3326
2020-02-27 02:02:18,297 Epoch 3326: total training loss 0.00054
2020-02-27 02:02:18,297 EPOCH 3327
2020-02-27 02:02:24,197 Epoch 3327 Step:   369250 Batch Loss:     0.000005 Tokens per Sec: 10269268, Lr: 0.000343
2020-02-27 02:02:28,375 Epoch 3327: total training loss 0.00058
2020-02-27 02:02:28,376 EPOCH 3328
2020-02-27 02:02:38,523 Epoch 3328: total training loss 0.00053
2020-02-27 02:02:38,525 EPOCH 3329
2020-02-27 02:02:46,861 Epoch 3329 Step:   369500 Batch Loss:     0.000005 Tokens per Sec: 10033116, Lr: 0.000343
2020-02-27 02:02:48,744 Epoch 3329: total training loss 0.00052
2020-02-27 02:02:48,744 EPOCH 3330
2020-02-27 02:02:58,951 Epoch 3330: total training loss 0.00052
2020-02-27 02:02:58,951 EPOCH 3331
2020-02-27 02:03:08,946 Epoch 3331: total training loss 0.00052
2020-02-27 02:03:08,947 EPOCH 3332
2020-02-27 02:03:09,793 Epoch 3332 Step:   369750 Batch Loss:     0.000005 Tokens per Sec: 10029972, Lr: 0.000343
2020-02-27 02:03:19,142 Epoch 3332: total training loss 0.00052
2020-02-27 02:03:19,143 EPOCH 3333
2020-02-27 02:03:29,565 Epoch 3333: total training loss 0.00052
2020-02-27 02:03:29,565 EPOCH 3334
2020-02-27 02:03:32,965 Epoch 3334 Step:   370000 Batch Loss:     0.000006 Tokens per Sec:  9884608, Lr: 0.000343
2020-02-27 02:03:32,966 Model noise rate: 5
2020-02-27 02:04:34,368 Validation result at epoch 3334, step   370000: Val DTW Score:  10.75, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0430, GT DTW Score:      nan, duration: 61.4018s
2020-02-27 02:04:41,096 Epoch 3334: total training loss 0.00054
2020-02-27 02:04:41,097 EPOCH 3335
2020-02-27 02:04:50,830 Epoch 3335: total training loss 0.00053
2020-02-27 02:04:50,830 EPOCH 3336
2020-02-27 02:04:56,570 Epoch 3336 Step:   370250 Batch Loss:     0.000004 Tokens per Sec: 10611850, Lr: 0.000343
2020-02-27 02:05:00,608 Epoch 3336: total training loss 0.00054
2020-02-27 02:05:00,609 EPOCH 3337
2020-02-27 02:05:10,773 Epoch 3337: total training loss 0.00053
2020-02-27 02:05:10,775 EPOCH 3338
2020-02-27 02:05:19,664 Epoch 3338 Step:   370500 Batch Loss:     0.000007 Tokens per Sec:  9733149, Lr: 0.000343
2020-02-27 02:05:21,307 Epoch 3338: total training loss 0.00054
2020-02-27 02:05:21,307 EPOCH 3339
2020-02-27 02:05:32,078 Epoch 3339: total training loss 0.00054
2020-02-27 02:05:32,079 EPOCH 3340
2020-02-27 02:05:42,996 Epoch 3340: total training loss 0.00053
2020-02-27 02:05:42,997 EPOCH 3341
2020-02-27 02:05:44,099 Epoch 3341 Step:   370750 Batch Loss:     0.000005 Tokens per Sec:  9826994, Lr: 0.000343
2020-02-27 02:05:53,798 Epoch 3341: total training loss 0.00054
2020-02-27 02:05:53,799 EPOCH 3342
2020-02-27 02:06:03,787 Epoch 3342: total training loss 0.00053
2020-02-27 02:06:03,787 EPOCH 3343
2020-02-27 02:06:07,082 Epoch 3343 Step:   371000 Batch Loss:     0.000004 Tokens per Sec: 10140216, Lr: 0.000343
2020-02-27 02:06:13,749 Epoch 3343: total training loss 0.00052
2020-02-27 02:06:13,750 EPOCH 3344
2020-02-27 02:06:23,883 Epoch 3344: total training loss 0.00052
2020-02-27 02:06:23,884 EPOCH 3345
2020-02-27 02:06:29,821 Epoch 3345 Step:   371250 Batch Loss:     0.000005 Tokens per Sec: 10267510, Lr: 0.000343
2020-02-27 02:06:33,751 Epoch 3345: total training loss 0.00053
2020-02-27 02:06:33,751 EPOCH 3346
2020-02-27 02:06:43,494 Epoch 3346: total training loss 0.00053
2020-02-27 02:06:43,495 EPOCH 3347
2020-02-27 02:06:51,761 Epoch 3347 Step:   371500 Batch Loss:     0.000003 Tokens per Sec: 10532651, Lr: 0.000343
2020-02-27 02:06:53,194 Epoch 3347: total training loss 0.00052
2020-02-27 02:06:53,195 EPOCH 3348
2020-02-27 02:07:03,301 Epoch 3348: total training loss 0.00052
2020-02-27 02:07:03,305 EPOCH 3349
2020-02-27 02:07:13,657 Epoch 3349: total training loss 0.00053
2020-02-27 02:07:13,658 EPOCH 3350
2020-02-27 02:07:14,651 Epoch 3350 Step:   371750 Batch Loss:     0.000005 Tokens per Sec:  9906455, Lr: 0.000343
2020-02-27 02:07:23,871 Epoch 3350: total training loss 0.00052
2020-02-27 02:07:23,872 EPOCH 3351
2020-02-27 02:07:34,028 Epoch 3351: total training loss 0.00053
2020-02-27 02:07:34,029 EPOCH 3352
2020-02-27 02:07:37,421 Epoch 3352 Step:   372000 Batch Loss:     0.000004 Tokens per Sec:  9719677, Lr: 0.000343
2020-02-27 02:07:44,612 Epoch 3352: total training loss 0.00052
2020-02-27 02:07:44,613 EPOCH 3353
2020-02-27 02:07:54,940 Epoch 3353: total training loss 0.00054
2020-02-27 02:07:54,940 EPOCH 3354
2020-02-27 02:08:01,012 Epoch 3354 Step:   372250 Batch Loss:     0.000006 Tokens per Sec: 10166904, Lr: 0.000343
2020-02-27 02:08:05,180 Epoch 3354: total training loss 0.00052
2020-02-27 02:08:05,180 EPOCH 3355
2020-02-27 02:08:15,423 Epoch 3355: total training loss 0.00053
2020-02-27 02:08:15,424 EPOCH 3356
2020-02-27 02:08:24,073 Epoch 3356 Step:   372500 Batch Loss:     0.000004 Tokens per Sec: 10132779, Lr: 0.000343
2020-02-27 02:08:25,602 Epoch 3356: total training loss 0.00054
2020-02-27 02:08:25,603 EPOCH 3357
2020-02-27 02:08:35,505 Epoch 3357: total training loss 0.00058
2020-02-27 02:08:35,505 EPOCH 3358
2020-02-27 02:08:45,394 Epoch 3358: total training loss 0.00054
2020-02-27 02:08:45,396 EPOCH 3359
2020-02-27 02:08:46,615 Epoch 3359 Step:   372750 Batch Loss:     0.000007 Tokens per Sec:  9922347, Lr: 0.000343
2020-02-27 02:08:55,607 Epoch 3359: total training loss 0.00054
2020-02-27 02:08:55,608 EPOCH 3360
2020-02-27 02:09:06,024 Epoch 3360: total training loss 0.00053
2020-02-27 02:09:06,024 EPOCH 3361
2020-02-27 02:09:09,711 Epoch 3361 Step:   373000 Batch Loss:     0.000006 Tokens per Sec:  9833095, Lr: 0.000343
2020-02-27 02:09:16,474 Epoch 3361: total training loss 0.00052
2020-02-27 02:09:16,475 EPOCH 3362
2020-02-27 02:09:26,933 Epoch 3362: total training loss 0.00054
2020-02-27 02:09:26,933 EPOCH 3363
2020-02-27 02:09:33,089 Epoch 3363 Step:   373250 Batch Loss:     0.000005 Tokens per Sec: 10104968, Lr: 0.000343
2020-02-27 02:09:36,991 Epoch 3363: total training loss 0.00057
2020-02-27 02:09:36,991 EPOCH 3364
2020-02-27 02:09:46,769 Epoch 3364: total training loss 0.00053
2020-02-27 02:09:46,769 EPOCH 3365
2020-02-27 02:09:55,201 Epoch 3365 Step:   373500 Batch Loss:     0.000003 Tokens per Sec: 10396863, Lr: 0.000343
2020-02-27 02:09:56,576 Epoch 3365: total training loss 0.00055
2020-02-27 02:09:56,576 EPOCH 3366
2020-02-27 02:10:06,489 Epoch 3366: total training loss 0.00053
2020-02-27 02:10:06,489 EPOCH 3367
2020-02-27 02:10:16,442 Epoch 3367: total training loss 0.00053
2020-02-27 02:10:16,444 EPOCH 3368
2020-02-27 02:10:17,638 Epoch 3368 Step:   373750 Batch Loss:     0.000004 Tokens per Sec: 10182872, Lr: 0.000343
2020-02-27 02:10:27,009 Epoch 3368: total training loss 0.00052
2020-02-27 02:10:27,010 EPOCH 3369
2020-02-27 02:10:37,563 Epoch 3369: total training loss 0.00053
2020-02-27 02:10:37,563 EPOCH 3370
2020-02-27 02:10:41,739 Epoch 3370 Step:   374000 Batch Loss:     0.000006 Tokens per Sec:  9487810, Lr: 0.000343
2020-02-27 02:10:48,275 Epoch 3370: total training loss 0.00052
2020-02-27 02:10:48,276 EPOCH 3371
2020-02-27 02:10:59,158 Epoch 3371: total training loss 0.00052
2020-02-27 02:10:59,159 EPOCH 3372
2020-02-27 02:11:05,776 Epoch 3372 Step:   374250 Batch Loss:     0.000006 Tokens per Sec:  9755783, Lr: 0.000343
2020-02-27 02:11:09,512 Epoch 3372: total training loss 0.00052
2020-02-27 02:11:09,512 EPOCH 3373
2020-02-27 02:11:19,614 Epoch 3373: total training loss 0.00052
2020-02-27 02:11:19,614 EPOCH 3374
2020-02-27 02:11:28,315 Epoch 3374 Step:   374500 Batch Loss:     0.000004 Tokens per Sec: 10262137, Lr: 0.000343
2020-02-27 02:11:29,548 Epoch 3374: total training loss 0.00052
2020-02-27 02:11:29,549 EPOCH 3375
2020-02-27 02:11:39,481 Epoch 3375: total training loss 0.00052
2020-02-27 02:11:39,481 EPOCH 3376
2020-02-27 02:11:49,345 Epoch 3376: total training loss 0.00052
2020-02-27 02:11:49,346 EPOCH 3377
2020-02-27 02:11:50,582 Epoch 3377 Step:   374750 Batch Loss:     0.000009 Tokens per Sec:  9853724, Lr: 0.000343
2020-02-27 02:11:59,247 Epoch 3377: total training loss 0.00052
2020-02-27 02:11:59,248 EPOCH 3378
2020-02-27 02:12:09,140 Epoch 3378: total training loss 0.00052
2020-02-27 02:12:09,141 EPOCH 3379
2020-02-27 02:12:12,965 Epoch 3379 Step:   375000 Batch Loss:     0.000005 Tokens per Sec: 10034820, Lr: 0.000343
2020-02-27 02:12:19,331 Epoch 3379: total training loss 0.00053
2020-02-27 02:12:19,332 EPOCH 3380
2020-02-27 02:12:29,744 Epoch 3380: total training loss 0.00053
2020-02-27 02:12:29,745 EPOCH 3381
2020-02-27 02:12:36,262 Epoch 3381 Step:   375250 Batch Loss:     0.000003 Tokens per Sec:  9888313, Lr: 0.000343
2020-02-27 02:12:40,010 Epoch 3381: total training loss 0.00053
2020-02-27 02:12:40,010 EPOCH 3382
2020-02-27 02:12:50,339 Epoch 3382: total training loss 0.00053
2020-02-27 02:12:50,341 EPOCH 3383
2020-02-27 02:12:59,418 Epoch 3383 Step:   375500 Batch Loss:     0.000005 Tokens per Sec:  9861036, Lr: 0.000343
2020-02-27 02:13:00,685 Epoch 3383: total training loss 0.00052
2020-02-27 02:13:00,686 EPOCH 3384
2020-02-27 02:13:10,889 Epoch 3384: total training loss 0.00052
2020-02-27 02:13:10,889 EPOCH 3385
2020-02-27 02:13:21,042 Epoch 3385: total training loss 0.00052
2020-02-27 02:13:21,042 EPOCH 3386
2020-02-27 02:13:22,296 Epoch 3386 Step:   375750 Batch Loss:     0.000003 Tokens per Sec:  9942722, Lr: 0.000343
2020-02-27 02:13:31,180 Epoch 3386: total training loss 0.00054
2020-02-27 02:13:31,181 EPOCH 3387
2020-02-27 02:13:41,288 Epoch 3387: total training loss 0.00052
2020-02-27 02:13:41,288 EPOCH 3388
2020-02-27 02:13:45,145 Epoch 3388 Step:   376000 Batch Loss:     0.000004 Tokens per Sec: 10361319, Lr: 0.000343
2020-02-27 02:13:51,431 Epoch 3388: total training loss 0.00054
2020-02-27 02:13:51,432 EPOCH 3389
2020-02-27 02:14:02,031 Epoch 3389: total training loss 0.00053
2020-02-27 02:14:02,032 EPOCH 3390
2020-02-27 02:14:08,732 Epoch 3390 Step:   376250 Batch Loss:     0.000004 Tokens per Sec:  9963889, Lr: 0.000343
2020-02-27 02:14:12,471 Epoch 3390: total training loss 0.00056
2020-02-27 02:14:12,472 EPOCH 3391
2020-02-27 02:14:22,684 Epoch 3391: total training loss 0.00057
2020-02-27 02:14:22,684 EPOCH 3392
2020-02-27 02:14:31,911 Epoch 3392 Step:   376500 Batch Loss:     0.000005 Tokens per Sec:  9911622, Lr: 0.000343
2020-02-27 02:14:33,088 Epoch 3392: total training loss 0.00053
2020-02-27 02:14:33,088 EPOCH 3393
2020-02-27 02:14:43,227 Epoch 3393: total training loss 0.00054
2020-02-27 02:14:43,227 EPOCH 3394
2020-02-27 02:14:53,088 Epoch 3394: total training loss 0.00053
2020-02-27 02:14:53,088 EPOCH 3395
2020-02-27 02:14:54,754 Epoch 3395 Step:   376750 Batch Loss:     0.000003 Tokens per Sec: 10531037, Lr: 0.000343
2020-02-27 02:15:03,185 Epoch 3395: total training loss 0.00052
2020-02-27 02:15:03,186 EPOCH 3396
2020-02-27 02:15:13,061 Epoch 3396: total training loss 0.00052
2020-02-27 02:15:13,062 EPOCH 3397
2020-02-27 02:15:16,886 Epoch 3397 Step:   377000 Batch Loss:     0.000005 Tokens per Sec: 10219354, Lr: 0.000343
2020-02-27 02:15:22,916 Epoch 3397: total training loss 0.00053
2020-02-27 02:15:22,916 EPOCH 3398
2020-02-27 02:15:32,852 Epoch 3398: total training loss 0.00052
2020-02-27 02:15:32,853 EPOCH 3399
2020-02-27 02:15:39,110 Epoch 3399 Step:   377250 Batch Loss:     0.000007 Tokens per Sec: 10300613, Lr: 0.000343
2020-02-27 02:15:42,808 Epoch 3399: total training loss 0.00052
2020-02-27 02:15:42,809 EPOCH 3400
2020-02-27 02:15:53,165 Epoch 3400: total training loss 0.00052
2020-02-27 02:15:53,166 EPOCH 3401
2020-02-27 02:16:02,779 Epoch 3401 Step:   377500 Batch Loss:     0.000005 Tokens per Sec:  9582895, Lr: 0.000343
2020-02-27 02:16:03,830 Epoch 3401: total training loss 0.00052
2020-02-27 02:16:03,830 EPOCH 3402
2020-02-27 02:16:14,311 Epoch 3402: total training loss 0.00053
2020-02-27 02:16:14,311 EPOCH 3403
2020-02-27 02:16:24,477 Epoch 3403: total training loss 0.00053
2020-02-27 02:16:24,478 EPOCH 3404
2020-02-27 02:16:25,962 Epoch 3404 Step:   377750 Batch Loss:     0.000005 Tokens per Sec: 10241405, Lr: 0.000343
2020-02-27 02:16:34,359 Epoch 3404: total training loss 0.00053
2020-02-27 02:16:34,360 EPOCH 3405
2020-02-27 02:16:44,307 Epoch 3405: total training loss 0.00054
2020-02-27 02:16:44,308 EPOCH 3406
2020-02-27 02:16:48,256 Epoch 3406 Step:   378000 Batch Loss:     0.000004 Tokens per Sec: 10699328, Lr: 0.000343
2020-02-27 02:16:54,386 Epoch 3406: total training loss 0.00053
2020-02-27 02:16:54,386 EPOCH 3407
2020-02-27 02:17:04,642 Epoch 3407: total training loss 0.00052
2020-02-27 02:17:04,644 EPOCH 3408
2020-02-27 02:17:11,364 Epoch 3408 Step:   378250 Batch Loss:     0.000004 Tokens per Sec:  9967481, Lr: 0.000343
2020-02-27 02:17:14,784 Epoch 3408: total training loss 0.00052
2020-02-27 02:17:14,784 EPOCH 3409
2020-02-27 02:17:24,844 Epoch 3409: total training loss 0.00052
2020-02-27 02:17:24,844 EPOCH 3410
2020-02-27 02:17:33,904 Epoch 3410 Step:   378500 Batch Loss:     0.000003 Tokens per Sec: 10221267, Lr: 0.000343
2020-02-27 02:17:34,874 Epoch 3410: total training loss 0.00052
2020-02-27 02:17:34,874 EPOCH 3411
2020-02-27 02:17:44,952 Epoch 3411: total training loss 0.00052
2020-02-27 02:17:44,953 EPOCH 3412
2020-02-27 02:17:54,845 Epoch 3412: total training loss 0.00054
2020-02-27 02:17:54,845 EPOCH 3413
2020-02-27 02:17:56,522 Epoch 3413 Step:   378750 Batch Loss:     0.000003 Tokens per Sec: 10122631, Lr: 0.000343
2020-02-27 02:18:04,696 Epoch 3413: total training loss 0.00054
2020-02-27 02:18:04,697 EPOCH 3414
2020-02-27 02:18:14,479 Epoch 3414: total training loss 0.00053
2020-02-27 02:18:14,481 EPOCH 3415
2020-02-27 02:18:18,652 Epoch 3415 Step:   379000 Batch Loss:     0.000004 Tokens per Sec: 10408168, Lr: 0.000343
2020-02-27 02:18:24,446 Epoch 3415: total training loss 0.00052
2020-02-27 02:18:24,447 EPOCH 3416
2020-02-27 02:18:35,005 Epoch 3416: total training loss 0.00053
2020-02-27 02:18:35,006 EPOCH 3417
2020-02-27 02:18:41,788 Epoch 3417 Step:   379250 Batch Loss:     0.000003 Tokens per Sec:  9595210, Lr: 0.000343
2020-02-27 02:18:45,539 Epoch 3417: total training loss 0.00054
2020-02-27 02:18:45,539 EPOCH 3418
2020-02-27 02:18:56,399 Epoch 3418: total training loss 0.00053
2020-02-27 02:18:56,400 EPOCH 3419
2020-02-27 02:19:05,939 Epoch 3419 Step:   379500 Batch Loss:     0.000003 Tokens per Sec:  9830482, Lr: 0.000343
2020-02-27 02:19:06,835 Epoch 3419: total training loss 0.00052
2020-02-27 02:19:06,836 EPOCH 3420
2020-02-27 02:19:16,984 Epoch 3420: total training loss 0.00052
2020-02-27 02:19:16,984 EPOCH 3421
2020-02-27 02:19:26,935 Epoch 3421: total training loss 0.00052
2020-02-27 02:19:26,935 EPOCH 3422
2020-02-27 02:19:28,706 Epoch 3422 Step:   379750 Batch Loss:     0.000007 Tokens per Sec: 10059246, Lr: 0.000343
2020-02-27 02:19:37,063 Epoch 3422: total training loss 0.00052
2020-02-27 02:19:37,063 EPOCH 3423
2020-02-27 02:19:46,907 Epoch 3423: total training loss 0.00052
2020-02-27 02:19:46,907 EPOCH 3424
2020-02-27 02:19:51,072 Epoch 3424 Step:   380000 Batch Loss:     0.000005 Tokens per Sec: 10309928, Lr: 0.000343
2020-02-27 02:19:51,072 Model noise rate: 5
2020-02-27 02:20:52,957 Validation result at epoch 3424, step   380000: Val DTW Score:  10.74, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0430, GT DTW Score:      nan, duration: 61.8847s
2020-02-27 02:20:58,762 Epoch 3424: total training loss 0.00053
2020-02-27 02:20:58,763 EPOCH 3425
2020-02-27 02:21:08,605 Epoch 3425: total training loss 0.00053
2020-02-27 02:21:08,605 EPOCH 3426
2020-02-27 02:21:15,242 Epoch 3426 Step:   380250 Batch Loss:     0.000003 Tokens per Sec: 10440877, Lr: 0.000343
2020-02-27 02:21:18,503 Epoch 3426: total training loss 0.00052
2020-02-27 02:21:18,503 EPOCH 3427
2020-02-27 02:21:28,572 Epoch 3427: total training loss 0.00054
2020-02-27 02:21:28,573 EPOCH 3428
2020-02-27 02:21:37,693 Epoch 3428 Step:   380500 Batch Loss:     0.000005 Tokens per Sec: 10345897, Lr: 0.000343
2020-02-27 02:21:38,479 Epoch 3428: total training loss 0.00052
2020-02-27 02:21:38,479 EPOCH 3429
2020-02-27 02:21:48,484 Epoch 3429: total training loss 0.00052
2020-02-27 02:21:48,485 EPOCH 3430
2020-02-27 02:21:58,509 Epoch 3430: total training loss 0.00052
2020-02-27 02:21:58,511 EPOCH 3431
2020-02-27 02:22:00,534 Epoch 3431 Step:   380750 Batch Loss:     0.000004 Tokens per Sec: 10138956, Lr: 0.000343
2020-02-27 02:22:09,099 Epoch 3431: total training loss 0.00052
2020-02-27 02:22:09,100 EPOCH 3432
2020-02-27 02:22:19,883 Epoch 3432: total training loss 0.00054
2020-02-27 02:22:19,883 EPOCH 3433
2020-02-27 02:22:24,531 Epoch 3433 Step:   381000 Batch Loss:     0.000004 Tokens per Sec:  9628551, Lr: 0.000343
2020-02-27 02:22:30,411 Epoch 3433: total training loss 0.00053
2020-02-27 02:22:30,411 EPOCH 3434
2020-02-27 02:22:40,865 Epoch 3434: total training loss 0.00053
2020-02-27 02:22:40,866 EPOCH 3435
2020-02-27 02:22:48,155 Epoch 3435 Step:   381250 Batch Loss:     0.000006 Tokens per Sec:  9843811, Lr: 0.000343
2020-02-27 02:22:51,310 Epoch 3435: total training loss 0.00052
2020-02-27 02:22:51,310 EPOCH 3436
2020-02-27 02:23:01,103 Epoch 3436: total training loss 0.00052
2020-02-27 02:23:01,103 EPOCH 3437
2020-02-27 02:23:10,329 Epoch 3437 Step:   381500 Batch Loss:     0.000005 Tokens per Sec: 10585363, Lr: 0.000343
2020-02-27 02:23:10,916 Epoch 3437: total training loss 0.00051
2020-02-27 02:23:10,917 EPOCH 3438
2020-02-27 02:23:20,773 Epoch 3438: total training loss 0.00052
2020-02-27 02:23:20,774 EPOCH 3439
2020-02-27 02:23:30,700 Epoch 3439: total training loss 0.00053
2020-02-27 02:23:30,701 EPOCH 3440
2020-02-27 02:23:32,640 Epoch 3440 Step:   381750 Batch Loss:     0.000006 Tokens per Sec: 10329890, Lr: 0.000343
2020-02-27 02:23:40,720 Epoch 3440: total training loss 0.00053
2020-02-27 02:23:40,721 EPOCH 3441
2020-02-27 02:23:51,069 Epoch 3441: total training loss 0.00055
2020-02-27 02:23:51,069 EPOCH 3442
2020-02-27 02:23:55,522 Epoch 3442 Step:   382000 Batch Loss:     0.000004 Tokens per Sec:  9762760, Lr: 0.000343
2020-02-27 02:24:01,341 Epoch 3442: total training loss 0.00054
2020-02-27 02:24:01,341 EPOCH 3443
2020-02-27 02:24:11,838 Epoch 3443: total training loss 0.00052
2020-02-27 02:24:11,840 EPOCH 3444
2020-02-27 02:24:19,497 Epoch 3444 Step:   382250 Batch Loss:     0.000008 Tokens per Sec:  9190839, Lr: 0.000343
2020-02-27 02:24:22,877 Epoch 3444: total training loss 0.00052
2020-02-27 02:24:22,877 EPOCH 3445
2020-02-27 02:24:33,180 Epoch 3445: total training loss 0.00052
2020-02-27 02:24:33,180 EPOCH 3446
2020-02-27 02:24:43,023 Epoch 3446 Step:   382500 Batch Loss:     0.000004 Tokens per Sec:  9896129, Lr: 0.000343
2020-02-27 02:24:43,601 Epoch 3446: total training loss 0.00051
2020-02-27 02:24:43,602 EPOCH 3447
2020-02-27 02:24:53,657 Epoch 3447: total training loss 0.00052
2020-02-27 02:24:53,657 EPOCH 3448
2020-02-27 02:25:03,560 Epoch 3448: total training loss 0.00052
2020-02-27 02:25:03,560 EPOCH 3449
2020-02-27 02:25:05,523 Epoch 3449 Step:   382750 Batch Loss:     0.000004 Tokens per Sec: 10497461, Lr: 0.000343
2020-02-27 02:25:13,328 Epoch 3449: total training loss 0.00052
2020-02-27 02:25:13,329 EPOCH 3450
2020-02-27 02:25:23,374 Epoch 3450: total training loss 0.00054
2020-02-27 02:25:23,374 EPOCH 3451
2020-02-27 02:25:28,130 Epoch 3451 Step:   383000 Batch Loss:     0.000003 Tokens per Sec:  9975569, Lr: 0.000343
2020-02-27 02:25:33,754 Epoch 3451: total training loss 0.00058
2020-02-27 02:25:33,754 EPOCH 3452
2020-02-27 02:25:44,035 Epoch 3452: total training loss 0.00053
2020-02-27 02:25:44,036 EPOCH 3453
2020-02-27 02:25:51,339 Epoch 3453 Step:   383250 Batch Loss:     0.000005 Tokens per Sec: 10167913, Lr: 0.000343
2020-02-27 02:25:54,209 Epoch 3453: total training loss 0.00052
2020-02-27 02:25:54,210 EPOCH 3454
2020-02-27 02:26:04,537 Epoch 3454: total training loss 0.00052
2020-02-27 02:26:04,537 EPOCH 3455
2020-02-27 02:26:14,131 Epoch 3455 Step:   383500 Batch Loss:     0.000005 Tokens per Sec: 10308556, Lr: 0.000343
2020-02-27 02:26:14,547 Epoch 3455: total training loss 0.00054
2020-02-27 02:26:14,547 EPOCH 3456
2020-02-27 02:26:24,335 Epoch 3456: total training loss 0.00052
2020-02-27 02:26:24,335 EPOCH 3457
2020-02-27 02:26:34,210 Epoch 3457: total training loss 0.00051
2020-02-27 02:26:34,211 EPOCH 3458
2020-02-27 02:26:36,312 Epoch 3458 Step:   383750 Batch Loss:     0.000006 Tokens per Sec: 10189144, Lr: 0.000343
2020-02-27 02:26:44,204 Epoch 3458: total training loss 0.00051
2020-02-27 02:26:44,205 EPOCH 3459
2020-02-27 02:26:53,899 Epoch 3459: total training loss 0.00051
2020-02-27 02:26:53,902 EPOCH 3460
2020-02-27 02:26:58,398 Epoch 3460 Step:   384000 Batch Loss:     0.000005 Tokens per Sec: 10270507, Lr: 0.000343
2020-02-27 02:27:03,879 Epoch 3460: total training loss 0.00052
2020-02-27 02:27:03,880 EPOCH 3461
2020-02-27 02:27:14,109 Epoch 3461: total training loss 0.00053
2020-02-27 02:27:14,110 EPOCH 3462
2020-02-27 02:27:21,532 Epoch 3462 Step:   384250 Batch Loss:     0.000005 Tokens per Sec:  9906191, Lr: 0.000343
2020-02-27 02:27:24,561 Epoch 3462: total training loss 0.00053
2020-02-27 02:27:24,562 EPOCH 3463
2020-02-27 02:27:35,164 Epoch 3463: total training loss 0.00052
2020-02-27 02:27:35,164 EPOCH 3464
2020-02-27 02:27:45,460 Epoch 3464 Step:   384500 Batch Loss:     0.000005 Tokens per Sec:  9627534, Lr: 0.000343
2020-02-27 02:27:45,788 Epoch 3464: total training loss 0.00052
2020-02-27 02:27:45,788 EPOCH 3465
2020-02-27 02:27:56,419 Epoch 3465: total training loss 0.00052
2020-02-27 02:27:56,419 EPOCH 3466
2020-02-27 02:28:06,440 Epoch 3466: total training loss 0.00055
2020-02-27 02:28:06,440 EPOCH 3467
2020-02-27 02:28:08,508 Epoch 3467 Step:   384750 Batch Loss:     0.000003 Tokens per Sec: 10017701, Lr: 0.000343
2020-02-27 02:28:16,415 Epoch 3467: total training loss 0.00055
2020-02-27 02:28:16,415 EPOCH 3468
2020-02-27 02:28:26,242 Epoch 3468: total training loss 0.00053
2020-02-27 02:28:26,243 EPOCH 3469
2020-02-27 02:28:31,055 Epoch 3469 Step:   385000 Batch Loss:     0.000007 Tokens per Sec: 10529127, Lr: 0.000343
2020-02-27 02:28:36,126 Epoch 3469: total training loss 0.00052
2020-02-27 02:28:36,126 EPOCH 3470
2020-02-27 02:28:45,947 Epoch 3470: total training loss 0.00052
2020-02-27 02:28:45,947 EPOCH 3471
2020-02-27 02:28:52,991 Epoch 3471 Step:   385250 Batch Loss:     0.000004 Tokens per Sec: 10580523, Lr: 0.000343
2020-02-27 02:28:55,746 Epoch 3471: total training loss 0.00051
2020-02-27 02:28:55,746 EPOCH 3472
2020-02-27 02:29:05,788 Epoch 3472: total training loss 0.00052
2020-02-27 02:29:05,788 EPOCH 3473
2020-02-27 02:29:15,749 Epoch 3473 Step:   385500 Batch Loss:     0.000003 Tokens per Sec: 10012697, Lr: 0.000343
2020-02-27 02:29:16,043 Epoch 3473: total training loss 0.00051
2020-02-27 02:29:16,044 EPOCH 3474
2020-02-27 02:29:26,216 Epoch 3474: total training loss 0.00052
2020-02-27 02:29:26,217 EPOCH 3475
2020-02-27 02:29:36,514 Epoch 3475: total training loss 0.00052
2020-02-27 02:29:36,514 EPOCH 3476
2020-02-27 02:29:38,797 Epoch 3476 Step:   385750 Batch Loss:     0.000006 Tokens per Sec:  9883119, Lr: 0.000343
2020-02-27 02:29:46,835 Epoch 3476: total training loss 0.00053
2020-02-27 02:29:46,835 EPOCH 3477
2020-02-27 02:29:57,040 Epoch 3477: total training loss 0.00053
2020-02-27 02:29:57,040 EPOCH 3478
2020-02-27 02:30:01,808 Epoch 3478 Step:   386000 Batch Loss:     0.000003 Tokens per Sec: 10126637, Lr: 0.000343
2020-02-27 02:30:07,180 Epoch 3478: total training loss 0.00052
2020-02-27 02:30:07,181 EPOCH 3479
2020-02-27 02:30:17,346 Epoch 3479: total training loss 0.00052
2020-02-27 02:30:17,347 EPOCH 3480
2020-02-27 02:30:24,765 Epoch 3480 Step:   386250 Batch Loss:     0.000004 Tokens per Sec: 10175744, Lr: 0.000343
2020-02-27 02:30:27,485 Epoch 3480: total training loss 0.00053
2020-02-27 02:30:27,485 EPOCH 3481
2020-02-27 02:30:37,599 Epoch 3481: total training loss 0.00052
2020-02-27 02:30:37,600 EPOCH 3482
2020-02-27 02:30:47,904 Epoch 3482 Step:   386500 Batch Loss:     0.000003 Tokens per Sec:  9766686, Lr: 0.000343
2020-02-27 02:30:48,089 Epoch 3482: total training loss 0.00052
2020-02-27 02:30:48,089 EPOCH 3483
2020-02-27 02:30:58,755 Epoch 3483: total training loss 0.00052
2020-02-27 02:30:58,755 EPOCH 3484
2020-02-27 02:31:09,496 Epoch 3484: total training loss 0.00053
2020-02-27 02:31:09,496 EPOCH 3485
2020-02-27 02:31:11,917 Epoch 3485 Step:   386750 Batch Loss:     0.000002 Tokens per Sec:  9722915, Lr: 0.000343
2020-02-27 02:31:19,816 Epoch 3485: total training loss 0.00052
2020-02-27 02:31:19,817 EPOCH 3486
2020-02-27 02:31:29,951 Epoch 3486: total training loss 0.00052
2020-02-27 02:31:29,952 EPOCH 3487
2020-02-27 02:31:34,901 Epoch 3487 Step:   387000 Batch Loss:     0.000004 Tokens per Sec: 10365488, Lr: 0.000343
2020-02-27 02:31:39,925 Epoch 3487: total training loss 0.00056
2020-02-27 02:31:39,926 EPOCH 3488
2020-02-27 02:31:49,982 Epoch 3488: total training loss 0.00054
2020-02-27 02:31:49,982 EPOCH 3489
2020-02-27 02:31:57,318 Epoch 3489 Step:   387250 Batch Loss:     0.000009 Tokens per Sec: 10361940, Lr: 0.000343
2020-02-27 02:31:59,941 Epoch 3489: total training loss 0.00052
2020-02-27 02:31:59,941 EPOCH 3490
2020-02-27 02:32:09,884 Epoch 3490: total training loss 0.00052
2020-02-27 02:32:09,884 EPOCH 3491
2020-02-27 02:32:19,827 Epoch 3491 Step:   387500 Batch Loss:     0.000005 Tokens per Sec: 10334620, Lr: 0.000343
2020-02-27 02:32:19,894 Epoch 3491: total training loss 0.00053
2020-02-27 02:32:19,894 EPOCH 3492
2020-02-27 02:32:29,903 Epoch 3492: total training loss 0.00054
2020-02-27 02:32:29,904 EPOCH 3493
2020-02-27 02:32:40,180 Epoch 3493: total training loss 0.00053
2020-02-27 02:32:40,181 EPOCH 3494
2020-02-27 02:32:42,634 Epoch 3494 Step:   387750 Batch Loss:     0.000003 Tokens per Sec:  9201241, Lr: 0.000343
2020-02-27 02:32:51,098 Epoch 3494: total training loss 0.00052
2020-02-27 02:32:51,099 EPOCH 3495
2020-02-27 02:33:01,902 Epoch 3495: total training loss 0.00052
2020-02-27 02:33:01,903 EPOCH 3496
2020-02-27 02:33:06,958 Epoch 3496 Step:   388000 Batch Loss:     0.000006 Tokens per Sec:  9837086, Lr: 0.000343
2020-02-27 02:33:12,199 Epoch 3496: total training loss 0.00052
2020-02-27 02:33:12,199 EPOCH 3497
2020-02-27 02:33:22,306 Epoch 3497: total training loss 0.00052
2020-02-27 02:33:22,307 EPOCH 3498
2020-02-27 02:33:29,753 Epoch 3498 Step:   388250 Batch Loss:     0.000005 Tokens per Sec: 10214599, Lr: 0.000343
2020-02-27 02:33:32,297 Epoch 3498: total training loss 0.00052
2020-02-27 02:33:32,297 EPOCH 3499
2020-02-27 02:33:42,520 Epoch 3499: total training loss 0.00052
2020-02-27 02:33:42,520 EPOCH 3500
2020-02-27 02:33:52,643 Epoch 3500 Step:   388500 Batch Loss:     0.000004 Tokens per Sec: 10139696, Lr: 0.000343
2020-02-27 02:33:52,644 Epoch 3500: total training loss 0.00053
2020-02-27 02:33:52,644 EPOCH 3501
2020-02-27 02:34:02,536 Epoch 3501: total training loss 0.00052
2020-02-27 02:34:02,537 EPOCH 3502
2020-02-27 02:34:12,450 Epoch 3502: total training loss 0.00052
2020-02-27 02:34:12,451 EPOCH 3503
2020-02-27 02:34:14,873 Epoch 3503 Step:   388750 Batch Loss:     0.000003 Tokens per Sec:  9944640, Lr: 0.000343
2020-02-27 02:34:22,978 Epoch 3503: total training loss 0.00052
2020-02-27 02:34:22,979 EPOCH 3504
2020-02-27 02:34:34,225 Epoch 3504: total training loss 0.00052
2020-02-27 02:34:34,226 EPOCH 3505
2020-02-27 02:34:39,687 Epoch 3505 Step:   389000 Batch Loss:     0.000005 Tokens per Sec:  9267484, Lr: 0.000343
2020-02-27 02:34:45,241 Epoch 3505: total training loss 0.00052
2020-02-27 02:34:45,242 EPOCH 3506
2020-02-27 02:34:56,086 Epoch 3506: total training loss 0.00052
2020-02-27 02:34:56,086 EPOCH 3507
2020-02-27 02:35:04,370 Epoch 3507 Step:   389250 Batch Loss:     0.000004 Tokens per Sec:  9247865, Lr: 0.000343
2020-02-27 02:35:07,112 Epoch 3507: total training loss 0.00053
2020-02-27 02:35:07,112 EPOCH 3508
2020-02-27 02:35:17,108 Epoch 3508: total training loss 0.00056
2020-02-27 02:35:17,108 EPOCH 3509
2020-02-27 02:35:27,066 Epoch 3509: total training loss 0.00052
2020-02-27 02:35:27,067 EPOCH 3510
2020-02-27 02:35:27,174 Epoch 3510 Step:   389500 Batch Loss:     0.000004 Tokens per Sec:  8588694, Lr: 0.000343
2020-02-27 02:35:36,982 Epoch 3510: total training loss 0.00052
2020-02-27 02:35:36,983 EPOCH 3511
2020-02-27 02:35:46,987 Epoch 3511: total training loss 0.00051
2020-02-27 02:35:46,988 EPOCH 3512
2020-02-27 02:35:49,624 Epoch 3512 Step:   389750 Batch Loss:     0.000006 Tokens per Sec: 10429925, Lr: 0.000343
2020-02-27 02:35:56,899 Epoch 3512: total training loss 0.00052
2020-02-27 02:35:56,900 EPOCH 3513
2020-02-27 02:36:07,173 Epoch 3513: total training loss 0.00052
2020-02-27 02:36:07,175 EPOCH 3514
2020-02-27 02:36:12,574 Epoch 3514 Step:   390000 Batch Loss:     0.000004 Tokens per Sec:  9487204, Lr: 0.000343
2020-02-27 02:36:12,575 Model noise rate: 5
2020-02-27 02:37:15,722 Validation result at epoch 3514, step   390000: Val DTW Score:  10.77, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0431, GT DTW Score:      nan, duration: 63.1464s
2020-02-27 02:37:20,578 Epoch 3514: total training loss 0.00052
2020-02-27 02:37:20,579 EPOCH 3515
2020-02-27 02:37:30,353 Epoch 3515: total training loss 0.00052
2020-02-27 02:37:30,353 EPOCH 3516
2020-02-27 02:37:37,903 Epoch 3516 Step:   390250 Batch Loss:     0.000005 Tokens per Sec: 10605877, Lr: 0.000343
2020-02-27 02:37:40,166 Epoch 3516: total training loss 0.00052
2020-02-27 02:37:40,166 EPOCH 3517
2020-02-27 02:37:50,330 Epoch 3517: total training loss 0.00053
2020-02-27 02:37:50,331 EPOCH 3518
2020-02-27 02:38:00,409 Epoch 3518: total training loss 0.00054
2020-02-27 02:38:00,410 EPOCH 3519
2020-02-27 02:38:00,583 Epoch 3519 Step:   390500 Batch Loss:     0.000004 Tokens per Sec:  9055986, Lr: 0.000343
2020-02-27 02:38:10,362 Epoch 3519: total training loss 0.00054
2020-02-27 02:38:10,363 EPOCH 3520
2020-02-27 02:38:20,031 Epoch 3520: total training loss 0.00053
2020-02-27 02:38:20,031 EPOCH 3521
2020-02-27 02:38:22,599 Epoch 3521 Step:   390750 Batch Loss:     0.000007 Tokens per Sec: 10767929, Lr: 0.000343
2020-02-27 02:38:29,807 Epoch 3521: total training loss 0.00052
2020-02-27 02:38:29,807 EPOCH 3522
2020-02-27 02:38:39,728 Epoch 3522: total training loss 0.00052
2020-02-27 02:38:39,729 EPOCH 3523
2020-02-27 02:38:44,863 Epoch 3523 Step:   391000 Batch Loss:     0.000004 Tokens per Sec: 10136993, Lr: 0.000343
2020-02-27 02:38:49,927 Epoch 3523: total training loss 0.00052
2020-02-27 02:38:49,928 EPOCH 3524
2020-02-27 02:39:00,071 Epoch 3524: total training loss 0.00052
2020-02-27 02:39:00,071 EPOCH 3525
2020-02-27 02:39:08,211 Epoch 3525 Step:   391250 Batch Loss:     0.000006 Tokens per Sec: 10017755, Lr: 0.000343
2020-02-27 02:39:10,398 Epoch 3525: total training loss 0.00052
2020-02-27 02:39:10,399 EPOCH 3526
2020-02-27 02:39:20,354 Epoch 3526: total training loss 0.00053
2020-02-27 02:39:20,355 EPOCH 3527
2020-02-27 02:39:30,222 Epoch 3527: total training loss 0.00052
2020-02-27 02:39:30,223 EPOCH 3528
2020-02-27 02:39:30,494 Epoch 3528 Step:   391500 Batch Loss:     0.000003 Tokens per Sec:  9880931, Lr: 0.000343
2020-02-27 02:39:40,102 Epoch 3528: total training loss 0.00052
2020-02-27 02:39:40,102 EPOCH 3529
2020-02-27 02:39:50,001 Epoch 3529: total training loss 0.00052
2020-02-27 02:39:50,002 EPOCH 3530
2020-02-27 02:39:52,710 Epoch 3530 Step:   391750 Batch Loss:     0.000003 Tokens per Sec: 10547283, Lr: 0.000343
2020-02-27 02:39:59,830 Epoch 3530: total training loss 0.00051
2020-02-27 02:39:59,830 EPOCH 3531
2020-02-27 02:40:09,904 Epoch 3531: total training loss 0.00052
2020-02-27 02:40:09,905 EPOCH 3532
2020-02-27 02:40:15,348 Epoch 3532 Step:   392000 Batch Loss:     0.000004 Tokens per Sec: 10028441, Lr: 0.000343
2020-02-27 02:40:20,090 Epoch 3532: total training loss 0.00052
2020-02-27 02:40:20,091 EPOCH 3533
2020-02-27 02:40:30,333 Epoch 3533: total training loss 0.00052
2020-02-27 02:40:30,334 EPOCH 3534
2020-02-27 02:40:38,263 Epoch 3534 Step:   392250 Batch Loss:     0.000004 Tokens per Sec: 10071800, Lr: 0.000343
2020-02-27 02:40:40,517 Epoch 3534: total training loss 0.00051
2020-02-27 02:40:40,518 EPOCH 3535
2020-02-27 02:40:50,663 Epoch 3535: total training loss 0.00053
2020-02-27 02:40:50,663 EPOCH 3536
2020-02-27 02:41:00,700 Epoch 3536: total training loss 0.00053
2020-02-27 02:41:00,701 EPOCH 3537
2020-02-27 02:41:01,060 Epoch 3537 Step:   392500 Batch Loss:     0.000005 Tokens per Sec:  8970148, Lr: 0.000343
2020-02-27 02:41:10,804 Epoch 3537: total training loss 0.00053
2020-02-27 02:41:10,805 EPOCH 3538
2020-02-27 02:41:20,792 Epoch 3538: total training loss 0.00054
2020-02-27 02:41:20,793 EPOCH 3539
2020-02-27 02:41:23,923 Epoch 3539 Step:   392750 Batch Loss:     0.000007 Tokens per Sec: 10320850, Lr: 0.000343
2020-02-27 02:41:30,795 Epoch 3539: total training loss 0.00052
2020-02-27 02:41:30,796 EPOCH 3540
2020-02-27 02:41:40,684 Epoch 3540: total training loss 0.00051
2020-02-27 02:41:40,684 EPOCH 3541
2020-02-27 02:41:46,200 Epoch 3541 Step:   393000 Batch Loss:     0.000008 Tokens per Sec: 10269019, Lr: 0.000343
2020-02-27 02:41:50,658 Epoch 3541: total training loss 0.00052
2020-02-27 02:41:50,659 EPOCH 3542
2020-02-27 02:42:00,954 Epoch 3542: total training loss 0.00051
2020-02-27 02:42:00,954 EPOCH 3543
2020-02-27 02:42:09,189 Epoch 3543 Step:   393250 Batch Loss:     0.000003 Tokens per Sec:  9844098, Lr: 0.000343
2020-02-27 02:42:11,335 Epoch 3543: total training loss 0.00052
2020-02-27 02:42:11,335 EPOCH 3544
2020-02-27 02:42:21,659 Epoch 3544: total training loss 0.00051
2020-02-27 02:42:21,660 EPOCH 3545
2020-02-27 02:42:32,045 Epoch 3545: total training loss 0.00052
2020-02-27 02:42:32,046 EPOCH 3546
2020-02-27 02:42:32,587 Epoch 3546 Step:   393500 Batch Loss:     0.000004 Tokens per Sec:  8769225, Lr: 0.000343
2020-02-27 02:42:42,239 Epoch 3546: total training loss 0.00053
2020-02-27 02:42:42,240 EPOCH 3547
2020-02-27 02:42:52,201 Epoch 3547: total training loss 0.00056
2020-02-27 02:42:52,202 EPOCH 3548
2020-02-27 02:42:55,078 Epoch 3548 Step:   393750 Batch Loss:     0.000003 Tokens per Sec: 10252864, Lr: 0.000343
2020-02-27 02:43:02,040 Epoch 3548: total training loss 0.00054
2020-02-27 02:43:02,041 EPOCH 3549
2020-02-27 02:43:11,911 Epoch 3549: total training loss 0.00055
2020-02-27 02:43:11,911 EPOCH 3550
2020-02-27 02:43:17,434 Epoch 3550 Step:   394000 Batch Loss:     0.000004 Tokens per Sec: 10247597, Lr: 0.000343
2020-02-27 02:43:21,823 Epoch 3550: total training loss 0.00053
2020-02-27 02:43:21,824 EPOCH 3551
2020-02-27 02:43:31,578 Epoch 3551: total training loss 0.00052
2020-02-27 02:43:31,578 EPOCH 3552
2020-02-27 02:43:39,417 Epoch 3552 Step:   394250 Batch Loss:     0.000004 Tokens per Sec: 10296117, Lr: 0.000343
2020-02-27 02:43:41,478 Epoch 3552: total training loss 0.00053
2020-02-27 02:43:41,479 EPOCH 3553
2020-02-27 02:43:51,755 Epoch 3553: total training loss 0.00052
2020-02-27 02:43:51,755 EPOCH 3554
2020-02-27 02:44:02,082 Epoch 3554: total training loss 0.00052
2020-02-27 02:44:02,082 EPOCH 3555
2020-02-27 02:44:02,632 Epoch 3555 Step:   394500 Batch Loss:     0.000005 Tokens per Sec:  9189607, Lr: 0.000343
2020-02-27 02:44:12,521 Epoch 3555: total training loss 0.00052
2020-02-27 02:44:12,522 EPOCH 3556
2020-02-27 02:44:22,893 Epoch 3556: total training loss 0.00051
2020-02-27 02:44:22,894 EPOCH 3557
2020-02-27 02:44:26,074 Epoch 3557 Step:   394750 Batch Loss:     0.000004 Tokens per Sec: 10074729, Lr: 0.000343
2020-02-27 02:44:33,217 Epoch 3557: total training loss 0.00051
2020-02-27 02:44:33,218 EPOCH 3558
2020-02-27 02:44:43,327 Epoch 3558: total training loss 0.00053
2020-02-27 02:44:43,328 EPOCH 3559
2020-02-27 02:44:48,918 Epoch 3559 Step:   395000 Batch Loss:     0.000007 Tokens per Sec: 10163959, Lr: 0.000343
2020-02-27 02:44:53,448 Epoch 3559: total training loss 0.00053
2020-02-27 02:44:53,449 EPOCH 3560
2020-02-27 02:45:03,558 Epoch 3560: total training loss 0.00053
2020-02-27 02:45:03,559 EPOCH 3561
2020-02-27 02:45:11,521 Epoch 3561 Step:   395250 Batch Loss:     0.000004 Tokens per Sec: 10276179, Lr: 0.000343
2020-02-27 02:45:13,558 Epoch 3561: total training loss 0.00054
2020-02-27 02:45:13,558 EPOCH 3562
2020-02-27 02:45:23,360 Epoch 3562: total training loss 0.00052
2020-02-27 02:45:23,361 EPOCH 3563
2020-02-27 02:45:33,490 Epoch 3563: total training loss 0.00053
2020-02-27 02:45:33,491 EPOCH 3564
2020-02-27 02:45:34,135 Epoch 3564 Step:   395500 Batch Loss:     0.000004 Tokens per Sec:  9400999, Lr: 0.000343
2020-02-27 02:45:44,011 Epoch 3564: total training loss 0.00052
2020-02-27 02:45:44,012 EPOCH 3565
2020-02-27 02:45:54,237 Epoch 3565: total training loss 0.00052
2020-02-27 02:45:54,238 EPOCH 3566
2020-02-27 02:45:57,516 Epoch 3566 Step:   395750 Batch Loss:     0.000006 Tokens per Sec: 10139350, Lr: 0.000343
2020-02-27 02:46:04,319 Epoch 3566: total training loss 0.00052
2020-02-27 02:46:04,319 EPOCH 3567
2020-02-27 02:46:14,444 Epoch 3567: total training loss 0.00052
2020-02-27 02:46:14,445 EPOCH 3568
2020-02-27 02:46:20,003 Epoch 3568 Step:   396000 Batch Loss:     0.000004 Tokens per Sec: 10522139, Lr: 0.000343
2020-02-27 02:46:24,184 Epoch 3568: total training loss 0.00051
2020-02-27 02:46:24,184 EPOCH 3569
2020-02-27 02:46:34,375 Epoch 3569: total training loss 0.00051
2020-02-27 02:46:34,376 EPOCH 3570
2020-02-27 02:46:42,578 Epoch 3570 Step:   396250 Batch Loss:     0.000006 Tokens per Sec: 10353171, Lr: 0.000343
2020-02-27 02:46:44,369 Epoch 3570: total training loss 0.00052
2020-02-27 02:46:44,369 EPOCH 3571
2020-02-27 02:46:54,393 Epoch 3571: total training loss 0.00051
2020-02-27 02:46:54,394 EPOCH 3572
2020-02-27 02:47:04,307 Epoch 3572: total training loss 0.00052
2020-02-27 02:47:04,308 EPOCH 3573
2020-02-27 02:47:05,055 Epoch 3573 Step:   396500 Batch Loss:     0.000002 Tokens per Sec: 10314426, Lr: 0.000343
2020-02-27 02:47:14,333 Epoch 3573: total training loss 0.00051
2020-02-27 02:47:14,334 EPOCH 3574
2020-02-27 02:47:24,688 Epoch 3574: total training loss 0.00053
2020-02-27 02:47:24,688 EPOCH 3575
2020-02-27 02:47:28,285 Epoch 3575 Step:   396750 Batch Loss:     0.000006 Tokens per Sec:  9429433, Lr: 0.000343
2020-02-27 02:47:35,570 Epoch 3575: total training loss 0.00052
2020-02-27 02:47:35,570 EPOCH 3576
2020-02-27 02:47:46,295 Epoch 3576: total training loss 0.00051
2020-02-27 02:47:46,295 EPOCH 3577
2020-02-27 02:47:52,127 Epoch 3577 Step:   397000 Batch Loss:     0.000005 Tokens per Sec:  9828927, Lr: 0.000343
2020-02-27 02:47:56,627 Epoch 3577: total training loss 0.00053
2020-02-27 02:47:56,627 EPOCH 3578
2020-02-27 02:48:06,900 Epoch 3578: total training loss 0.00060
2020-02-27 02:48:06,901 EPOCH 3579
2020-02-27 02:48:15,236 Epoch 3579 Step:   397250 Batch Loss:     0.000005 Tokens per Sec: 10172842, Lr: 0.000343
2020-02-27 02:48:16,976 Epoch 3579: total training loss 0.00054
2020-02-27 02:48:16,976 EPOCH 3580
2020-02-27 02:48:26,911 Epoch 3580: total training loss 0.00053
2020-02-27 02:48:26,912 EPOCH 3581
2020-02-27 02:48:36,705 Epoch 3581: total training loss 0.00052
2020-02-27 02:48:36,706 EPOCH 3582
2020-02-27 02:48:37,684 Epoch 3582 Step:   397500 Batch Loss:     0.000008 Tokens per Sec: 10354563, Lr: 0.000343
2020-02-27 02:48:46,590 Epoch 3582: total training loss 0.00051
2020-02-27 02:48:46,591 EPOCH 3583
2020-02-27 02:48:56,338 Epoch 3583: total training loss 0.00051
2020-02-27 02:48:56,339 EPOCH 3584
2020-02-27 02:48:59,710 Epoch 3584 Step:   397750 Batch Loss:     0.000005 Tokens per Sec: 10629994, Lr: 0.000343
2020-02-27 02:49:06,283 Epoch 3584: total training loss 0.00051
2020-02-27 02:49:06,284 EPOCH 3585
2020-02-27 02:49:16,488 Epoch 3585: total training loss 0.00052
2020-02-27 02:49:16,488 EPOCH 3586
2020-02-27 02:49:22,409 Epoch 3586 Step:   398000 Batch Loss:     0.000003 Tokens per Sec: 10039818, Lr: 0.000343
2020-02-27 02:49:26,622 Epoch 3586: total training loss 0.00053
2020-02-27 02:49:26,623 EPOCH 3587
2020-02-27 02:49:36,983 Epoch 3587: total training loss 0.00052
2020-02-27 02:49:36,984 EPOCH 3588
2020-02-27 02:49:45,749 Epoch 3588 Step:   398250 Batch Loss:     0.000005 Tokens per Sec:  9920305, Lr: 0.000343
2020-02-27 02:49:47,377 Epoch 3588: total training loss 0.00052
2020-02-27 02:49:47,378 EPOCH 3589
2020-02-27 02:49:57,654 Epoch 3589: total training loss 0.00051
2020-02-27 02:49:57,655 EPOCH 3590
2020-02-27 02:50:07,833 Epoch 3590: total training loss 0.00053
2020-02-27 02:50:07,833 EPOCH 3591
2020-02-27 02:50:08,816 Epoch 3591 Step:   398500 Batch Loss:     0.000005 Tokens per Sec:  9707638, Lr: 0.000343
2020-02-27 02:50:18,108 Epoch 3591: total training loss 0.00052
2020-02-27 02:50:18,109 EPOCH 3592
2020-02-27 02:50:28,096 Epoch 3592: total training loss 0.00052
2020-02-27 02:50:28,097 EPOCH 3593
2020-02-27 02:50:31,427 Epoch 3593 Step:   398750 Batch Loss:     0.000004 Tokens per Sec: 10258387, Lr: 0.000343
2020-02-27 02:50:37,880 Epoch 3593: total training loss 0.00053
2020-02-27 02:50:37,880 EPOCH 3594
2020-02-27 02:50:47,692 Epoch 3594: total training loss 0.00056
2020-02-27 02:50:47,692 EPOCH 3595
2020-02-27 02:50:53,488 Epoch 3595 Step:   399000 Batch Loss:     0.000004 Tokens per Sec: 10462875, Lr: 0.000343
2020-02-27 02:50:57,491 Epoch 3595: total training loss 0.00052
2020-02-27 02:50:57,491 EPOCH 3596
2020-02-27 02:51:07,799 Epoch 3596: total training loss 0.00052
2020-02-27 02:51:07,800 EPOCH 3597
2020-02-27 02:51:16,529 Epoch 3597 Step:   399250 Batch Loss:     0.000003 Tokens per Sec: 10011745, Lr: 0.000343
2020-02-27 02:51:18,106 Epoch 3597: total training loss 0.00051
2020-02-27 02:51:18,106 EPOCH 3598
2020-02-27 02:51:28,417 Epoch 3598: total training loss 0.00051
2020-02-27 02:51:28,417 EPOCH 3599
2020-02-27 02:51:38,745 Epoch 3599: total training loss 0.00051
2020-02-27 02:51:38,746 EPOCH 3600
2020-02-27 02:51:39,779 Epoch 3600 Step:   399500 Batch Loss:     0.000005 Tokens per Sec: 10336292, Lr: 0.000343
2020-02-27 02:51:48,754 Epoch 3600: total training loss 0.00051
2020-02-27 02:51:48,755 EPOCH 3601
2020-02-27 02:51:58,682 Epoch 3601: total training loss 0.00052
2020-02-27 02:51:58,683 EPOCH 3602
2020-02-27 02:52:02,042 Epoch 3602 Step:   399750 Batch Loss:     0.000004 Tokens per Sec: 10017753, Lr: 0.000343
2020-02-27 02:52:08,602 Epoch 3602: total training loss 0.00052
2020-02-27 02:52:08,603 EPOCH 3603
2020-02-27 02:52:18,496 Epoch 3603: total training loss 0.00053
2020-02-27 02:52:18,497 EPOCH 3604
2020-02-27 02:52:24,799 Epoch 3604 Step:   400000 Batch Loss:     0.000005 Tokens per Sec: 10177706, Lr: 0.000343
2020-02-27 02:52:24,800 Model noise rate: 5
2020-02-27 02:53:27,829 Validation result at epoch 3604, step   400000: Val DTW Score:  10.76, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0430, GT DTW Score:      nan, duration: 63.0285s
2020-02-27 02:53:31,754 Epoch 3604: total training loss 0.00054
2020-02-27 02:53:31,755 EPOCH 3605
2020-02-27 02:53:41,641 Epoch 3605: total training loss 0.00052
2020-02-27 02:53:41,642 EPOCH 3606
2020-02-27 02:53:50,209 Epoch 3606 Step:   400250 Batch Loss:     0.000004 Tokens per Sec: 10235753, Lr: 0.000343
2020-02-27 02:53:51,648 Epoch 3606: total training loss 0.00052
2020-02-27 02:53:51,648 EPOCH 3607
2020-02-27 02:54:01,577 Epoch 3607: total training loss 0.00052
2020-02-27 02:54:01,578 EPOCH 3608
2020-02-27 02:54:12,050 Epoch 3608: total training loss 0.00051
2020-02-27 02:54:12,051 EPOCH 3609
2020-02-27 02:54:13,133 Epoch 3609 Step:   400500 Batch Loss:     0.000005 Tokens per Sec:  9281074, Lr: 0.000343
2020-02-27 02:54:22,494 Epoch 3609: total training loss 0.00051
2020-02-27 02:54:22,495 EPOCH 3610
2020-02-27 02:54:32,937 Epoch 3610: total training loss 0.00051
2020-02-27 02:54:32,938 EPOCH 3611
2020-02-27 02:54:36,524 Epoch 3611 Step:   400750 Batch Loss:     0.000007 Tokens per Sec: 10015406, Lr: 0.000343
2020-02-27 02:54:43,401 Epoch 3611: total training loss 0.00052
2020-02-27 02:54:43,402 EPOCH 3612
2020-02-27 02:54:53,539 Epoch 3612: total training loss 0.00052
2020-02-27 02:54:53,540 EPOCH 3613
2020-02-27 02:54:59,515 Epoch 3613 Step:   401000 Batch Loss:     0.000007 Tokens per Sec: 10277115, Lr: 0.000343
2020-02-27 02:55:03,645 Epoch 3613: total training loss 0.00051
2020-02-27 02:55:03,645 EPOCH 3614
2020-02-27 02:55:13,767 Epoch 3614: total training loss 0.00051
2020-02-27 02:55:13,767 EPOCH 3615
2020-02-27 02:55:22,335 Epoch 3615 Step:   401250 Batch Loss:     0.000005 Tokens per Sec: 10268114, Lr: 0.000343
2020-02-27 02:55:23,755 Epoch 3615: total training loss 0.00052
2020-02-27 02:55:23,755 EPOCH 3616
2020-02-27 02:55:33,658 Epoch 3616: total training loss 0.00052
2020-02-27 02:55:33,658 EPOCH 3617
2020-02-27 02:55:43,521 Epoch 3617: total training loss 0.00052
2020-02-27 02:55:43,522 EPOCH 3618
2020-02-27 02:55:44,740 Epoch 3618 Step:   401500 Batch Loss:     0.000003 Tokens per Sec:  9728354, Lr: 0.000343
2020-02-27 02:55:53,715 Epoch 3618: total training loss 0.00052
2020-02-27 02:55:53,716 EPOCH 3619
2020-02-27 02:56:04,215 Epoch 3619: total training loss 0.00052
2020-02-27 02:56:04,215 EPOCH 3620
2020-02-27 02:56:07,812 Epoch 3620 Step:   401750 Batch Loss:     0.000005 Tokens per Sec: 10016464, Lr: 0.000343
2020-02-27 02:56:14,425 Epoch 3620: total training loss 0.00051
2020-02-27 02:56:14,425 EPOCH 3621
2020-02-27 02:56:24,714 Epoch 3621: total training loss 0.00052
2020-02-27 02:56:24,714 EPOCH 3622
2020-02-27 02:56:31,026 Epoch 3622 Step:   402000 Batch Loss:     0.000005 Tokens per Sec: 10255964, Lr: 0.000343
2020-02-27 02:56:34,702 Epoch 3622: total training loss 0.00052
2020-02-27 02:56:34,703 EPOCH 3623
2020-02-27 02:56:44,642 Epoch 3623: total training loss 0.00052
2020-02-27 02:56:44,643 EPOCH 3624
2020-02-27 02:56:53,120 Epoch 3624 Step:   402250 Batch Loss:     0.000005 Tokens per Sec: 10457122, Lr: 0.000343
2020-02-27 02:56:54,470 Epoch 3624: total training loss 0.00052
2020-02-27 02:56:54,470 EPOCH 3625
2020-02-27 02:57:04,433 Epoch 3625: total training loss 0.00052
2020-02-27 02:57:04,434 EPOCH 3626
2020-02-27 02:57:14,259 Epoch 3626: total training loss 0.00051
2020-02-27 02:57:14,261 EPOCH 3627
2020-02-27 02:57:15,687 Epoch 3627 Step:   402500 Batch Loss:     0.000008 Tokens per Sec: 10443204, Lr: 0.000343
2020-02-27 02:57:24,295 Epoch 3627: total training loss 0.00052
2020-02-27 02:57:24,297 EPOCH 3628
2020-02-27 02:57:35,004 Epoch 3628: total training loss 0.00053
2020-02-27 02:57:35,006 EPOCH 3629
2020-02-27 02:57:39,231 Epoch 3629 Step:   402750 Batch Loss:     0.000005 Tokens per Sec:  9349720, Lr: 0.000343
2020-02-27 02:57:45,716 Epoch 3629: total training loss 0.00052
2020-02-27 02:57:45,717 EPOCH 3630
2020-02-27 02:57:56,556 Epoch 3630: total training loss 0.00051
2020-02-27 02:57:56,557 EPOCH 3631
2020-02-27 02:58:03,182 Epoch 3631 Step:   403000 Batch Loss:     0.000004 Tokens per Sec: 10013631, Lr: 0.000343
2020-02-27 02:58:06,724 Epoch 3631: total training loss 0.00053
2020-02-27 02:58:06,724 EPOCH 3632
2020-02-27 02:58:16,621 Epoch 3632: total training loss 0.00052
2020-02-27 02:58:16,621 EPOCH 3633
2020-02-27 02:58:25,506 Epoch 3633 Step:   403250 Batch Loss:     0.000005 Tokens per Sec: 10187093, Lr: 0.000343
2020-02-27 02:58:26,662 Epoch 3633: total training loss 0.00052
2020-02-27 02:58:26,662 EPOCH 3634
2020-02-27 02:58:36,516 Epoch 3634: total training loss 0.00051
2020-02-27 02:58:36,517 EPOCH 3635
2020-02-27 02:58:46,376 Epoch 3635: total training loss 0.00051
2020-02-27 02:58:46,376 EPOCH 3636
2020-02-27 02:58:47,726 Epoch 3636 Step:   403500 Batch Loss:     0.000005 Tokens per Sec: 10588981, Lr: 0.000343
2020-02-27 02:58:56,139 Epoch 3636: total training loss 0.00051
2020-02-27 02:58:56,140 EPOCH 3637
2020-02-27 02:59:06,279 Epoch 3637: total training loss 0.00051
2020-02-27 02:59:06,279 EPOCH 3638
2020-02-27 02:59:10,107 Epoch 3638 Step:   403750 Batch Loss:     0.000003 Tokens per Sec: 10491820, Lr: 0.000343
2020-02-27 02:59:16,390 Epoch 3638: total training loss 0.00051
2020-02-27 02:59:16,390 EPOCH 3639
2020-02-27 02:59:26,161 Epoch 3639: total training loss 0.00053
2020-02-27 02:59:26,161 EPOCH 3640
2020-02-27 02:59:32,489 Epoch 3640 Step:   404000 Batch Loss:     0.000007 Tokens per Sec: 10358052, Lr: 0.000343
2020-02-27 02:59:36,112 Epoch 3640: total training loss 0.00052
2020-02-27 02:59:36,112 EPOCH 3641
2020-02-27 02:59:45,793 Epoch 3641: total training loss 0.00051
2020-02-27 02:59:45,793 EPOCH 3642
2020-02-27 02:59:55,005 Epoch 3642 Step:   404250 Batch Loss:     0.000005 Tokens per Sec:  9949780, Lr: 0.000343
2020-02-27 02:59:56,226 Epoch 3642: total training loss 0.00051
2020-02-27 02:59:56,226 EPOCH 3643
2020-02-27 03:00:06,837 Epoch 3643: total training loss 0.00051
2020-02-27 03:00:06,838 EPOCH 3644
2020-02-27 03:00:17,651 Epoch 3644: total training loss 0.00052
2020-02-27 03:00:17,652 EPOCH 3645
2020-02-27 03:00:18,893 Epoch 3645 Step:   404500 Batch Loss:     0.000004 Tokens per Sec: 10055056, Lr: 0.000343
2020-02-27 03:00:27,843 Epoch 3645: total training loss 0.00051
2020-02-27 03:00:27,843 EPOCH 3646
2020-02-27 03:00:37,912 Epoch 3646: total training loss 0.00051
2020-02-27 03:00:37,913 EPOCH 3647
2020-02-27 03:00:41,896 Epoch 3647 Step:   404750 Batch Loss:     0.000003 Tokens per Sec: 10377015, Lr: 0.000343
2020-02-27 03:00:47,795 Epoch 3647: total training loss 0.00052
2020-02-27 03:00:47,795 EPOCH 3648
2020-02-27 03:00:57,466 Epoch 3648: total training loss 0.00053
2020-02-27 03:00:57,466 EPOCH 3649
2020-02-27 03:01:03,439 Epoch 3649 Step:   405000 Batch Loss:     0.000007 Tokens per Sec: 10466401, Lr: 0.000343
2020-02-27 03:01:07,141 Epoch 3649: total training loss 0.00052
2020-02-27 03:01:07,141 EPOCH 3650
2020-02-27 03:01:16,803 Epoch 3650: total training loss 0.00051
2020-02-27 03:01:16,804 EPOCH 3651
2020-02-27 03:01:25,619 Epoch 3651 Step:   405250 Batch Loss:     0.000007 Tokens per Sec: 10549593, Lr: 0.000343
2020-02-27 03:01:26,568 Epoch 3651: total training loss 0.00051
2020-02-27 03:01:26,568 EPOCH 3652
2020-02-27 03:01:36,279 Epoch 3652: total training loss 0.00052
2020-02-27 03:01:36,280 EPOCH 3653
2020-02-27 03:01:46,389 Epoch 3653: total training loss 0.00060
2020-02-27 03:01:46,389 EPOCH 3654
2020-02-27 03:01:47,967 Epoch 3654 Step:   405500 Batch Loss:     0.000005 Tokens per Sec: 10168843, Lr: 0.000343
2020-02-27 03:01:56,850 Epoch 3654: total training loss 0.00056
2020-02-27 03:01:56,851 EPOCH 3655
2020-02-27 03:02:07,027 Epoch 3655: total training loss 0.00052
2020-02-27 03:02:07,027 EPOCH 3656
2020-02-27 03:02:11,114 Epoch 3656 Step:   405750 Batch Loss:     0.000005 Tokens per Sec:  9891816, Lr: 0.000343
2020-02-27 03:02:17,427 Epoch 3656: total training loss 0.00051
2020-02-27 03:02:17,427 EPOCH 3657
2020-02-27 03:02:27,357 Epoch 3657: total training loss 0.00051
2020-02-27 03:02:27,357 EPOCH 3658
2020-02-27 03:02:33,850 Epoch 3658 Step:   406000 Batch Loss:     0.000005 Tokens per Sec: 10360952, Lr: 0.000343
2020-02-27 03:02:37,261 Epoch 3658: total training loss 0.00051
2020-02-27 03:02:37,262 EPOCH 3659
2020-02-27 03:02:47,162 Epoch 3659: total training loss 0.00052
2020-02-27 03:02:47,162 EPOCH 3660
2020-02-27 03:02:56,218 Epoch 3660 Step:   406250 Batch Loss:     0.000007 Tokens per Sec: 10321873, Lr: 0.000343
2020-02-27 03:02:57,107 Epoch 3660: total training loss 0.00051
2020-02-27 03:02:57,107 EPOCH 3661
2020-02-27 03:03:06,847 Epoch 3661: total training loss 0.00051
2020-02-27 03:03:06,847 EPOCH 3662
2020-02-27 03:03:16,591 Epoch 3662: total training loss 0.00051
2020-02-27 03:03:16,593 EPOCH 3663
2020-02-27 03:03:18,339 Epoch 3663 Step:   406500 Batch Loss:     0.000003 Tokens per Sec:  9813422, Lr: 0.000343
2020-02-27 03:03:26,869 Epoch 3663: total training loss 0.00051
2020-02-27 03:03:26,870 EPOCH 3664
2020-02-27 03:03:36,971 Epoch 3664: total training loss 0.00051
2020-02-27 03:03:36,971 EPOCH 3665
2020-02-27 03:03:41,615 Epoch 3665 Step:   406750 Batch Loss:     0.000003 Tokens per Sec:  9588014, Lr: 0.000343
2020-02-27 03:03:47,587 Epoch 3665: total training loss 0.00051
2020-02-27 03:03:47,589 EPOCH 3666
2020-02-27 03:03:58,264 Epoch 3666: total training loss 0.00054
2020-02-27 03:03:58,265 EPOCH 3667
2020-02-27 03:04:05,114 Epoch 3667 Step:   407000 Batch Loss:     0.000004 Tokens per Sec: 10026297, Lr: 0.000343
2020-02-27 03:04:08,473 Epoch 3667: total training loss 0.00053
2020-02-27 03:04:08,474 EPOCH 3668
2020-02-27 03:04:18,507 Epoch 3668: total training loss 0.00052
2020-02-27 03:04:18,507 EPOCH 3669
2020-02-27 03:04:27,659 Epoch 3669 Step:   407250 Batch Loss:     0.000004 Tokens per Sec: 10304763, Lr: 0.000343
2020-02-27 03:04:28,489 Epoch 3669: total training loss 0.00051
2020-02-27 03:04:28,489 EPOCH 3670
2020-02-27 03:04:38,416 Epoch 3670: total training loss 0.00051
2020-02-27 03:04:38,416 EPOCH 3671
2020-02-27 03:04:48,336 Epoch 3671: total training loss 0.00051
2020-02-27 03:04:48,338 EPOCH 3672
2020-02-27 03:04:50,123 Epoch 3672 Step:   407500 Batch Loss:     0.000004 Tokens per Sec: 10205835, Lr: 0.000343
2020-02-27 03:04:58,202 Epoch 3672: total training loss 0.00051
2020-02-27 03:04:58,204 EPOCH 3673
2020-02-27 03:05:08,800 Epoch 3673: total training loss 0.00051
2020-02-27 03:05:08,800 EPOCH 3674
2020-02-27 03:05:13,088 Epoch 3674 Step:   407750 Batch Loss:     0.000006 Tokens per Sec: 10001498, Lr: 0.000343
2020-02-27 03:05:19,008 Epoch 3674: total training loss 0.00051
2020-02-27 03:05:19,008 EPOCH 3675
2020-02-27 03:05:29,340 Epoch 3675: total training loss 0.00052
2020-02-27 03:05:29,341 EPOCH 3676
2020-02-27 03:05:36,619 Epoch 3676 Step:   408000 Batch Loss:     0.000004 Tokens per Sec:  9739954, Lr: 0.000343
2020-02-27 03:05:39,876 Epoch 3676: total training loss 0.00052
2020-02-27 03:05:39,877 EPOCH 3677
2020-02-27 03:05:49,944 Epoch 3677: total training loss 0.00051
2020-02-27 03:05:49,945 EPOCH 3678
2020-02-27 03:05:59,122 Epoch 3678 Step:   408250 Batch Loss:     0.000004 Tokens per Sec: 10420119, Lr: 0.000343
2020-02-27 03:05:59,831 Epoch 3678: total training loss 0.00051
2020-02-27 03:05:59,832 EPOCH 3679
2020-02-27 03:06:09,590 Epoch 3679: total training loss 0.00051
2020-02-27 03:06:09,591 EPOCH 3680
2020-02-27 03:06:19,482 Epoch 3680: total training loss 0.00052
2020-02-27 03:06:19,483 EPOCH 3681
2020-02-27 03:06:21,416 Epoch 3681 Step:   408500 Batch Loss:     0.000005 Tokens per Sec: 10017349, Lr: 0.000343
2020-02-27 03:06:29,611 Epoch 3681: total training loss 0.00052
2020-02-27 03:06:29,612 EPOCH 3682
2020-02-27 03:06:40,256 Epoch 3682: total training loss 0.00052
2020-02-27 03:06:40,257 EPOCH 3683
2020-02-27 03:06:45,090 Epoch 3683 Step:   408750 Batch Loss:     0.000005 Tokens per Sec:  9511317, Lr: 0.000343
2020-02-27 03:06:51,010 Epoch 3683: total training loss 0.00051
2020-02-27 03:06:51,011 EPOCH 3684
2020-02-27 03:07:01,713 Epoch 3684: total training loss 0.00051
2020-02-27 03:07:01,713 EPOCH 3685
2020-02-27 03:07:09,035 Epoch 3685 Step:   409000 Batch Loss:     0.000004 Tokens per Sec:  9702160, Lr: 0.000343
2020-02-27 03:07:12,349 Epoch 3685: total training loss 0.00054
2020-02-27 03:07:12,350 EPOCH 3686
2020-02-27 03:07:22,513 Epoch 3686: total training loss 0.00052
2020-02-27 03:07:22,513 EPOCH 3687
2020-02-27 03:07:31,841 Epoch 3687 Step:   409250 Batch Loss:     0.000004 Tokens per Sec: 10402508, Lr: 0.000343
2020-02-27 03:07:32,420 Epoch 3687: total training loss 0.00053
2020-02-27 03:07:32,421 EPOCH 3688
2020-02-27 03:07:42,367 Epoch 3688: total training loss 0.00053
2020-02-27 03:07:42,367 EPOCH 3689
2020-02-27 03:07:52,277 Epoch 3689: total training loss 0.00052
2020-02-27 03:07:52,278 EPOCH 3690
2020-02-27 03:07:54,114 Epoch 3690 Step:   409500 Batch Loss:     0.000004 Tokens per Sec: 10329993, Lr: 0.000343
2020-02-27 03:08:02,170 Epoch 3690: total training loss 0.00052
2020-02-27 03:08:02,172 EPOCH 3691
2020-02-27 03:08:12,152 Epoch 3691: total training loss 0.00051
2020-02-27 03:08:12,154 EPOCH 3692
2020-02-27 03:08:16,792 Epoch 3692 Step:   409750 Batch Loss:     0.000003 Tokens per Sec:  9731367, Lr: 0.000343
2020-02-27 03:08:22,488 Epoch 3692: total training loss 0.00052
2020-02-27 03:08:22,489 EPOCH 3693
2020-02-27 03:08:32,910 Epoch 3693: total training loss 0.00051
2020-02-27 03:08:32,911 EPOCH 3694
2020-02-27 03:08:40,295 Epoch 3694 Step:   410000 Batch Loss:     0.000004 Tokens per Sec:  9978254, Lr: 0.000343
2020-02-27 03:08:40,295 Model noise rate: 5
2020-02-27 03:09:41,876 Validation result at epoch 3694, step   410000: Val DTW Score:  10.77, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0429, GT DTW Score:      nan, duration: 61.5803s
2020-02-27 03:09:44,816 Epoch 3694: total training loss 0.00056
2020-02-27 03:09:44,817 EPOCH 3695
2020-02-27 03:09:54,767 Epoch 3695: total training loss 0.00053
2020-02-27 03:09:54,768 EPOCH 3696
2020-02-27 03:10:04,508 Epoch 3696 Step:   410250 Batch Loss:     0.000004 Tokens per Sec: 10073206, Lr: 0.000343
2020-02-27 03:10:05,075 Epoch 3696: total training loss 0.00051
2020-02-27 03:10:05,075 EPOCH 3697
2020-02-27 03:10:15,808 Epoch 3697: total training loss 0.00051
2020-02-27 03:10:15,809 EPOCH 3698
2020-02-27 03:10:26,071 Epoch 3698: total training loss 0.00051
2020-02-27 03:10:26,072 EPOCH 3699
2020-02-27 03:10:28,212 Epoch 3699 Step:   410500 Batch Loss:     0.000004 Tokens per Sec:  9804730, Lr: 0.000343
2020-02-27 03:10:36,355 Epoch 3699: total training loss 0.00051
2020-02-27 03:10:36,356 EPOCH 3700
2020-02-27 03:10:46,860 Epoch 3700: total training loss 0.00051
2020-02-27 03:10:46,861 EPOCH 3701
2020-02-27 03:10:51,213 Epoch 3701 Step:   410750 Batch Loss:     0.000005 Tokens per Sec: 10580944, Lr: 0.000343
2020-02-27 03:10:56,793 Epoch 3701: total training loss 0.00051
2020-02-27 03:10:56,793 EPOCH 3702
2020-02-27 03:11:06,839 Epoch 3702: total training loss 0.00051
2020-02-27 03:11:06,839 EPOCH 3703
2020-02-27 03:11:13,789 Epoch 3703 Step:   411000 Batch Loss:     0.000005 Tokens per Sec: 10128275, Lr: 0.000343
2020-02-27 03:11:16,903 Epoch 3703: total training loss 0.00051
2020-02-27 03:11:16,904 EPOCH 3704
2020-02-27 03:11:27,015 Epoch 3704: total training loss 0.00051
2020-02-27 03:11:27,017 EPOCH 3705
2020-02-27 03:11:36,820 Epoch 3705 Step:   411250 Batch Loss:     0.000005 Tokens per Sec: 10007116, Lr: 0.000343
2020-02-27 03:11:37,320 Epoch 3705: total training loss 0.00052
2020-02-27 03:11:37,321 EPOCH 3706
2020-02-27 03:11:47,930 Epoch 3706: total training loss 0.00052
2020-02-27 03:11:47,931 EPOCH 3707
2020-02-27 03:11:58,907 Epoch 3707: total training loss 0.00052
2020-02-27 03:11:58,908 EPOCH 3708
2020-02-27 03:12:01,037 Epoch 3708 Step:   411500 Batch Loss:     0.000004 Tokens per Sec:  9588857, Lr: 0.000343
2020-02-27 03:12:09,758 Epoch 3708: total training loss 0.00052
2020-02-27 03:12:09,759 EPOCH 3709
2020-02-27 03:12:20,280 Epoch 3709: total training loss 0.00052
2020-02-27 03:12:20,280 EPOCH 3710
2020-02-27 03:12:24,954 Epoch 3710 Step:   411750 Batch Loss:     0.000006 Tokens per Sec:  9536297, Lr: 0.000343
2020-02-27 03:12:30,607 Epoch 3710: total training loss 0.00051
2020-02-27 03:12:30,607 EPOCH 3711
2020-02-27 03:12:40,687 Epoch 3711: total training loss 0.00051
2020-02-27 03:12:40,687 EPOCH 3712
2020-02-27 03:12:47,968 Epoch 3712 Step:   412000 Batch Loss:     0.000006 Tokens per Sec: 10298762, Lr: 0.000343
2020-02-27 03:12:50,683 Epoch 3712: total training loss 0.00051
2020-02-27 03:12:50,683 EPOCH 3713
2020-02-27 03:13:00,569 Epoch 3713: total training loss 0.00052
2020-02-27 03:13:00,569 EPOCH 3714
2020-02-27 03:13:10,021 Epoch 3714 Step:   412250 Batch Loss:     0.000003 Tokens per Sec: 10455135, Lr: 0.000343
2020-02-27 03:13:10,399 Epoch 3714: total training loss 0.00051
2020-02-27 03:13:10,399 EPOCH 3715
2020-02-27 03:13:20,430 Epoch 3715: total training loss 0.00051
2020-02-27 03:13:20,432 EPOCH 3716
2020-02-27 03:13:31,009 Epoch 3716: total training loss 0.00051
2020-02-27 03:13:31,009 EPOCH 3717
2020-02-27 03:13:33,285 Epoch 3717 Step:   412500 Batch Loss:     0.000006 Tokens per Sec:  8927530, Lr: 0.000343
2020-02-27 03:13:41,867 Epoch 3717: total training loss 0.00051
2020-02-27 03:13:41,867 EPOCH 3718
2020-02-27 03:13:52,362 Epoch 3718: total training loss 0.00052
2020-02-27 03:13:52,362 EPOCH 3719
2020-02-27 03:13:57,324 Epoch 3719 Step:   412750 Batch Loss:     0.000004 Tokens per Sec:  9453234, Lr: 0.000343
2020-02-27 03:14:03,037 Epoch 3719: total training loss 0.00052
2020-02-27 03:14:03,037 EPOCH 3720
2020-02-27 03:14:13,259 Epoch 3720: total training loss 0.00052
2020-02-27 03:14:13,259 EPOCH 3721
2020-02-27 03:14:20,759 Epoch 3721 Step:   413000 Batch Loss:     0.000002 Tokens per Sec: 10021581, Lr: 0.000343
2020-02-27 03:14:23,471 Epoch 3721: total training loss 0.00052
2020-02-27 03:14:23,471 EPOCH 3722
2020-02-27 03:14:33,662 Epoch 3722: total training loss 0.00052
2020-02-27 03:14:33,663 EPOCH 3723
2020-02-27 03:14:43,509 Epoch 3723 Step:   413250 Batch Loss:     0.000007 Tokens per Sec: 10192322, Lr: 0.000343
2020-02-27 03:14:43,774 Epoch 3723: total training loss 0.00051
2020-02-27 03:14:43,774 EPOCH 3724
2020-02-27 03:14:53,635 Epoch 3724: total training loss 0.00051
2020-02-27 03:14:53,635 EPOCH 3725
2020-02-27 03:15:03,599 Epoch 3725: total training loss 0.00051
2020-02-27 03:15:03,601 EPOCH 3726
2020-02-27 03:15:06,078 Epoch 3726 Step:   413500 Batch Loss:     0.000004 Tokens per Sec: 10685013, Lr: 0.000343
2020-02-27 03:15:14,023 Epoch 3726: total training loss 0.00050
2020-02-27 03:15:14,024 EPOCH 3727
2020-02-27 03:15:24,492 Epoch 3727: total training loss 0.00051
2020-02-27 03:15:24,493 EPOCH 3728
2020-02-27 03:15:29,399 Epoch 3728 Step:   413750 Batch Loss:     0.000004 Tokens per Sec:  9895539, Lr: 0.000343
2020-02-27 03:15:34,688 Epoch 3728: total training loss 0.00051
2020-02-27 03:15:34,689 EPOCH 3729
2020-02-27 03:15:44,985 Epoch 3729: total training loss 0.00051
2020-02-27 03:15:44,986 EPOCH 3730
2020-02-27 03:15:52,591 Epoch 3730 Step:   414000 Batch Loss:     0.000004 Tokens per Sec:  9901763, Lr: 0.000343
2020-02-27 03:15:55,193 Epoch 3730: total training loss 0.00051
2020-02-27 03:15:55,194 EPOCH 3731
2020-02-27 03:16:05,234 Epoch 3731: total training loss 0.00050
2020-02-27 03:16:05,234 EPOCH 3732
2020-02-27 03:16:14,847 Epoch 3732 Step:   414250 Batch Loss:     0.000007 Tokens per Sec: 10453699, Lr: 0.000343
2020-02-27 03:16:15,030 Epoch 3732: total training loss 0.00051
2020-02-27 03:16:15,030 EPOCH 3733
2020-02-27 03:16:25,103 Epoch 3733: total training loss 0.00051
2020-02-27 03:16:25,104 EPOCH 3734
2020-02-27 03:16:35,164 Epoch 3734: total training loss 0.00053
2020-02-27 03:16:35,165 EPOCH 3735
2020-02-27 03:16:37,606 Epoch 3735 Step:   414500 Batch Loss:     0.000008 Tokens per Sec: 10209280, Lr: 0.000343
2020-02-27 03:16:45,202 Epoch 3735: total training loss 0.00053
2020-02-27 03:16:45,204 EPOCH 3736
2020-02-27 03:16:55,654 Epoch 3736: total training loss 0.00054
2020-02-27 03:16:55,655 EPOCH 3737
2020-02-27 03:17:00,854 Epoch 3737 Step:   414750 Batch Loss:     0.000003 Tokens per Sec:  9279540, Lr: 0.000343
2020-02-27 03:17:06,443 Epoch 3737: total training loss 0.00052
2020-02-27 03:17:06,443 EPOCH 3738
2020-02-27 03:17:16,968 Epoch 3738: total training loss 0.00051
2020-02-27 03:17:16,969 EPOCH 3739
2020-02-27 03:17:24,560 Epoch 3739 Step:   415000 Batch Loss:     0.000005 Tokens per Sec:  9997022, Lr: 0.000343
2020-02-27 03:17:27,331 Epoch 3739: total training loss 0.00052
2020-02-27 03:17:27,331 EPOCH 3740
2020-02-27 03:17:37,726 Epoch 3740: total training loss 0.00054
2020-02-27 03:17:37,726 EPOCH 3741
2020-02-27 03:17:47,497 Epoch 3741 Step:   415250 Batch Loss:     0.000004 Tokens per Sec: 10482718, Lr: 0.000343
2020-02-27 03:17:47,584 Epoch 3741: total training loss 0.00052
2020-02-27 03:17:47,585 EPOCH 3742
2020-02-27 03:17:57,508 Epoch 3742: total training loss 0.00051
2020-02-27 03:17:57,509 EPOCH 3743
2020-02-27 03:18:07,355 Epoch 3743: total training loss 0.00052
2020-02-27 03:18:07,357 EPOCH 3744
2020-02-27 03:18:09,979 Epoch 3744 Step:   415500 Batch Loss:     0.000004 Tokens per Sec: 10436538, Lr: 0.000343
2020-02-27 03:18:17,572 Epoch 3744: total training loss 0.00052
2020-02-27 03:18:17,573 EPOCH 3745
2020-02-27 03:18:27,942 Epoch 3745: total training loss 0.00051
2020-02-27 03:18:27,942 EPOCH 3746
2020-02-27 03:18:32,972 Epoch 3746 Step:   415750 Batch Loss:     0.000003 Tokens per Sec: 10085761, Lr: 0.000343
2020-02-27 03:18:38,214 Epoch 3746: total training loss 0.00052
2020-02-27 03:18:38,215 EPOCH 3747
2020-02-27 03:18:48,562 Epoch 3747: total training loss 0.00051
2020-02-27 03:18:48,562 EPOCH 3748
2020-02-27 03:18:56,251 Epoch 3748 Step:   416000 Batch Loss:     0.000004 Tokens per Sec:  9895136, Lr: 0.000343
2020-02-27 03:18:58,969 Epoch 3748: total training loss 0.00052
2020-02-27 03:18:58,970 EPOCH 3749
2020-02-27 03:19:08,835 Epoch 3749: total training loss 0.00052
2020-02-27 03:19:08,836 EPOCH 3750
2020-02-27 03:19:19,034 Epoch 3750 Step:   416250 Batch Loss:     0.000004 Tokens per Sec: 10094027, Lr: 0.000343
2020-02-27 03:19:19,035 Epoch 3750: total training loss 0.00051
2020-02-27 03:19:19,035 EPOCH 3751
2020-02-27 03:19:29,040 Epoch 3751: total training loss 0.00051
2020-02-27 03:19:29,040 EPOCH 3752
2020-02-27 03:19:38,983 Epoch 3752: total training loss 0.00051
2020-02-27 03:19:38,984 EPOCH 3753
2020-02-27 03:19:41,454 Epoch 3753 Step:   416500 Batch Loss:     0.000004 Tokens per Sec: 10227944, Lr: 0.000343
2020-02-27 03:19:48,805 Epoch 3753: total training loss 0.00051
2020-02-27 03:19:48,806 EPOCH 3754
2020-02-27 03:19:58,767 Epoch 3754: total training loss 0.00051
2020-02-27 03:19:58,769 EPOCH 3755
2020-02-27 03:20:03,977 Epoch 3755 Step:   416750 Batch Loss:     0.000004 Tokens per Sec: 10150091, Lr: 0.000343
2020-02-27 03:20:08,916 Epoch 3755: total training loss 0.00051
2020-02-27 03:20:08,917 EPOCH 3756
2020-02-27 03:20:18,907 Epoch 3756: total training loss 0.00050
2020-02-27 03:20:18,908 EPOCH 3757
2020-02-27 03:20:26,527 Epoch 3757 Step:   417000 Batch Loss:     0.000005 Tokens per Sec: 10273298, Lr: 0.000343
2020-02-27 03:20:28,935 Epoch 3757: total training loss 0.00052
2020-02-27 03:20:28,935 EPOCH 3758
2020-02-27 03:20:38,828 Epoch 3758: total training loss 0.00051
2020-02-27 03:20:38,828 EPOCH 3759
2020-02-27 03:20:48,743 Epoch 3759: total training loss 0.00051
2020-02-27 03:20:48,744 EPOCH 3760
2020-02-27 03:20:48,873 Epoch 3760 Step:   417250 Batch Loss:     0.000003 Tokens per Sec:  8884160, Lr: 0.000343
2020-02-27 03:20:59,017 Epoch 3760: total training loss 0.00051
2020-02-27 03:20:59,018 EPOCH 3761
2020-02-27 03:21:09,174 Epoch 3761: total training loss 0.00053
2020-02-27 03:21:09,176 EPOCH 3762
2020-02-27 03:21:11,870 Epoch 3762 Step:   417500 Batch Loss:     0.000003 Tokens per Sec: 10152417, Lr: 0.000343
2020-02-27 03:21:19,465 Epoch 3762: total training loss 0.00054
2020-02-27 03:21:19,465 EPOCH 3763
2020-02-27 03:21:29,360 Epoch 3763: total training loss 0.00058
2020-02-27 03:21:29,360 EPOCH 3764
2020-02-27 03:21:34,737 Epoch 3764 Step:   417750 Batch Loss:     0.000005 Tokens per Sec:  9957976, Lr: 0.000343
2020-02-27 03:21:39,554 Epoch 3764: total training loss 0.00052
2020-02-27 03:21:39,555 EPOCH 3765
2020-02-27 03:21:49,746 Epoch 3765: total training loss 0.00051
2020-02-27 03:21:49,747 EPOCH 3766
2020-02-27 03:21:57,705 Epoch 3766 Step:   418000 Batch Loss:     0.000006 Tokens per Sec: 10203840, Lr: 0.000343
2020-02-27 03:21:59,927 Epoch 3766: total training loss 0.00051
2020-02-27 03:21:59,928 EPOCH 3767
2020-02-27 03:22:09,820 Epoch 3767: total training loss 0.00051
2020-02-27 03:22:09,820 EPOCH 3768
2020-02-27 03:22:19,735 Epoch 3768: total training loss 0.00051
2020-02-27 03:22:19,736 EPOCH 3769
2020-02-27 03:22:19,950 Epoch 3769 Step:   418250 Batch Loss:     0.000005 Tokens per Sec:  9614053, Lr: 0.000343
2020-02-27 03:22:29,544 Epoch 3769: total training loss 0.00051
2020-02-27 03:22:29,545 EPOCH 3770
2020-02-27 03:22:39,907 Epoch 3770: total training loss 0.00052
2020-02-27 03:22:39,907 EPOCH 3771
2020-02-27 03:22:42,936 Epoch 3771 Step:   418500 Batch Loss:     0.000005 Tokens per Sec:  9919522, Lr: 0.000343
2020-02-27 03:22:50,257 Epoch 3771: total training loss 0.00051
2020-02-27 03:22:50,258 EPOCH 3772
2020-02-27 03:23:00,603 Epoch 3772: total training loss 0.00052
2020-02-27 03:23:00,603 EPOCH 3773
2020-02-27 03:23:06,212 Epoch 3773 Step:   418750 Batch Loss:     0.000006 Tokens per Sec:  9653225, Lr: 0.000343
2020-02-27 03:23:11,006 Epoch 3773: total training loss 0.00051
2020-02-27 03:23:11,007 EPOCH 3774
2020-02-27 03:23:21,110 Epoch 3774: total training loss 0.00051
2020-02-27 03:23:21,111 EPOCH 3775
2020-02-27 03:23:28,835 Epoch 3775 Step:   419000 Batch Loss:     0.000005 Tokens per Sec: 10422745, Lr: 0.000343
2020-02-27 03:23:30,995 Epoch 3775: total training loss 0.00051
2020-02-27 03:23:30,995 EPOCH 3776
2020-02-27 03:23:41,149 Epoch 3776: total training loss 0.00051
2020-02-27 03:23:41,150 EPOCH 3777
2020-02-27 03:23:51,079 Epoch 3777: total training loss 0.00051
2020-02-27 03:23:51,080 EPOCH 3778
2020-02-27 03:23:51,418 Epoch 3778 Step:   419250 Batch Loss:     0.000007 Tokens per Sec: 10001927, Lr: 0.000343
2020-02-27 03:24:00,970 Epoch 3778: total training loss 0.00051
2020-02-27 03:24:00,971 EPOCH 3779
2020-02-27 03:24:10,924 Epoch 3779: total training loss 0.00052
2020-02-27 03:24:10,926 EPOCH 3780
2020-02-27 03:24:13,802 Epoch 3780 Step:   419500 Batch Loss:     0.000005 Tokens per Sec: 10189206, Lr: 0.000343
2020-02-27 03:24:21,370 Epoch 3780: total training loss 0.00052
2020-02-27 03:24:21,370 EPOCH 3781
2020-02-27 03:24:32,107 Epoch 3781: total training loss 0.00051
2020-02-27 03:24:32,110 EPOCH 3782
2020-02-27 03:24:37,745 Epoch 3782 Step:   419750 Batch Loss:     0.000003 Tokens per Sec:  9456238, Lr: 0.000343
2020-02-27 03:24:42,801 Epoch 3782: total training loss 0.00051
2020-02-27 03:24:42,802 EPOCH 3783
2020-02-27 03:24:53,412 Epoch 3783: total training loss 0.00051
2020-02-27 03:24:53,412 EPOCH 3784
2020-02-27 03:25:01,309 Epoch 3784 Step:   420000 Batch Loss:     0.000003 Tokens per Sec: 10014948, Lr: 0.000343
2020-02-27 03:25:01,309 Model noise rate: 5
2020-02-27 03:26:02,639 Validation result at epoch 3784, step   420000: Val DTW Score:  10.77, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0432, GT DTW Score:      nan, duration: 61.3293s
2020-02-27 03:26:04,977 Epoch 3784: total training loss 0.00051
2020-02-27 03:26:04,977 EPOCH 3785
2020-02-27 03:26:15,312 Epoch 3785: total training loss 0.00051
2020-02-27 03:26:15,313 EPOCH 3786
2020-02-27 03:26:25,593 Epoch 3786: total training loss 0.00050
2020-02-27 03:26:25,594 EPOCH 3787
2020-02-27 03:26:26,025 Epoch 3787 Step:   420250 Batch Loss:     0.000005 Tokens per Sec: 10191072, Lr: 0.000240
2020-02-27 03:26:35,769 Epoch 3787: total training loss 0.00049
2020-02-27 03:26:35,769 EPOCH 3788
2020-02-27 03:26:45,870 Epoch 3788: total training loss 0.00050
2020-02-27 03:26:45,872 EPOCH 3789
2020-02-27 03:26:48,743 Epoch 3789 Step:   420500 Batch Loss:     0.000005 Tokens per Sec: 10073421, Lr: 0.000240
2020-02-27 03:26:56,033 Epoch 3789: total training loss 0.00050
2020-02-27 03:26:56,033 EPOCH 3790
2020-02-27 03:27:06,140 Epoch 3790: total training loss 0.00049
2020-02-27 03:27:06,141 EPOCH 3791
2020-02-27 03:27:11,535 Epoch 3791 Step:   420750 Batch Loss:     0.000003 Tokens per Sec: 10173206, Lr: 0.000240
2020-02-27 03:27:16,096 Epoch 3791: total training loss 0.00050
2020-02-27 03:27:16,096 EPOCH 3792
2020-02-27 03:27:26,170 Epoch 3792: total training loss 0.00049
2020-02-27 03:27:26,170 EPOCH 3793
2020-02-27 03:27:34,087 Epoch 3793 Step:   421000 Batch Loss:     0.000004 Tokens per Sec:  9991688, Lr: 0.000240
2020-02-27 03:27:36,347 Epoch 3793: total training loss 0.00051
2020-02-27 03:27:36,347 EPOCH 3794
2020-02-27 03:27:46,827 Epoch 3794: total training loss 0.00050
2020-02-27 03:27:46,828 EPOCH 3795
2020-02-27 03:27:57,230 Epoch 3795: total training loss 0.00050
2020-02-27 03:27:57,231 EPOCH 3796
2020-02-27 03:27:57,754 Epoch 3796 Step:   421250 Batch Loss:     0.000005 Tokens per Sec:  9511517, Lr: 0.000240
2020-02-27 03:28:07,551 Epoch 3796: total training loss 0.00049
2020-02-27 03:28:07,552 EPOCH 3797
2020-02-27 03:28:17,631 Epoch 3797: total training loss 0.00050
2020-02-27 03:28:17,632 EPOCH 3798
2020-02-27 03:28:20,800 Epoch 3798 Step:   421500 Batch Loss:     0.000004 Tokens per Sec: 10582830, Lr: 0.000240
2020-02-27 03:28:27,518 Epoch 3798: total training loss 0.00050
2020-02-27 03:28:27,518 EPOCH 3799
2020-02-27 03:28:37,328 Epoch 3799: total training loss 0.00050
2020-02-27 03:28:37,328 EPOCH 3800
2020-02-27 03:28:42,835 Epoch 3800 Step:   421750 Batch Loss:     0.000004 Tokens per Sec: 10428335, Lr: 0.000240
2020-02-27 03:28:47,225 Epoch 3800: total training loss 0.00050
2020-02-27 03:28:47,225 EPOCH 3801
2020-02-27 03:28:57,130 Epoch 3801: total training loss 0.00049
2020-02-27 03:28:57,132 EPOCH 3802
2020-02-27 03:29:05,222 Epoch 3802 Step:   422000 Batch Loss:     0.000005 Tokens per Sec: 10167149, Lr: 0.000240
2020-02-27 03:29:07,335 Epoch 3802: total training loss 0.00049
2020-02-27 03:29:07,336 EPOCH 3803
2020-02-27 03:29:18,119 Epoch 3803: total training loss 0.00049
2020-02-27 03:29:18,120 EPOCH 3804
2020-02-27 03:29:28,961 Epoch 3804: total training loss 0.00058
2020-02-27 03:29:28,962 EPOCH 3805
2020-02-27 03:29:29,635 Epoch 3805 Step:   422250 Batch Loss:     0.000004 Tokens per Sec:  9525380, Lr: 0.000240
2020-02-27 03:29:39,468 Epoch 3805: total training loss 0.00052
2020-02-27 03:29:39,469 EPOCH 3806
2020-02-27 03:29:50,155 Epoch 3806: total training loss 0.00050
2020-02-27 03:29:50,155 EPOCH 3807
2020-02-27 03:29:53,305 Epoch 3807 Step:   422500 Batch Loss:     0.000004 Tokens per Sec:  9514587, Lr: 0.000240
2020-02-27 03:30:00,427 Epoch 3807: total training loss 0.00049
2020-02-27 03:30:00,429 EPOCH 3808
2020-02-27 03:30:10,447 Epoch 3808: total training loss 0.00050
2020-02-27 03:30:10,449 EPOCH 3809
2020-02-27 03:30:15,921 Epoch 3809 Step:   422750 Batch Loss:     0.000005 Tokens per Sec: 10197674, Lr: 0.000240
2020-02-27 03:30:20,401 Epoch 3809: total training loss 0.00050
2020-02-27 03:30:20,401 EPOCH 3810
2020-02-27 03:30:30,347 Epoch 3810: total training loss 0.00049
2020-02-27 03:30:30,347 EPOCH 3811
2020-02-27 03:30:38,355 Epoch 3811 Step:   423000 Batch Loss:     0.000004 Tokens per Sec: 10280877, Lr: 0.000240
2020-02-27 03:30:40,388 Epoch 3811: total training loss 0.00050
2020-02-27 03:30:40,389 EPOCH 3812
2020-02-27 03:30:50,836 Epoch 3812: total training loss 0.00049
2020-02-27 03:30:50,837 EPOCH 3813
2020-02-27 03:31:01,339 Epoch 3813: total training loss 0.00049
2020-02-27 03:31:01,339 EPOCH 3814
2020-02-27 03:31:02,012 Epoch 3814 Step:   423250 Batch Loss:     0.000006 Tokens per Sec:  9725407, Lr: 0.000240
2020-02-27 03:31:11,605 Epoch 3814: total training loss 0.00050
2020-02-27 03:31:11,605 EPOCH 3815
2020-02-27 03:31:21,967 Epoch 3815: total training loss 0.00050
2020-02-27 03:31:21,968 EPOCH 3816
2020-02-27 03:31:25,255 Epoch 3816 Step:   423500 Batch Loss:     0.000005 Tokens per Sec:  9822357, Lr: 0.000240
2020-02-27 03:31:32,156 Epoch 3816: total training loss 0.00050
2020-02-27 03:31:32,157 EPOCH 3817
2020-02-27 03:31:42,242 Epoch 3817: total training loss 0.00049
2020-02-27 03:31:42,242 EPOCH 3818
2020-02-27 03:31:48,073 Epoch 3818 Step:   423750 Batch Loss:     0.000003 Tokens per Sec: 10049088, Lr: 0.000240
2020-02-27 03:31:52,370 Epoch 3818: total training loss 0.00050
2020-02-27 03:31:52,371 EPOCH 3819
2020-02-27 03:32:02,405 Epoch 3819: total training loss 0.00049
2020-02-27 03:32:02,405 EPOCH 3820
2020-02-27 03:32:10,073 Epoch 3820 Step:   424000 Batch Loss:     0.000005 Tokens per Sec: 10387918, Lr: 0.000240
2020-02-27 03:32:12,178 Epoch 3820: total training loss 0.00050
2020-02-27 03:32:12,178 EPOCH 3821
2020-02-27 03:32:21,957 Epoch 3821: total training loss 0.00050
2020-02-27 03:32:21,958 EPOCH 3822
2020-02-27 03:32:32,208 Epoch 3822: total training loss 0.00050
2020-02-27 03:32:32,208 EPOCH 3823
2020-02-27 03:32:32,987 Epoch 3823 Step:   424250 Batch Loss:     0.000004 Tokens per Sec:  9823089, Lr: 0.000240
2020-02-27 03:32:42,645 Epoch 3823: total training loss 0.00049
2020-02-27 03:32:42,646 EPOCH 3824
2020-02-27 03:32:53,162 Epoch 3824: total training loss 0.00049
2020-02-27 03:32:53,162 EPOCH 3825
2020-02-27 03:32:56,586 Epoch 3825 Step:   424500 Batch Loss:     0.000004 Tokens per Sec:  9492952, Lr: 0.000240
2020-02-27 03:33:03,683 Epoch 3825: total training loss 0.00053
2020-02-27 03:33:03,684 EPOCH 3826
2020-02-27 03:33:13,933 Epoch 3826: total training loss 0.00051
2020-02-27 03:33:13,934 EPOCH 3827
2020-02-27 03:33:19,581 Epoch 3827 Step:   424750 Batch Loss:     0.000004 Tokens per Sec: 10330821, Lr: 0.000240
2020-02-27 03:33:23,822 Epoch 3827: total training loss 0.00050
2020-02-27 03:33:23,823 EPOCH 3828
2020-02-27 03:33:33,735 Epoch 3828: total training loss 0.00049
2020-02-27 03:33:33,735 EPOCH 3829
2020-02-27 03:33:42,012 Epoch 3829 Step:   425000 Batch Loss:     0.000005 Tokens per Sec: 10244617, Lr: 0.000240
2020-02-27 03:33:43,790 Epoch 3829: total training loss 0.00049
2020-02-27 03:33:43,791 EPOCH 3830
2020-02-27 03:33:53,519 Epoch 3830: total training loss 0.00050
2020-02-27 03:33:53,519 EPOCH 3831
2020-02-27 03:34:03,377 Epoch 3831: total training loss 0.00049
2020-02-27 03:34:03,378 EPOCH 3832
2020-02-27 03:34:04,124 Epoch 3832 Step:   425250 Batch Loss:     0.000004 Tokens per Sec:  9865508, Lr: 0.000240
2020-02-27 03:34:13,497 Epoch 3832: total training loss 0.00050
2020-02-27 03:34:13,498 EPOCH 3833
2020-02-27 03:34:23,803 Epoch 3833: total training loss 0.00049
2020-02-27 03:34:23,803 EPOCH 3834
2020-02-27 03:34:27,189 Epoch 3834 Step:   425500 Batch Loss:     0.000005 Tokens per Sec: 10116191, Lr: 0.000240
2020-02-27 03:34:33,904 Epoch 3834: total training loss 0.00050
2020-02-27 03:34:33,905 EPOCH 3835
2020-02-27 03:34:44,339 Epoch 3835: total training loss 0.00049
2020-02-27 03:34:44,340 EPOCH 3836
2020-02-27 03:34:50,645 Epoch 3836 Step:   425750 Batch Loss:     0.000004 Tokens per Sec:  9215116, Lr: 0.000240
2020-02-27 03:34:54,971 Epoch 3836: total training loss 0.00050
2020-02-27 03:34:54,972 EPOCH 3837
2020-02-27 03:35:04,997 Epoch 3837: total training loss 0.00049
2020-02-27 03:35:04,998 EPOCH 3838
2020-02-27 03:35:13,502 Epoch 3838 Step:   426000 Batch Loss:     0.000003 Tokens per Sec: 10244411, Lr: 0.000240
2020-02-27 03:35:15,138 Epoch 3838: total training loss 0.00049
2020-02-27 03:35:15,138 EPOCH 3839
2020-02-27 03:35:25,289 Epoch 3839: total training loss 0.00049
2020-02-27 03:35:25,290 EPOCH 3840
2020-02-27 03:35:35,136 Epoch 3840: total training loss 0.00050
2020-02-27 03:35:35,137 EPOCH 3841
2020-02-27 03:35:36,062 Epoch 3841 Step:   426250 Batch Loss:     0.000004 Tokens per Sec: 10167793, Lr: 0.000240
2020-02-27 03:35:45,003 Epoch 3841: total training loss 0.00050
2020-02-27 03:35:45,003 EPOCH 3842
2020-02-27 03:35:54,871 Epoch 3842: total training loss 0.00049
2020-02-27 03:35:54,872 EPOCH 3843
2020-02-27 03:35:58,208 Epoch 3843 Step:   426500 Batch Loss:     0.000003 Tokens per Sec: 10322810, Lr: 0.000240
2020-02-27 03:36:04,925 Epoch 3843: total training loss 0.00050
2020-02-27 03:36:04,925 EPOCH 3844
2020-02-27 03:36:15,178 Epoch 3844: total training loss 0.00051
2020-02-27 03:36:15,178 EPOCH 3845
2020-02-27 03:36:21,220 Epoch 3845 Step:   426750 Batch Loss:     0.000005 Tokens per Sec: 10056709, Lr: 0.000240
2020-02-27 03:36:25,494 Epoch 3845: total training loss 0.00050
2020-02-27 03:36:25,495 EPOCH 3846
2020-02-27 03:36:35,775 Epoch 3846: total training loss 0.00050
2020-02-27 03:36:35,775 EPOCH 3847
2020-02-27 03:36:44,200 Epoch 3847 Step:   427000 Batch Loss:     0.000005 Tokens per Sec: 10073063, Lr: 0.000240
2020-02-27 03:36:45,865 Epoch 3847: total training loss 0.00049
2020-02-27 03:36:45,865 EPOCH 3848
2020-02-27 03:36:55,663 Epoch 3848: total training loss 0.00050
2020-02-27 03:36:55,664 EPOCH 3849
2020-02-27 03:37:05,525 Epoch 3849: total training loss 0.00049
2020-02-27 03:37:05,525 EPOCH 3850
2020-02-27 03:37:06,611 Epoch 3850 Step:   427250 Batch Loss:     0.000004 Tokens per Sec: 10671761, Lr: 0.000240
2020-02-27 03:37:15,496 Epoch 3850: total training loss 0.00049
2020-02-27 03:37:15,497 EPOCH 3851
2020-02-27 03:37:25,894 Epoch 3851: total training loss 0.00049
2020-02-27 03:37:25,895 EPOCH 3852
2020-02-27 03:37:29,636 Epoch 3852 Step:   427500 Batch Loss:     0.000002 Tokens per Sec:  9525477, Lr: 0.000240
2020-02-27 03:37:36,662 Epoch 3852: total training loss 0.00050
2020-02-27 03:37:36,663 EPOCH 3853
2020-02-27 03:37:47,475 Epoch 3853: total training loss 0.00050
2020-02-27 03:37:47,476 EPOCH 3854
2020-02-27 03:37:53,724 Epoch 3854 Step:   427750 Batch Loss:     0.000004 Tokens per Sec:  9616118, Lr: 0.000240
2020-02-27 03:37:58,178 Epoch 3854: total training loss 0.00051
2020-02-27 03:37:58,178 EPOCH 3855
2020-02-27 03:38:08,809 Epoch 3855: total training loss 0.00050
2020-02-27 03:38:08,809 EPOCH 3856
2020-02-27 03:38:17,311 Epoch 3856 Step:   428000 Batch Loss:     0.000004 Tokens per Sec: 10390329, Lr: 0.000240
2020-02-27 03:38:18,736 Epoch 3856: total training loss 0.00050
2020-02-27 03:38:18,736 EPOCH 3857
2020-02-27 03:38:28,592 Epoch 3857: total training loss 0.00050
2020-02-27 03:38:28,592 EPOCH 3858
2020-02-27 03:38:38,465 Epoch 3858: total training loss 0.00049
2020-02-27 03:38:38,465 EPOCH 3859
2020-02-27 03:38:39,533 Epoch 3859 Step:   428250 Batch Loss:     0.000003 Tokens per Sec: 10299773, Lr: 0.000240
2020-02-27 03:38:48,230 Epoch 3859: total training loss 0.00050
2020-02-27 03:38:48,231 EPOCH 3860
2020-02-27 03:38:58,069 Epoch 3860: total training loss 0.00049
2020-02-27 03:38:58,069 EPOCH 3861
2020-02-27 03:39:01,557 Epoch 3861 Step:   428500 Batch Loss:     0.000007 Tokens per Sec: 10655240, Lr: 0.000240
2020-02-27 03:39:07,890 Epoch 3861: total training loss 0.00049
2020-02-27 03:39:07,890 EPOCH 3862
2020-02-27 03:39:17,871 Epoch 3862: total training loss 0.00050
2020-02-27 03:39:17,872 EPOCH 3863
2020-02-27 03:39:24,069 Epoch 3863 Step:   428750 Batch Loss:     0.000006 Tokens per Sec: 10091568, Lr: 0.000240
2020-02-27 03:39:28,055 Epoch 3863: total training loss 0.00049
2020-02-27 03:39:28,056 EPOCH 3864
2020-02-27 03:39:38,278 Epoch 3864: total training loss 0.00050
2020-02-27 03:39:38,278 EPOCH 3865
2020-02-27 03:39:47,425 Epoch 3865 Step:   429000 Batch Loss:     0.000002 Tokens per Sec:  9955757, Lr: 0.000240
2020-02-27 03:39:48,799 Epoch 3865: total training loss 0.00049
2020-02-27 03:39:48,799 EPOCH 3866
2020-02-27 03:39:59,624 Epoch 3866: total training loss 0.00050
2020-02-27 03:39:59,625 EPOCH 3867
2020-02-27 03:40:09,622 Epoch 3867: total training loss 0.00050
2020-02-27 03:40:09,623 EPOCH 3868
2020-02-27 03:40:10,782 Epoch 3868 Step:   429250 Batch Loss:     0.000004 Tokens per Sec:  9937570, Lr: 0.000240
2020-02-27 03:40:19,721 Epoch 3868: total training loss 0.00049
2020-02-27 03:40:19,722 EPOCH 3869
2020-02-27 03:40:29,922 Epoch 3869: total training loss 0.00050
2020-02-27 03:40:29,922 EPOCH 3870
2020-02-27 03:40:33,716 Epoch 3870 Step:   429500 Batch Loss:     0.000006 Tokens per Sec: 10239652, Lr: 0.000240
2020-02-27 03:40:39,988 Epoch 3870: total training loss 0.00050
2020-02-27 03:40:39,988 EPOCH 3871
2020-02-27 03:40:49,822 Epoch 3871: total training loss 0.00049
2020-02-27 03:40:49,823 EPOCH 3872
2020-02-27 03:40:55,709 Epoch 3872 Step:   429750 Batch Loss:     0.000003 Tokens per Sec: 10454942, Lr: 0.000240
2020-02-27 03:40:59,621 Epoch 3872: total training loss 0.00049
2020-02-27 03:40:59,621 EPOCH 3873
2020-02-27 03:41:10,053 Epoch 3873: total training loss 0.00050
2020-02-27 03:41:10,054 EPOCH 3874
2020-02-27 03:41:18,954 Epoch 3874 Step:   430000 Batch Loss:     0.000005 Tokens per Sec: 10058393, Lr: 0.000240
2020-02-27 03:41:18,955 Model noise rate: 5
2020-02-27 03:42:22,862 Validation result at epoch 3874, step   430000: Val DTW Score:  10.78, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0436, GT DTW Score:      nan, duration: 63.9072s
2020-02-27 03:42:24,214 Epoch 3874: total training loss 0.00049
2020-02-27 03:42:24,215 EPOCH 3875
2020-02-27 03:42:34,396 Epoch 3875: total training loss 0.00049
2020-02-27 03:42:34,397 EPOCH 3876
2020-02-27 03:42:44,567 Epoch 3876: total training loss 0.00049
2020-02-27 03:42:44,568 EPOCH 3877
2020-02-27 03:42:45,901 Epoch 3877 Step:   430250 Batch Loss:     0.000007 Tokens per Sec: 10450335, Lr: 0.000240
2020-02-27 03:42:54,716 Epoch 3877: total training loss 0.00049
2020-02-27 03:42:54,717 EPOCH 3878
2020-02-27 03:43:04,857 Epoch 3878: total training loss 0.00050
2020-02-27 03:43:04,858 EPOCH 3879
2020-02-27 03:43:08,524 Epoch 3879 Step:   430500 Batch Loss:     0.000004 Tokens per Sec: 10424192, Lr: 0.000240
2020-02-27 03:43:14,725 Epoch 3879: total training loss 0.00050
2020-02-27 03:43:14,726 EPOCH 3880
2020-02-27 03:43:24,467 Epoch 3880: total training loss 0.00050
2020-02-27 03:43:24,468 EPOCH 3881
2020-02-27 03:43:30,843 Epoch 3881 Step:   430750 Batch Loss:     0.000008 Tokens per Sec: 10253354, Lr: 0.000240
2020-02-27 03:43:34,493 Epoch 3881: total training loss 0.00050
2020-02-27 03:43:34,493 EPOCH 3882
2020-02-27 03:43:44,657 Epoch 3882: total training loss 0.00049
2020-02-27 03:43:44,658 EPOCH 3883
2020-02-27 03:43:53,551 Epoch 3883 Step:   431000 Batch Loss:     0.000003 Tokens per Sec: 10105650, Lr: 0.000240
2020-02-27 03:43:54,745 Epoch 3883: total training loss 0.00049
2020-02-27 03:43:54,745 EPOCH 3884
2020-02-27 03:44:04,933 Epoch 3884: total training loss 0.00050
2020-02-27 03:44:04,933 EPOCH 3885
2020-02-27 03:44:15,388 Epoch 3885: total training loss 0.00049
2020-02-27 03:44:15,389 EPOCH 3886
2020-02-27 03:44:16,851 Epoch 3886 Step:   431250 Batch Loss:     0.000004 Tokens per Sec: 10201615, Lr: 0.000240
2020-02-27 03:44:25,468 Epoch 3886: total training loss 0.00049
2020-02-27 03:44:25,469 EPOCH 3887
2020-02-27 03:44:35,383 Epoch 3887: total training loss 0.00049
2020-02-27 03:44:35,383 EPOCH 3888
2020-02-27 03:44:39,381 Epoch 3888 Step:   431500 Batch Loss:     0.000007 Tokens per Sec: 10479989, Lr: 0.000240
2020-02-27 03:44:45,262 Epoch 3888: total training loss 0.00049
2020-02-27 03:44:45,263 EPOCH 3889
2020-02-27 03:44:55,238 Epoch 3889: total training loss 0.00049
2020-02-27 03:44:55,239 EPOCH 3890
2020-02-27 03:45:01,733 Epoch 3890 Step:   431750 Batch Loss:     0.000003 Tokens per Sec: 10188907, Lr: 0.000240
2020-02-27 03:45:05,182 Epoch 3890: total training loss 0.00051
2020-02-27 03:45:05,183 EPOCH 3891
2020-02-27 03:45:15,140 Epoch 3891: total training loss 0.00049
2020-02-27 03:45:15,141 EPOCH 3892
2020-02-27 03:45:23,942 Epoch 3892 Step:   432000 Batch Loss:     0.000004 Tokens per Sec: 10410670, Lr: 0.000240
2020-02-27 03:45:25,028 Epoch 3892: total training loss 0.00049
2020-02-27 03:45:25,028 EPOCH 3893
2020-02-27 03:45:34,987 Epoch 3893: total training loss 0.00049
2020-02-27 03:45:34,988 EPOCH 3894
2020-02-27 03:45:45,046 Epoch 3894: total training loss 0.00050
2020-02-27 03:45:45,047 EPOCH 3895
2020-02-27 03:45:46,563 Epoch 3895 Step:   432250 Batch Loss:     0.000004 Tokens per Sec: 10136910, Lr: 0.000240
2020-02-27 03:45:55,219 Epoch 3895: total training loss 0.00050
2020-02-27 03:45:55,220 EPOCH 3896
2020-02-27 03:46:05,448 Epoch 3896: total training loss 0.00049
2020-02-27 03:46:05,448 EPOCH 3897
2020-02-27 03:46:09,393 Epoch 3897 Step:   432500 Batch Loss:     0.000004 Tokens per Sec:  9906973, Lr: 0.000240
2020-02-27 03:46:15,625 Epoch 3897: total training loss 0.00049
2020-02-27 03:46:15,626 EPOCH 3898
2020-02-27 03:46:25,769 Epoch 3898: total training loss 0.00050
2020-02-27 03:46:25,771 EPOCH 3899
2020-02-27 03:46:32,207 Epoch 3899 Step:   432750 Batch Loss:     0.000005 Tokens per Sec: 10138957, Lr: 0.000240
2020-02-27 03:46:35,874 Epoch 3899: total training loss 0.00049
2020-02-27 03:46:35,874 EPOCH 3900
2020-02-27 03:46:46,053 Epoch 3900: total training loss 0.00049
2020-02-27 03:46:46,054 EPOCH 3901
2020-02-27 03:46:55,211 Epoch 3901 Step:   433000 Batch Loss:     0.000003 Tokens per Sec: 10175989, Lr: 0.000240
2020-02-27 03:46:56,175 Epoch 3901: total training loss 0.00049
2020-02-27 03:46:56,176 EPOCH 3902
2020-02-27 03:47:06,427 Epoch 3902: total training loss 0.00049
2020-02-27 03:47:06,427 EPOCH 3903
2020-02-27 03:47:16,518 Epoch 3903: total training loss 0.00049
2020-02-27 03:47:16,519 EPOCH 3904
2020-02-27 03:47:18,274 Epoch 3904 Step:   433250 Batch Loss:     0.000007 Tokens per Sec: 10378206, Lr: 0.000240
2020-02-27 03:47:26,520 Epoch 3904: total training loss 0.00049
2020-02-27 03:47:26,522 EPOCH 3905
2020-02-27 03:47:36,305 Epoch 3905: total training loss 0.00049
2020-02-27 03:47:36,305 EPOCH 3906
2020-02-27 03:47:40,202 Epoch 3906 Step:   433500 Batch Loss:     0.000005 Tokens per Sec: 10329325, Lr: 0.000240
2020-02-27 03:47:46,182 Epoch 3906: total training loss 0.00049
2020-02-27 03:47:46,182 EPOCH 3907
2020-02-27 03:47:56,123 Epoch 3907: total training loss 0.00051
2020-02-27 03:47:56,124 EPOCH 3908
2020-02-27 03:48:02,179 Epoch 3908 Step:   433750 Batch Loss:     0.000004 Tokens per Sec: 10609268, Lr: 0.000240
2020-02-27 03:48:05,704 Epoch 3908: total training loss 0.00051
2020-02-27 03:48:05,705 EPOCH 3909
2020-02-27 03:48:15,658 Epoch 3909: total training loss 0.00050
2020-02-27 03:48:15,659 EPOCH 3910
2020-02-27 03:48:24,705 Epoch 3910 Step:   434000 Batch Loss:     0.000004 Tokens per Sec: 10287180, Lr: 0.000240
2020-02-27 03:48:25,635 Epoch 3910: total training loss 0.00050
2020-02-27 03:48:25,636 EPOCH 3911
2020-02-27 03:48:35,535 Epoch 3911: total training loss 0.00049
2020-02-27 03:48:35,536 EPOCH 3912
2020-02-27 03:48:45,586 Epoch 3912: total training loss 0.00049
2020-02-27 03:48:45,586 EPOCH 3913
2020-02-27 03:48:47,161 Epoch 3913 Step:   434250 Batch Loss:     0.000006 Tokens per Sec: 10319939, Lr: 0.000240
2020-02-27 03:48:55,683 Epoch 3913: total training loss 0.00049
2020-02-27 03:48:55,683 EPOCH 3914
2020-02-27 03:49:05,503 Epoch 3914: total training loss 0.00050
2020-02-27 03:49:05,503 EPOCH 3915
2020-02-27 03:49:09,426 Epoch 3915 Step:   434500 Batch Loss:     0.000004 Tokens per Sec: 10178911, Lr: 0.000240
2020-02-27 03:49:15,312 Epoch 3915: total training loss 0.00050
2020-02-27 03:49:15,313 EPOCH 3916
2020-02-27 03:49:24,975 Epoch 3916: total training loss 0.00049
2020-02-27 03:49:24,976 EPOCH 3917
2020-02-27 03:49:31,477 Epoch 3917 Step:   434750 Batch Loss:     0.000004 Tokens per Sec: 10614537, Lr: 0.000240
2020-02-27 03:49:34,764 Epoch 3917: total training loss 0.00049
2020-02-27 03:49:34,765 EPOCH 3918
2020-02-27 03:49:44,607 Epoch 3918: total training loss 0.00049
2020-02-27 03:49:44,608 EPOCH 3919
2020-02-27 03:49:53,588 Epoch 3919 Step:   435000 Batch Loss:     0.000004 Tokens per Sec: 10569811, Lr: 0.000240
2020-02-27 03:49:54,339 Epoch 3919: total training loss 0.00049
2020-02-27 03:49:54,339 EPOCH 3920
2020-02-27 03:50:04,083 Epoch 3920: total training loss 0.00049
2020-02-27 03:50:04,083 EPOCH 3921
2020-02-27 03:50:14,114 Epoch 3921: total training loss 0.00050
2020-02-27 03:50:14,115 EPOCH 3922
2020-02-27 03:50:15,732 Epoch 3922 Step:   435250 Batch Loss:     0.000004 Tokens per Sec: 10087563, Lr: 0.000240
2020-02-27 03:50:24,076 Epoch 3922: total training loss 0.00050
2020-02-27 03:50:24,077 EPOCH 3923
2020-02-27 03:50:33,808 Epoch 3923: total training loss 0.00050
2020-02-27 03:50:33,809 EPOCH 3924
2020-02-27 03:50:37,936 Epoch 3924 Step:   435500 Batch Loss:     0.000003 Tokens per Sec: 10629423, Lr: 0.000240
2020-02-27 03:50:43,434 Epoch 3924: total training loss 0.00049
2020-02-27 03:50:43,435 EPOCH 3925
2020-02-27 03:50:53,059 Epoch 3925: total training loss 0.00049
2020-02-27 03:50:53,060 EPOCH 3926
2020-02-27 03:50:59,766 Epoch 3926 Step:   435750 Batch Loss:     0.000005 Tokens per Sec: 10646999, Lr: 0.000240
2020-02-27 03:51:02,808 Epoch 3926: total training loss 0.00049
2020-02-27 03:51:02,809 EPOCH 3927
2020-02-27 03:51:12,524 Epoch 3927: total training loss 0.00049
2020-02-27 03:51:12,525 EPOCH 3928
2020-02-27 03:51:21,487 Epoch 3928 Step:   436000 Batch Loss:     0.000005 Tokens per Sec: 10694885, Lr: 0.000240
2020-02-27 03:51:22,162 Epoch 3928: total training loss 0.00050
2020-02-27 03:51:22,163 EPOCH 3929
2020-02-27 03:51:31,944 Epoch 3929: total training loss 0.00049
2020-02-27 03:51:31,945 EPOCH 3930
2020-02-27 03:51:41,688 Epoch 3930: total training loss 0.00049
2020-02-27 03:51:41,689 EPOCH 3931
2020-02-27 03:51:43,366 Epoch 3931 Step:   436250 Batch Loss:     0.000004 Tokens per Sec: 10414614, Lr: 0.000240
2020-02-27 03:51:51,372 Epoch 3931: total training loss 0.00050
2020-02-27 03:51:51,373 EPOCH 3932
2020-02-27 03:52:01,020 Epoch 3932: total training loss 0.00050
2020-02-27 03:52:01,021 EPOCH 3933
2020-02-27 03:52:05,376 Epoch 3933 Step:   436500 Batch Loss:     0.000006 Tokens per Sec: 10741315, Lr: 0.000240
2020-02-27 03:52:10,682 Epoch 3933: total training loss 0.00050
2020-02-27 03:52:10,682 EPOCH 3934
2020-02-27 03:52:20,346 Epoch 3934: total training loss 0.00051
2020-02-27 03:52:20,347 EPOCH 3935
2020-02-27 03:52:27,263 Epoch 3935 Step:   436750 Batch Loss:     0.000004 Tokens per Sec: 10357402, Lr: 0.000240
2020-02-27 03:52:30,304 Epoch 3935: total training loss 0.00050
2020-02-27 03:52:30,304 EPOCH 3936
2020-02-27 03:52:40,358 Epoch 3936: total training loss 0.00049
2020-02-27 03:52:40,359 EPOCH 3937
2020-02-27 03:52:49,635 Epoch 3937 Step:   437000 Batch Loss:     0.000005 Tokens per Sec: 10262742, Lr: 0.000240
2020-02-27 03:52:50,305 Epoch 3937: total training loss 0.00049
2020-02-27 03:52:50,305 EPOCH 3938
2020-02-27 03:53:00,208 Epoch 3938: total training loss 0.00049
2020-02-27 03:53:00,209 EPOCH 3939
2020-02-27 03:53:10,239 Epoch 3939: total training loss 0.00049
2020-02-27 03:53:10,240 EPOCH 3940
2020-02-27 03:53:11,910 Epoch 3940 Step:   437250 Batch Loss:     0.000004 Tokens per Sec: 10108054, Lr: 0.000240
2020-02-27 03:53:19,979 Epoch 3940: total training loss 0.00049
2020-02-27 03:53:19,980 EPOCH 3941
2020-02-27 03:53:29,722 Epoch 3941: total training loss 0.00049
2020-02-27 03:53:29,723 EPOCH 3942
2020-02-27 03:53:33,928 Epoch 3942 Step:   437500 Batch Loss:     0.000004 Tokens per Sec: 10320929, Lr: 0.000240
2020-02-27 03:53:39,662 Epoch 3942: total training loss 0.00049
2020-02-27 03:53:39,663 EPOCH 3943
2020-02-27 03:53:49,450 Epoch 3943: total training loss 0.00049
2020-02-27 03:53:49,451 EPOCH 3944
2020-02-27 03:53:56,422 Epoch 3944 Step:   437750 Batch Loss:     0.000006 Tokens per Sec: 10512956, Lr: 0.000240
2020-02-27 03:53:59,203 Epoch 3944: total training loss 0.00049
2020-02-27 03:53:59,204 EPOCH 3945
2020-02-27 03:54:08,919 Epoch 3945: total training loss 0.00049
2020-02-27 03:54:08,919 EPOCH 3946
2020-02-27 03:54:18,278 Epoch 3946 Step:   438000 Batch Loss:     0.000003 Tokens per Sec: 10195419, Lr: 0.000240
2020-02-27 03:54:18,881 Epoch 3946: total training loss 0.00050
2020-02-27 03:54:18,882 EPOCH 3947
2020-02-27 03:54:28,937 Epoch 3947: total training loss 0.00049
2020-02-27 03:54:28,937 EPOCH 3948
2020-02-27 03:54:38,910 Epoch 3948: total training loss 0.00050
2020-02-27 03:54:38,911 EPOCH 3949
2020-02-27 03:54:40,922 Epoch 3949 Step:   438250 Batch Loss:     0.000003 Tokens per Sec: 10297322, Lr: 0.000240
2020-02-27 03:54:48,956 Epoch 3949: total training loss 0.00049
2020-02-27 03:54:48,957 EPOCH 3950
2020-02-27 03:54:59,096 Epoch 3950: total training loss 0.00049
2020-02-27 03:54:59,097 EPOCH 3951
2020-02-27 03:55:03,773 Epoch 3951 Step:   438500 Batch Loss:     0.000004 Tokens per Sec:  9923379, Lr: 0.000240
2020-02-27 03:55:09,350 Epoch 3951: total training loss 0.00049
2020-02-27 03:55:09,352 EPOCH 3952
2020-02-27 03:55:19,500 Epoch 3952: total training loss 0.00050
2020-02-27 03:55:19,501 EPOCH 3953
2020-02-27 03:55:26,288 Epoch 3953 Step:   438750 Batch Loss:     0.000003 Tokens per Sec: 10319705, Lr: 0.000240
2020-02-27 03:55:29,383 Epoch 3953: total training loss 0.00049
2020-02-27 03:55:29,384 EPOCH 3954
2020-02-27 03:55:39,394 Epoch 3954: total training loss 0.00050
2020-02-27 03:55:39,394 EPOCH 3955
2020-02-27 03:55:48,955 Epoch 3955 Step:   439000 Batch Loss:     0.000004 Tokens per Sec: 10314492, Lr: 0.000240
2020-02-27 03:55:49,339 Epoch 3955: total training loss 0.00050
2020-02-27 03:55:49,339 EPOCH 3956
2020-02-27 03:55:59,060 Epoch 3956: total training loss 0.00050
2020-02-27 03:55:59,060 EPOCH 3957
2020-02-27 03:56:08,872 Epoch 3957: total training loss 0.00050
2020-02-27 03:56:08,874 EPOCH 3958
2020-02-27 03:56:11,099 Epoch 3958 Step:   439250 Batch Loss:     0.000004 Tokens per Sec:  9975696, Lr: 0.000240
2020-02-27 03:56:18,961 Epoch 3958: total training loss 0.00049
2020-02-27 03:56:18,961 EPOCH 3959
2020-02-27 03:56:29,015 Epoch 3959: total training loss 0.00049
2020-02-27 03:56:29,015 EPOCH 3960
2020-02-27 03:56:33,431 Epoch 3960 Step:   439500 Batch Loss:     0.000003 Tokens per Sec: 10468099, Lr: 0.000240
2020-02-27 03:56:38,885 Epoch 3960: total training loss 0.00049
2020-02-27 03:56:38,885 EPOCH 3961
2020-02-27 03:56:48,875 Epoch 3961: total training loss 0.00049
2020-02-27 03:56:48,875 EPOCH 3962
2020-02-27 03:56:56,135 Epoch 3962 Step:   439750 Batch Loss:     0.000004 Tokens per Sec: 10557526, Lr: 0.000240
2020-02-27 03:56:58,775 Epoch 3962: total training loss 0.00049
2020-02-27 03:56:58,776 EPOCH 3963
2020-02-27 03:57:08,660 Epoch 3963: total training loss 0.00049
2020-02-27 03:57:08,661 EPOCH 3964
2020-02-27 03:57:18,365 Epoch 3964 Step:   440000 Batch Loss:     0.000008 Tokens per Sec: 10255535, Lr: 0.000240
2020-02-27 03:57:18,366 Model noise rate: 5
2020-02-27 03:58:21,757 Validation result at epoch 3964, step   440000: Val DTW Score:  10.78, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0436, GT DTW Score:      nan, duration: 63.3904s
2020-02-27 03:58:22,103 Epoch 3964: total training loss 0.00049
2020-02-27 03:58:22,104 EPOCH 3965
2020-02-27 03:58:32,363 Epoch 3965: total training loss 0.00049
2020-02-27 03:58:32,364 EPOCH 3966
2020-02-27 03:58:42,577 Epoch 3966: total training loss 0.00050
2020-02-27 03:58:42,578 EPOCH 3967
2020-02-27 03:58:44,964 Epoch 3967 Step:   440250 Batch Loss:     0.000004 Tokens per Sec: 10297876, Lr: 0.000240
2020-02-27 03:58:52,480 Epoch 3967: total training loss 0.00050
2020-02-27 03:58:52,480 EPOCH 3968
2020-02-27 03:59:02,485 Epoch 3968: total training loss 0.00048
2020-02-27 03:59:02,486 EPOCH 3969
2020-02-27 03:59:07,020 Epoch 3969 Step:   440500 Batch Loss:     0.000005 Tokens per Sec: 10199572, Lr: 0.000240
2020-02-27 03:59:12,384 Epoch 3969: total training loss 0.00049
2020-02-27 03:59:12,384 EPOCH 3970
2020-02-27 03:59:22,092 Epoch 3970: total training loss 0.00049
2020-02-27 03:59:22,093 EPOCH 3971
2020-02-27 03:59:29,165 Epoch 3971 Step:   440750 Batch Loss:     0.000004 Tokens per Sec: 10504118, Lr: 0.000240
2020-02-27 03:59:31,889 Epoch 3971: total training loss 0.00049
2020-02-27 03:59:31,889 EPOCH 3972
2020-02-27 03:59:41,838 Epoch 3972: total training loss 0.00049
2020-02-27 03:59:41,839 EPOCH 3973
2020-02-27 03:59:51,536 Epoch 3973 Step:   441000 Batch Loss:     0.000004 Tokens per Sec: 10276739, Lr: 0.000240
2020-02-27 03:59:51,853 Epoch 3973: total training loss 0.00049
2020-02-27 03:59:51,853 EPOCH 3974
2020-02-27 04:00:02,017 Epoch 3974: total training loss 0.00049
2020-02-27 04:00:02,018 EPOCH 3975
2020-02-27 04:00:12,471 Epoch 3975: total training loss 0.00050
2020-02-27 04:00:12,472 EPOCH 3976
2020-02-27 04:00:14,838 Epoch 3976 Step:   441250 Batch Loss:     0.000003 Tokens per Sec:  9617448, Lr: 0.000240
2020-02-27 04:00:23,037 Epoch 3976: total training loss 0.00050
2020-02-27 04:00:23,038 EPOCH 3977
2020-02-27 04:00:33,490 Epoch 3977: total training loss 0.00050
2020-02-27 04:00:33,491 EPOCH 3978
2020-02-27 04:00:38,241 Epoch 3978 Step:   441500 Batch Loss:     0.000004 Tokens per Sec: 10019913, Lr: 0.000240
2020-02-27 04:00:43,736 Epoch 3978: total training loss 0.00049
2020-02-27 04:00:43,737 EPOCH 3979
2020-02-27 04:00:53,984 Epoch 3979: total training loss 0.00050
2020-02-27 04:00:53,985 EPOCH 3980
2020-02-27 04:01:01,009 Epoch 3980 Step:   441750 Batch Loss:     0.000005 Tokens per Sec: 10381945, Lr: 0.000240
2020-02-27 04:01:03,919 Epoch 3980: total training loss 0.00049
2020-02-27 04:01:03,919 EPOCH 3981
2020-02-27 04:01:13,830 Epoch 3981: total training loss 0.00049
2020-02-27 04:01:13,831 EPOCH 3982
2020-02-27 04:01:23,516 Epoch 3982 Step:   442000 Batch Loss:     0.000007 Tokens per Sec: 10443416, Lr: 0.000240
2020-02-27 04:01:23,703 Epoch 3982: total training loss 0.00049
2020-02-27 04:01:23,703 EPOCH 3983
2020-02-27 04:01:34,012 Epoch 3983: total training loss 0.00049
2020-02-27 04:01:34,014 EPOCH 3984
2020-02-27 04:01:44,305 Epoch 3984: total training loss 0.00050
2020-02-27 04:01:44,306 EPOCH 3985
2020-02-27 04:01:46,979 Epoch 3985 Step:   442250 Batch Loss:     0.000004 Tokens per Sec:  9878613, Lr: 0.000240
2020-02-27 04:01:54,700 Epoch 3985: total training loss 0.00050
2020-02-27 04:01:54,701 EPOCH 3986
2020-02-27 04:02:04,881 Epoch 3986: total training loss 0.00049
2020-02-27 04:02:04,881 EPOCH 3987
2020-02-27 04:02:09,931 Epoch 3987 Step:   442500 Batch Loss:     0.000005 Tokens per Sec:  9888848, Lr: 0.000240
2020-02-27 04:02:15,017 Epoch 3987: total training loss 0.00049
2020-02-27 04:02:15,018 EPOCH 3988
2020-02-27 04:02:25,000 Epoch 3988: total training loss 0.00049
2020-02-27 04:02:25,001 EPOCH 3989
2020-02-27 04:02:32,404 Epoch 3989 Step:   442750 Batch Loss:     0.000005 Tokens per Sec: 10393086, Lr: 0.000240
2020-02-27 04:02:34,919 Epoch 3989: total training loss 0.00049
2020-02-27 04:02:34,920 EPOCH 3990
2020-02-27 04:02:44,797 Epoch 3990: total training loss 0.00049
2020-02-27 04:02:44,799 EPOCH 3991
2020-02-27 04:02:54,851 Epoch 3991 Step:   443000 Batch Loss:     0.000004 Tokens per Sec: 10159079, Lr: 0.000240
2020-02-27 04:02:54,951 Epoch 3991: total training loss 0.00049
2020-02-27 04:02:54,951 EPOCH 3992
2020-02-27 04:03:04,798 Epoch 3992: total training loss 0.00049
2020-02-27 04:03:04,798 EPOCH 3993
2020-02-27 04:03:15,138 Epoch 3993: total training loss 0.00049
2020-02-27 04:03:15,140 EPOCH 3994
2020-02-27 04:03:17,639 Epoch 3994 Step:   443250 Batch Loss:     0.000003 Tokens per Sec:  9765457, Lr: 0.000240
2020-02-27 04:03:25,323 Epoch 3994: total training loss 0.00049
2020-02-27 04:03:25,324 EPOCH 3995
2020-02-27 04:03:35,557 Epoch 3995: total training loss 0.00049
2020-02-27 04:03:35,558 EPOCH 3996
2020-02-27 04:03:40,800 Epoch 3996 Step:   443500 Batch Loss:     0.000007 Tokens per Sec: 10170068, Lr: 0.000240
2020-02-27 04:03:45,757 Epoch 3996: total training loss 0.00050
2020-02-27 04:03:45,758 EPOCH 3997
2020-02-27 04:03:55,991 Epoch 3997: total training loss 0.00049
2020-02-27 04:03:55,992 EPOCH 3998
2020-02-27 04:04:03,417 Epoch 3998 Step:   443750 Batch Loss:     0.000004 Tokens per Sec: 10024740, Lr: 0.000240
2020-02-27 04:04:06,142 Epoch 3998: total training loss 0.00050
2020-02-27 04:04:06,142 EPOCH 3999
2020-02-27 04:04:16,166 Epoch 3999: total training loss 0.00049
2020-02-27 04:04:16,167 EPOCH 4000
2020-02-27 04:04:26,190 Epoch 4000 Step:   444000 Batch Loss:     0.000004 Tokens per Sec: 10231341, Lr: 0.000240
2020-02-27 04:04:26,191 Epoch 4000: total training loss 0.00049
2020-02-27 04:04:26,191 EPOCH 4001
2020-02-27 04:04:36,125 Epoch 4001: total training loss 0.00049
2020-02-27 04:04:36,125 EPOCH 4002
2020-02-27 04:04:46,013 Epoch 4002: total training loss 0.00051
2020-02-27 04:04:46,014 EPOCH 4003
2020-02-27 04:04:48,575 Epoch 4003 Step:   444250 Batch Loss:     0.000004 Tokens per Sec: 10080114, Lr: 0.000240
2020-02-27 04:04:56,065 Epoch 4003: total training loss 0.00051
2020-02-27 04:04:56,067 EPOCH 4004
2020-02-27 04:05:06,444 Epoch 4004: total training loss 0.00049
2020-02-27 04:05:06,445 EPOCH 4005
2020-02-27 04:05:11,852 Epoch 4005 Step:   444500 Batch Loss:     0.000004 Tokens per Sec:  9847702, Lr: 0.000240
2020-02-27 04:05:16,791 Epoch 4005: total training loss 0.00050
2020-02-27 04:05:16,791 EPOCH 4006
2020-02-27 04:05:26,957 Epoch 4006: total training loss 0.00049
2020-02-27 04:05:26,958 EPOCH 4007
2020-02-27 04:05:34,555 Epoch 4007 Step:   444750 Batch Loss:     0.000003 Tokens per Sec: 10128154, Lr: 0.000240
2020-02-27 04:05:37,083 Epoch 4007: total training loss 0.00049
2020-02-27 04:05:37,083 EPOCH 4008
2020-02-27 04:05:46,866 Epoch 4008: total training loss 0.00049
2020-02-27 04:05:46,866 EPOCH 4009
2020-02-27 04:05:56,746 Epoch 4009: total training loss 0.00049
2020-02-27 04:05:56,747 EPOCH 4010
2020-02-27 04:05:56,849 Epoch 4010 Step:   445000 Batch Loss:     0.000005 Tokens per Sec:  7955331, Lr: 0.000240
2020-02-27 04:06:06,721 Epoch 4010: total training loss 0.00049
2020-02-27 04:06:06,723 EPOCH 4011
2020-02-27 04:06:16,714 Epoch 4011: total training loss 0.00049
2020-02-27 04:06:16,714 EPOCH 4012
2020-02-27 04:06:19,359 Epoch 4012 Step:   445250 Batch Loss:     0.000005 Tokens per Sec: 10292220, Lr: 0.000240
2020-02-27 04:06:26,632 Epoch 4012: total training loss 0.00049
2020-02-27 04:06:26,633 EPOCH 4013
2020-02-27 04:06:36,701 Epoch 4013: total training loss 0.00048
2020-02-27 04:06:36,701 EPOCH 4014
2020-02-27 04:06:41,748 Epoch 4014 Step:   445500 Batch Loss:     0.000004 Tokens per Sec: 10107450, Lr: 0.000240
2020-02-27 04:06:46,764 Epoch 4014: total training loss 0.00049
2020-02-27 04:06:46,765 EPOCH 4015
2020-02-27 04:06:57,307 Epoch 4015: total training loss 0.00049
2020-02-27 04:06:57,307 EPOCH 4016
2020-02-27 04:07:05,169 Epoch 4016 Step:   445750 Batch Loss:     0.000005 Tokens per Sec: 10003399, Lr: 0.000240
2020-02-27 04:07:07,617 Epoch 4016: total training loss 0.00049
2020-02-27 04:07:07,617 EPOCH 4017
2020-02-27 04:07:17,838 Epoch 4017: total training loss 0.00052
2020-02-27 04:07:17,838 EPOCH 4018
2020-02-27 04:07:28,207 Epoch 4018: total training loss 0.00053
2020-02-27 04:07:28,208 EPOCH 4019
2020-02-27 04:07:28,461 Epoch 4019 Step:   446000 Batch Loss:     0.000006 Tokens per Sec:  8439997, Lr: 0.000240
2020-02-27 04:07:38,051 Epoch 4019: total training loss 0.00050
2020-02-27 04:07:38,052 EPOCH 4020
2020-02-27 04:07:47,973 Epoch 4020: total training loss 0.00049
2020-02-27 04:07:47,974 EPOCH 4021
2020-02-27 04:07:50,616 Epoch 4021 Step:   446250 Batch Loss:     0.000004 Tokens per Sec: 10103455, Lr: 0.000240
2020-02-27 04:07:57,917 Epoch 4021: total training loss 0.00049
2020-02-27 04:07:57,918 EPOCH 4022
2020-02-27 04:08:07,759 Epoch 4022: total training loss 0.00049
2020-02-27 04:08:07,760 EPOCH 4023
2020-02-27 04:08:12,794 Epoch 4023 Step:   446500 Batch Loss:     0.000006 Tokens per Sec: 10541603, Lr: 0.000240
2020-02-27 04:08:17,479 Epoch 4023: total training loss 0.00049
2020-02-27 04:08:17,480 EPOCH 4024
2020-02-27 04:08:27,414 Epoch 4024: total training loss 0.00049
2020-02-27 04:08:27,414 EPOCH 4025
2020-02-27 04:08:34,940 Epoch 4025 Step:   446750 Batch Loss:     0.000005 Tokens per Sec: 10437713, Lr: 0.000240
2020-02-27 04:08:37,208 Epoch 4025: total training loss 0.00049
2020-02-27 04:08:37,209 EPOCH 4026
2020-02-27 04:08:47,698 Epoch 4026: total training loss 0.00049
2020-02-27 04:08:47,699 EPOCH 4027
2020-02-27 04:08:58,037 Epoch 4027: total training loss 0.00049
2020-02-27 04:08:58,038 EPOCH 4028
2020-02-27 04:08:58,333 Epoch 4028 Step:   447000 Batch Loss:     0.000004 Tokens per Sec:  8426237, Lr: 0.000240
2020-02-27 04:09:08,726 Epoch 4028: total training loss 0.00049
2020-02-27 04:09:08,727 EPOCH 4029
2020-02-27 04:09:19,401 Epoch 4029: total training loss 0.00050
2020-02-27 04:09:19,403 EPOCH 4030
2020-02-27 04:09:22,430 Epoch 4030 Step:   447250 Batch Loss:     0.000003 Tokens per Sec:  9189454, Lr: 0.000240
2020-02-27 04:09:29,520 Epoch 4030: total training loss 0.00050
2020-02-27 04:09:29,520 EPOCH 4031
2020-02-27 04:09:39,327 Epoch 4031: total training loss 0.00049
2020-02-27 04:09:39,328 EPOCH 4032
2020-02-27 04:09:44,646 Epoch 4032 Step:   447500 Batch Loss:     0.000005 Tokens per Sec: 10633289, Lr: 0.000240
2020-02-27 04:09:49,121 Epoch 4032: total training loss 0.00049
2020-02-27 04:09:49,121 EPOCH 4033
2020-02-27 04:09:59,116 Epoch 4033: total training loss 0.00048
2020-02-27 04:09:59,117 EPOCH 4034
2020-02-27 04:10:06,802 Epoch 4034 Step:   447750 Batch Loss:     0.000004 Tokens per Sec: 10493397, Lr: 0.000240
2020-02-27 04:10:08,888 Epoch 4034: total training loss 0.00049
2020-02-27 04:10:08,889 EPOCH 4035
2020-02-27 04:10:18,690 Epoch 4035: total training loss 0.00049
2020-02-27 04:10:18,691 EPOCH 4036
2020-02-27 04:10:28,708 Epoch 4036: total training loss 0.00049
2020-02-27 04:10:28,708 EPOCH 4037
2020-02-27 04:10:29,159 Epoch 4037 Step:   448000 Batch Loss:     0.000006 Tokens per Sec: 10483588, Lr: 0.000240
2020-02-27 04:10:38,586 Epoch 4037: total training loss 0.00049
2020-02-27 04:10:38,587 EPOCH 4038
2020-02-27 04:10:48,713 Epoch 4038: total training loss 0.00049
2020-02-27 04:10:48,714 EPOCH 4039
2020-02-27 04:10:51,587 Epoch 4039 Step:   448250 Batch Loss:     0.000006 Tokens per Sec: 10084416, Lr: 0.000240
2020-02-27 04:10:58,695 Epoch 4039: total training loss 0.00049
2020-02-27 04:10:58,696 EPOCH 4040
2020-02-27 04:11:08,814 Epoch 4040: total training loss 0.00049
2020-02-27 04:11:08,814 EPOCH 4041
2020-02-27 04:11:14,319 Epoch 4041 Step:   448500 Batch Loss:     0.000007 Tokens per Sec: 10459150, Lr: 0.000240
2020-02-27 04:11:18,708 Epoch 4041: total training loss 0.00049
2020-02-27 04:11:18,709 EPOCH 4042
2020-02-27 04:11:28,502 Epoch 4042: total training loss 0.00049
2020-02-27 04:11:28,503 EPOCH 4043
2020-02-27 04:11:36,380 Epoch 4043 Step:   448750 Batch Loss:     0.000005 Tokens per Sec: 10139887, Lr: 0.000240
2020-02-27 04:11:38,534 Epoch 4043: total training loss 0.00049
2020-02-27 04:11:38,535 EPOCH 4044
2020-02-27 04:11:48,579 Epoch 4044: total training loss 0.00049
2020-02-27 04:11:48,580 EPOCH 4045
2020-02-27 04:11:58,520 Epoch 4045: total training loss 0.00049
2020-02-27 04:11:58,521 EPOCH 4046
2020-02-27 04:11:59,012 Epoch 4046 Step:   449000 Batch Loss:     0.000002 Tokens per Sec: 10129669, Lr: 0.000240
2020-02-27 04:12:08,434 Epoch 4046: total training loss 0.00049
2020-02-27 04:12:08,434 EPOCH 4047
2020-02-27 04:12:18,561 Epoch 4047: total training loss 0.00049
2020-02-27 04:12:18,562 EPOCH 4048
2020-02-27 04:12:21,723 Epoch 4048 Step:   449250 Batch Loss:     0.000004 Tokens per Sec: 10072662, Lr: 0.000240
2020-02-27 04:12:28,818 Epoch 4048: total training loss 0.00049
2020-02-27 04:12:28,818 EPOCH 4049
2020-02-27 04:12:39,230 Epoch 4049: total training loss 0.00050
2020-02-27 04:12:39,231 EPOCH 4050
2020-02-27 04:12:44,864 Epoch 4050 Step:   449500 Batch Loss:     0.000006 Tokens per Sec: 10090844, Lr: 0.000240
2020-02-27 04:12:49,351 Epoch 4050: total training loss 0.00049
2020-02-27 04:12:49,352 EPOCH 4051
2020-02-27 04:12:59,407 Epoch 4051: total training loss 0.00049
2020-02-27 04:12:59,407 EPOCH 4052
2020-02-27 04:13:07,467 Epoch 4052 Step:   449750 Batch Loss:     0.000004 Tokens per Sec: 10365184, Lr: 0.000240
2020-02-27 04:13:09,303 Epoch 4052: total training loss 0.00049
2020-02-27 04:13:09,304 EPOCH 4053
2020-02-27 04:13:18,981 Epoch 4053: total training loss 0.00049
2020-02-27 04:13:18,982 EPOCH 4054
2020-02-27 04:13:28,810 Epoch 4054: total training loss 0.00050
2020-02-27 04:13:28,811 EPOCH 4055
2020-02-27 04:13:29,581 Epoch 4055 Step:   450000 Batch Loss:     0.000004 Tokens per Sec: 10320146, Lr: 0.000240
2020-02-27 04:13:29,581 Model noise rate: 5
2020-02-27 04:14:29,167 Validation result at epoch 4055, step   450000: Val DTW Score:  10.78, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0434, GT DTW Score:      nan, duration: 59.5852s
2020-02-27 04:14:38,972 Epoch 4055: total training loss 0.00049
2020-02-27 04:14:38,973 EPOCH 4056
2020-02-27 04:14:49,567 Epoch 4056: total training loss 0.00049
2020-02-27 04:14:49,567 EPOCH 4057
2020-02-27 04:14:52,632 Epoch 4057 Step:   450250 Batch Loss:     0.000002 Tokens per Sec: 10074150, Lr: 0.000240
2020-02-27 04:14:59,537 Epoch 4057: total training loss 0.00049
2020-02-27 04:14:59,538 EPOCH 4058
2020-02-27 04:15:09,683 Epoch 4058: total training loss 0.00049
2020-02-27 04:15:09,684 EPOCH 4059
2020-02-27 04:15:15,412 Epoch 4059 Step:   450500 Batch Loss:     0.000004 Tokens per Sec: 10263358, Lr: 0.000240
2020-02-27 04:15:19,689 Epoch 4059: total training loss 0.00049
2020-02-27 04:15:19,690 EPOCH 4060
2020-02-27 04:15:29,478 Epoch 4060: total training loss 0.00049
2020-02-27 04:15:29,478 EPOCH 4061
2020-02-27 04:15:37,699 Epoch 4061 Step:   450750 Batch Loss:     0.000004 Tokens per Sec: 10292978, Lr: 0.000240
2020-02-27 04:15:39,477 Epoch 4061: total training loss 0.00049
2020-02-27 04:15:39,477 EPOCH 4062
2020-02-27 04:15:49,555 Epoch 4062: total training loss 0.00048
2020-02-27 04:15:49,556 EPOCH 4063
2020-02-27 04:15:59,401 Epoch 4063: total training loss 0.00049
2020-02-27 04:15:59,402 EPOCH 4064
2020-02-27 04:16:00,228 Epoch 4064 Step:   451000 Batch Loss:     0.000003 Tokens per Sec: 10761897, Lr: 0.000240
2020-02-27 04:16:09,298 Epoch 4064: total training loss 0.00049
2020-02-27 04:16:09,299 EPOCH 4065
2020-02-27 04:16:19,208 Epoch 4065: total training loss 0.00049
2020-02-27 04:16:19,209 EPOCH 4066
2020-02-27 04:16:22,272 Epoch 4066 Step:   451250 Batch Loss:     0.000005 Tokens per Sec: 10200047, Lr: 0.000240
2020-02-27 04:16:29,338 Epoch 4066: total training loss 0.00049
2020-02-27 04:16:29,339 EPOCH 4067
2020-02-27 04:16:39,672 Epoch 4067: total training loss 0.00049
2020-02-27 04:16:39,673 EPOCH 4068
2020-02-27 04:16:46,005 Epoch 4068 Step:   451500 Batch Loss:     0.000004 Tokens per Sec:  9420141, Lr: 0.000240
2020-02-27 04:16:50,507 Epoch 4068: total training loss 0.00049
2020-02-27 04:16:50,507 EPOCH 4069
2020-02-27 04:17:00,938 Epoch 4069: total training loss 0.00048
2020-02-27 04:17:00,939 EPOCH 4070
2020-02-27 04:17:09,729 Epoch 4070 Step:   451750 Batch Loss:     0.000005 Tokens per Sec:  9623884, Lr: 0.000240
2020-02-27 04:17:11,480 Epoch 4070: total training loss 0.00049
2020-02-27 04:17:11,480 EPOCH 4071
2020-02-27 04:17:21,554 Epoch 4071: total training loss 0.00049
2020-02-27 04:17:21,554 EPOCH 4072
2020-02-27 04:17:31,389 Epoch 4072: total training loss 0.00050
2020-02-27 04:17:31,389 EPOCH 4073
2020-02-27 04:17:32,177 Epoch 4073 Step:   452000 Batch Loss:     0.000005 Tokens per Sec: 10312911, Lr: 0.000240
2020-02-27 04:17:41,442 Epoch 4073: total training loss 0.00048
2020-02-27 04:17:41,442 EPOCH 4074
2020-02-27 04:17:51,300 Epoch 4074: total training loss 0.00049
2020-02-27 04:17:51,301 EPOCH 4075
2020-02-27 04:17:54,495 Epoch 4075 Step:   452250 Batch Loss:     0.000006 Tokens per Sec: 10431170, Lr: 0.000240
2020-02-27 04:18:01,085 Epoch 4075: total training loss 0.00049
2020-02-27 04:18:01,086 EPOCH 4076
2020-02-27 04:18:11,053 Epoch 4076: total training loss 0.00049
2020-02-27 04:18:11,055 EPOCH 4077
2020-02-27 04:18:16,772 Epoch 4077 Step:   452500 Batch Loss:     0.000004 Tokens per Sec: 10039234, Lr: 0.000240
2020-02-27 04:18:21,316 Epoch 4077: total training loss 0.00050
2020-02-27 04:18:21,317 EPOCH 4078
2020-02-27 04:18:31,565 Epoch 4078: total training loss 0.00049
2020-02-27 04:18:31,565 EPOCH 4079
2020-02-27 04:18:39,976 Epoch 4079 Step:   452750 Batch Loss:     0.000004 Tokens per Sec: 10138014, Lr: 0.000240
2020-02-27 04:18:41,685 Epoch 4079: total training loss 0.00049
2020-02-27 04:18:41,685 EPOCH 4080
2020-02-27 04:18:52,022 Epoch 4080: total training loss 0.00048
2020-02-27 04:18:52,022 EPOCH 4081
2020-02-27 04:19:01,874 Epoch 4081: total training loss 0.00049
2020-02-27 04:19:01,874 EPOCH 4082
2020-02-27 04:19:02,693 Epoch 4082 Step:   453000 Batch Loss:     0.000007 Tokens per Sec:  9778145, Lr: 0.000240
2020-02-27 04:19:12,004 Epoch 4082: total training loss 0.00049
2020-02-27 04:19:12,005 EPOCH 4083
2020-02-27 04:19:22,129 Epoch 4083: total training loss 0.00048
2020-02-27 04:19:22,130 EPOCH 4084
2020-02-27 04:19:25,799 Epoch 4084 Step:   453250 Batch Loss:     0.000004 Tokens per Sec: 10282884, Lr: 0.000240
2020-02-27 04:19:32,246 Epoch 4084: total training loss 0.00048
2020-02-27 04:19:32,247 EPOCH 4085
2020-02-27 04:19:42,030 Epoch 4085: total training loss 0.00050
2020-02-27 04:19:42,031 EPOCH 4086
2020-02-27 04:19:47,943 Epoch 4086 Step:   453500 Batch Loss:     0.000004 Tokens per Sec: 10226034, Lr: 0.000240
2020-02-27 04:19:52,025 Epoch 4086: total training loss 0.00049
2020-02-27 04:19:52,025 EPOCH 4087
2020-02-27 04:20:01,807 Epoch 4087: total training loss 0.00049
2020-02-27 04:20:01,809 EPOCH 4088
2020-02-27 04:20:10,525 Epoch 4088 Step:   453750 Batch Loss:     0.000004 Tokens per Sec: 10045082, Lr: 0.000240
2020-02-27 04:20:12,115 Epoch 4088: total training loss 0.00049
2020-02-27 04:20:12,116 EPOCH 4089
2020-02-27 04:20:22,216 Epoch 4089: total training loss 0.00049
2020-02-27 04:20:22,216 EPOCH 4090
2020-02-27 04:20:32,273 Epoch 4090: total training loss 0.00049
2020-02-27 04:20:32,274 EPOCH 4091
2020-02-27 04:20:33,229 Epoch 4091 Step:   454000 Batch Loss:     0.000004 Tokens per Sec:  9540408, Lr: 0.000240
2020-02-27 04:20:42,450 Epoch 4091: total training loss 0.00048
2020-02-27 04:20:42,451 EPOCH 4092
2020-02-27 04:20:52,484 Epoch 4092: total training loss 0.00049
2020-02-27 04:20:52,484 EPOCH 4093
2020-02-27 04:20:55,961 Epoch 4093 Step:   454250 Batch Loss:     0.000005 Tokens per Sec: 10184356, Lr: 0.000240
2020-02-27 04:21:02,530 Epoch 4093: total training loss 0.00049
2020-02-27 04:21:02,531 EPOCH 4094
2020-02-27 04:21:12,362 Epoch 4094: total training loss 0.00049
2020-02-27 04:21:12,362 EPOCH 4095
2020-02-27 04:21:17,923 Epoch 4095 Step:   454500 Batch Loss:     0.000003 Tokens per Sec: 10392571, Lr: 0.000240
2020-02-27 04:21:22,124 Epoch 4095: total training loss 0.00049
2020-02-27 04:21:22,124 EPOCH 4096
2020-02-27 04:21:31,967 Epoch 4096: total training loss 0.00050
2020-02-27 04:21:31,967 EPOCH 4097
2020-02-27 04:21:40,387 Epoch 4097 Step:   454750 Batch Loss:     0.000004 Tokens per Sec: 10401211, Lr: 0.000240
2020-02-27 04:21:41,937 Epoch 4097: total training loss 0.00049
2020-02-27 04:21:41,937 EPOCH 4098
2020-02-27 04:21:51,973 Epoch 4098: total training loss 0.00050
2020-02-27 04:21:51,974 EPOCH 4099
2020-02-27 04:22:02,600 Epoch 4099: total training loss 0.00049
2020-02-27 04:22:02,601 EPOCH 4100
2020-02-27 04:22:03,573 Epoch 4100 Step:   455000 Batch Loss:     0.000006 Tokens per Sec:  9883238, Lr: 0.000240
2020-02-27 04:22:13,031 Epoch 4100: total training loss 0.00049
2020-02-27 04:22:13,031 EPOCH 4101
2020-02-27 04:22:23,613 Epoch 4101: total training loss 0.00049
2020-02-27 04:22:23,614 EPOCH 4102
2020-02-27 04:22:27,295 Epoch 4102 Step:   455250 Batch Loss:     0.000006 Tokens per Sec:  8938877, Lr: 0.000240
2020-02-27 04:22:34,468 Epoch 4102: total training loss 0.00049
2020-02-27 04:22:34,469 EPOCH 4103
2020-02-27 04:22:44,441 Epoch 4103: total training loss 0.00048
2020-02-27 04:22:44,441 EPOCH 4104
2020-02-27 04:22:50,569 Epoch 4104 Step:   455500 Batch Loss:     0.000003 Tokens per Sec: 10298916, Lr: 0.000240
2020-02-27 04:22:54,339 Epoch 4104: total training loss 0.00050
2020-02-27 04:22:54,339 EPOCH 4105
2020-02-27 04:23:04,276 Epoch 4105: total training loss 0.00049
2020-02-27 04:23:04,276 EPOCH 4106
2020-02-27 04:23:12,752 Epoch 4106 Step:   455750 Batch Loss:     0.000004 Tokens per Sec: 10430277, Lr: 0.000240
2020-02-27 04:23:14,172 Epoch 4106: total training loss 0.00049
2020-02-27 04:23:14,172 EPOCH 4107
2020-02-27 04:23:24,138 Epoch 4107: total training loss 0.00049
2020-02-27 04:23:24,139 EPOCH 4108
2020-02-27 04:23:34,316 Epoch 4108: total training loss 0.00051
2020-02-27 04:23:34,317 EPOCH 4109
2020-02-27 04:23:35,338 Epoch 4109 Step:   456000 Batch Loss:     0.000004 Tokens per Sec: 10175216, Lr: 0.000240
2020-02-27 04:23:44,524 Epoch 4109: total training loss 0.00051
2020-02-27 04:23:44,525 EPOCH 4110
2020-02-27 04:23:54,681 Epoch 4110: total training loss 0.00049
2020-02-27 04:23:54,681 EPOCH 4111
2020-02-27 04:23:58,169 Epoch 4111 Step:   456250 Batch Loss:     0.000004 Tokens per Sec: 10316251, Lr: 0.000240
2020-02-27 04:24:04,751 Epoch 4111: total training loss 0.00049
2020-02-27 04:24:04,752 EPOCH 4112
2020-02-27 04:24:14,688 Epoch 4112: total training loss 0.00049
2020-02-27 04:24:14,688 EPOCH 4113
2020-02-27 04:24:20,915 Epoch 4113 Step:   456500 Batch Loss:     0.000006 Tokens per Sec: 10302887, Lr: 0.000240
2020-02-27 04:24:24,795 Epoch 4113: total training loss 0.00048
2020-02-27 04:24:24,796 EPOCH 4114
2020-02-27 04:24:34,929 Epoch 4114: total training loss 0.00049
2020-02-27 04:24:34,930 EPOCH 4115
2020-02-27 04:24:43,628 Epoch 4115 Step:   456750 Batch Loss:     0.000007 Tokens per Sec: 10053780, Lr: 0.000240
2020-02-27 04:24:45,043 Epoch 4115: total training loss 0.00049
2020-02-27 04:24:45,044 EPOCH 4116
2020-02-27 04:24:55,049 Epoch 4116: total training loss 0.00049
2020-02-27 04:24:55,050 EPOCH 4117
2020-02-27 04:25:04,941 Epoch 4117: total training loss 0.00049
2020-02-27 04:25:04,942 EPOCH 4118
2020-02-27 04:25:06,304 Epoch 4118 Step:   457000 Batch Loss:     0.000005 Tokens per Sec: 10056971, Lr: 0.000240
2020-02-27 04:25:15,058 Epoch 4118: total training loss 0.00049
2020-02-27 04:25:15,059 EPOCH 4119
2020-02-27 04:25:25,258 Epoch 4119: total training loss 0.00049
2020-02-27 04:25:25,259 EPOCH 4120
2020-02-27 04:25:29,228 Epoch 4120 Step:   457250 Batch Loss:     0.000004 Tokens per Sec: 10029930, Lr: 0.000240
2020-02-27 04:25:35,386 Epoch 4120: total training loss 0.00049
2020-02-27 04:25:35,387 EPOCH 4121
2020-02-27 04:25:45,617 Epoch 4121: total training loss 0.00049
2020-02-27 04:25:45,618 EPOCH 4122
2020-02-27 04:25:51,954 Epoch 4122 Step:   457500 Batch Loss:     0.000004 Tokens per Sec: 10161820, Lr: 0.000240
2020-02-27 04:25:55,784 Epoch 4122: total training loss 0.00049
2020-02-27 04:25:55,784 EPOCH 4123
2020-02-27 04:26:05,725 Epoch 4123: total training loss 0.00049
2020-02-27 04:26:05,725 EPOCH 4124
2020-02-27 04:26:14,356 Epoch 4124 Step:   457750 Batch Loss:     0.000004 Tokens per Sec: 10519106, Lr: 0.000240
2020-02-27 04:26:15,587 Epoch 4124: total training loss 0.00049
2020-02-27 04:26:15,587 EPOCH 4125
2020-02-27 04:26:25,638 Epoch 4125: total training loss 0.00049
2020-02-27 04:26:25,638 EPOCH 4126
2020-02-27 04:26:35,482 Epoch 4126: total training loss 0.00050
2020-02-27 04:26:35,483 EPOCH 4127
2020-02-27 04:26:36,705 Epoch 4127 Step:   458000 Batch Loss:     0.000005 Tokens per Sec: 10276268, Lr: 0.000240
2020-02-27 04:26:45,200 Epoch 4127: total training loss 0.00049
2020-02-27 04:26:45,201 EPOCH 4128
2020-02-27 04:26:55,074 Epoch 4128: total training loss 0.00050
2020-02-27 04:26:55,074 EPOCH 4129
2020-02-27 04:26:58,677 Epoch 4129 Step:   458250 Batch Loss:     0.000005 Tokens per Sec: 10308284, Lr: 0.000240
2020-02-27 04:27:04,970 Epoch 4129: total training loss 0.00049
2020-02-27 04:27:04,971 EPOCH 4130
2020-02-27 04:27:15,385 Epoch 4130: total training loss 0.00048
2020-02-27 04:27:15,385 EPOCH 4131
2020-02-27 04:27:22,200 Epoch 4131 Step:   458500 Batch Loss:     0.000002 Tokens per Sec:  9277185, Lr: 0.000240
2020-02-27 04:27:26,182 Epoch 4131: total training loss 0.00049
2020-02-27 04:27:26,183 EPOCH 4132
2020-02-27 04:27:36,762 Epoch 4132: total training loss 0.00048
2020-02-27 04:27:36,762 EPOCH 4133
2020-02-27 04:27:46,003 Epoch 4133 Step:   458750 Batch Loss:     0.000006 Tokens per Sec:  9784991, Lr: 0.000240
2020-02-27 04:27:47,186 Epoch 4133: total training loss 0.00049
2020-02-27 04:27:47,186 EPOCH 4134
2020-02-27 04:27:57,199 Epoch 4134: total training loss 0.00049
2020-02-27 04:27:57,199 EPOCH 4135
2020-02-27 04:28:07,215 Epoch 4135: total training loss 0.00048
2020-02-27 04:28:07,215 EPOCH 4136
2020-02-27 04:28:08,751 Epoch 4136 Step:   459000 Batch Loss:     0.000004 Tokens per Sec: 10310032, Lr: 0.000240
2020-02-27 04:28:17,162 Epoch 4136: total training loss 0.00049
2020-02-27 04:28:17,162 EPOCH 4137
2020-02-27 04:28:27,200 Epoch 4137: total training loss 0.00049
2020-02-27 04:28:27,200 EPOCH 4138
2020-02-27 04:28:31,038 Epoch 4138 Step:   459250 Batch Loss:     0.000004 Tokens per Sec: 10343510, Lr: 0.000240
2020-02-27 04:28:37,023 Epoch 4138: total training loss 0.00054
2020-02-27 04:28:37,024 EPOCH 4139
2020-02-27 04:28:46,810 Epoch 4139: total training loss 0.00050
2020-02-27 04:28:46,810 EPOCH 4140
2020-02-27 04:28:53,124 Epoch 4140 Step:   459500 Batch Loss:     0.000005 Tokens per Sec: 10400342, Lr: 0.000240
2020-02-27 04:28:56,678 Epoch 4140: total training loss 0.00049
2020-02-27 04:28:56,679 EPOCH 4141
2020-02-27 04:29:06,720 Epoch 4141: total training loss 0.00049
2020-02-27 04:29:06,721 EPOCH 4142
2020-02-27 04:29:15,683 Epoch 4142 Step:   459750 Batch Loss:     0.000005 Tokens per Sec: 10180353, Lr: 0.000240
2020-02-27 04:29:16,808 Epoch 4142: total training loss 0.00049
2020-02-27 04:29:16,808 EPOCH 4143
2020-02-27 04:29:26,876 Epoch 4143: total training loss 0.00048
2020-02-27 04:29:26,876 EPOCH 4144
2020-02-27 04:29:36,776 Epoch 4144: total training loss 0.00048
2020-02-27 04:29:36,776 EPOCH 4145
2020-02-27 04:29:38,307 Epoch 4145 Step:   460000 Batch Loss:     0.000004 Tokens per Sec: 10495912, Lr: 0.000240
2020-02-27 04:29:38,307 Model noise rate: 5
2020-02-27 04:30:39,997 Validation result at epoch 4145, step   460000: Val DTW Score:  10.81, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0436, GT DTW Score:      nan, duration: 61.6898s
2020-02-27 04:30:48,676 Epoch 4145: total training loss 0.00049
2020-02-27 04:30:48,676 EPOCH 4146
2020-02-27 04:30:59,105 Epoch 4146: total training loss 0.00048
2020-02-27 04:30:59,105 EPOCH 4147
2020-02-27 04:31:03,060 Epoch 4147 Step:   460250 Batch Loss:     0.000004 Tokens per Sec:  9821545, Lr: 0.000240
2020-02-27 04:31:09,327 Epoch 4147: total training loss 0.00049
2020-02-27 04:31:09,328 EPOCH 4148
2020-02-27 04:31:19,507 Epoch 4148: total training loss 0.00049
2020-02-27 04:31:19,507 EPOCH 4149
2020-02-27 04:31:26,306 Epoch 4149 Step:   460500 Batch Loss:     0.000004 Tokens per Sec: 10067865, Lr: 0.000240
2020-02-27 04:31:29,815 Epoch 4149: total training loss 0.00050
2020-02-27 04:31:29,816 EPOCH 4150
2020-02-27 04:31:39,735 Epoch 4150: total training loss 0.00049
2020-02-27 04:31:39,735 EPOCH 4151
2020-02-27 04:31:48,720 Epoch 4151 Step:   460750 Batch Loss:     0.000005 Tokens per Sec: 10431348, Lr: 0.000240
2020-02-27 04:31:49,702 Epoch 4151: total training loss 0.00048
2020-02-27 04:31:49,703 EPOCH 4152
2020-02-27 04:31:59,790 Epoch 4152: total training loss 0.00049
2020-02-27 04:31:59,791 EPOCH 4153
2020-02-27 04:32:09,690 Epoch 4153: total training loss 0.00049
2020-02-27 04:32:09,690 EPOCH 4154
2020-02-27 04:32:11,284 Epoch 4154 Step:   461000 Batch Loss:     0.000005 Tokens per Sec: 10513894, Lr: 0.000240
2020-02-27 04:32:19,462 Epoch 4154: total training loss 0.00049
2020-02-27 04:32:19,463 EPOCH 4155
2020-02-27 04:32:29,406 Epoch 4155: total training loss 0.00049
2020-02-27 04:32:29,407 EPOCH 4156
2020-02-27 04:32:33,309 Epoch 4156 Step:   461250 Batch Loss:     0.000006 Tokens per Sec: 10103612, Lr: 0.000240
2020-02-27 04:32:39,797 Epoch 4156: total training loss 0.00049
2020-02-27 04:32:39,797 EPOCH 4157
2020-02-27 04:32:50,212 Epoch 4157: total training loss 0.00050
2020-02-27 04:32:50,213 EPOCH 4158
2020-02-27 04:32:57,220 Epoch 4158 Step:   461500 Batch Loss:     0.000005 Tokens per Sec:  9630026, Lr: 0.000240
2020-02-27 04:33:00,824 Epoch 4158: total training loss 0.00057
2020-02-27 04:33:00,825 EPOCH 4159
2020-02-27 04:33:11,286 Epoch 4159: total training loss 0.00050
2020-02-27 04:33:11,287 EPOCH 4160
2020-02-27 04:33:20,671 Epoch 4160 Step:   461750 Batch Loss:     0.000004 Tokens per Sec: 10056276, Lr: 0.000240
2020-02-27 04:33:21,565 Epoch 4160: total training loss 0.00049
2020-02-27 04:33:21,566 EPOCH 4161
2020-02-27 04:33:31,429 Epoch 4161: total training loss 0.00048
2020-02-27 04:33:31,429 EPOCH 4162
2020-02-27 04:33:41,326 Epoch 4162: total training loss 0.00049
2020-02-27 04:33:41,326 EPOCH 4163
2020-02-27 04:33:42,873 Epoch 4163 Step:   462000 Batch Loss:     0.000004 Tokens per Sec: 10075803, Lr: 0.000240
2020-02-27 04:33:51,274 Epoch 4163: total training loss 0.00048
2020-02-27 04:33:51,274 EPOCH 4164
2020-02-27 04:34:00,923 Epoch 4164: total training loss 0.00049
2020-02-27 04:34:00,924 EPOCH 4165
2020-02-27 04:34:04,813 Epoch 4165 Step:   462250 Batch Loss:     0.000003 Tokens per Sec: 10486030, Lr: 0.000240
2020-02-27 04:34:10,782 Epoch 4165: total training loss 0.00048
2020-02-27 04:34:10,783 EPOCH 4166
2020-02-27 04:34:20,712 Epoch 4166: total training loss 0.00049
2020-02-27 04:34:20,713 EPOCH 4167
2020-02-27 04:34:27,369 Epoch 4167 Step:   462500 Batch Loss:     0.000005 Tokens per Sec: 10491248, Lr: 0.000240
2020-02-27 04:34:30,658 Epoch 4167: total training loss 0.00048
2020-02-27 04:34:30,659 EPOCH 4168
2020-02-27 04:34:40,896 Epoch 4168: total training loss 0.00048
2020-02-27 04:34:40,897 EPOCH 4169
2020-02-27 04:34:50,166 Epoch 4169 Step:   462750 Batch Loss:     0.000005 Tokens per Sec: 10105065, Lr: 0.000240
2020-02-27 04:34:51,075 Epoch 4169: total training loss 0.00048
2020-02-27 04:34:51,075 EPOCH 4170
2020-02-27 04:35:01,365 Epoch 4170: total training loss 0.00048
2020-02-27 04:35:01,365 EPOCH 4171
2020-02-27 04:35:11,499 Epoch 4171: total training loss 0.00048
2020-02-27 04:35:11,501 EPOCH 4172
2020-02-27 04:35:13,264 Epoch 4172 Step:   463000 Batch Loss:     0.000004 Tokens per Sec: 10267651, Lr: 0.000240
2020-02-27 04:35:21,441 Epoch 4172: total training loss 0.00048
2020-02-27 04:35:21,441 EPOCH 4173
2020-02-27 04:35:31,379 Epoch 4173: total training loss 0.00049
2020-02-27 04:35:31,380 EPOCH 4174
2020-02-27 04:35:35,632 Epoch 4174 Step:   463250 Batch Loss:     0.000003 Tokens per Sec: 10484819, Lr: 0.000240
2020-02-27 04:35:41,203 Epoch 4174: total training loss 0.00049
2020-02-27 04:35:41,203 EPOCH 4175
2020-02-27 04:35:50,906 Epoch 4175: total training loss 0.00049
2020-02-27 04:35:50,907 EPOCH 4176
2020-02-27 04:35:57,254 Epoch 4176 Step:   463500 Batch Loss:     0.000003 Tokens per Sec: 10721890, Lr: 0.000240
2020-02-27 04:36:00,446 Epoch 4176: total training loss 0.00050
2020-02-27 04:36:00,447 EPOCH 4177
2020-02-27 04:36:10,191 Epoch 4177: total training loss 0.00050
2020-02-27 04:36:10,192 EPOCH 4178
2020-02-27 04:36:19,176 Epoch 4178 Step:   463750 Batch Loss:     0.000003 Tokens per Sec: 10521119, Lr: 0.000240
2020-02-27 04:36:19,937 Epoch 4178: total training loss 0.00049
2020-02-27 04:36:19,938 EPOCH 4179
2020-02-27 04:36:29,885 Epoch 4179: total training loss 0.00049
2020-02-27 04:36:29,886 EPOCH 4180
2020-02-27 04:36:39,665 Epoch 4180: total training loss 0.00049
2020-02-27 04:36:39,665 EPOCH 4181
2020-02-27 04:36:41,598 Epoch 4181 Step:   464000 Batch Loss:     0.000003 Tokens per Sec: 10116877, Lr: 0.000240
2020-02-27 04:36:49,705 Epoch 4181: total training loss 0.00049
2020-02-27 04:36:49,707 EPOCH 4182
2020-02-27 04:37:00,154 Epoch 4182: total training loss 0.00049
2020-02-27 04:37:00,154 EPOCH 4183
2020-02-27 04:37:04,379 Epoch 4183 Step:   464250 Batch Loss:     0.000002 Tokens per Sec: 10049873, Lr: 0.000240
2020-02-27 04:37:10,221 Epoch 4183: total training loss 0.00049
2020-02-27 04:37:10,221 EPOCH 4184
2020-02-27 04:37:20,486 Epoch 4184: total training loss 0.00049
2020-02-27 04:37:20,486 EPOCH 4185
2020-02-27 04:37:27,358 Epoch 4185 Step:   464500 Batch Loss:     0.000005 Tokens per Sec: 10081374, Lr: 0.000240
2020-02-27 04:37:30,630 Epoch 4185: total training loss 0.00048
2020-02-27 04:37:30,631 EPOCH 4186
2020-02-27 04:37:40,336 Epoch 4186: total training loss 0.00048
2020-02-27 04:37:40,337 EPOCH 4187
2020-02-27 04:37:49,478 Epoch 4187 Step:   464750 Batch Loss:     0.000007 Tokens per Sec: 10567903, Lr: 0.000240
2020-02-27 04:37:50,053 Epoch 4187: total training loss 0.00049
2020-02-27 04:37:50,054 EPOCH 4188
2020-02-27 04:37:59,733 Epoch 4188: total training loss 0.00050
2020-02-27 04:37:59,734 EPOCH 4189
2020-02-27 04:38:09,607 Epoch 4189: total training loss 0.00049
2020-02-27 04:38:09,607 EPOCH 4190
2020-02-27 04:38:11,404 Epoch 4190 Step:   465000 Batch Loss:     0.000005 Tokens per Sec: 10049169, Lr: 0.000240
2020-02-27 04:38:19,504 Epoch 4190: total training loss 0.00049
2020-02-27 04:38:19,505 EPOCH 4191
2020-02-27 04:38:29,525 Epoch 4191: total training loss 0.00049
2020-02-27 04:38:29,527 EPOCH 4192
2020-02-27 04:38:34,532 Epoch 4192 Step:   465250 Batch Loss:     0.000004 Tokens per Sec:  9775773, Lr: 0.000240
2020-02-27 04:38:40,162 Epoch 4192: total training loss 0.00049
2020-02-27 04:38:40,163 EPOCH 4193
2020-02-27 04:38:50,746 Epoch 4193: total training loss 0.00049
2020-02-27 04:38:50,748 EPOCH 4194
2020-02-27 04:38:58,204 Epoch 4194 Step:   465500 Batch Loss:     0.000003 Tokens per Sec:  9396754, Lr: 0.000240
2020-02-27 04:39:01,566 Epoch 4194: total training loss 0.00049
2020-02-27 04:39:01,566 EPOCH 4195
2020-02-27 04:39:11,962 Epoch 4195: total training loss 0.00049
2020-02-27 04:39:11,964 EPOCH 4196
2020-02-27 04:39:21,456 Epoch 4196 Step:   465750 Batch Loss:     0.000005 Tokens per Sec: 10245431, Lr: 0.000240
2020-02-27 04:39:21,995 Epoch 4196: total training loss 0.00048
2020-02-27 04:39:21,996 EPOCH 4197
2020-02-27 04:39:31,963 Epoch 4197: total training loss 0.00049
2020-02-27 04:39:31,964 EPOCH 4198
2020-02-27 04:39:41,960 Epoch 4198: total training loss 0.00048
2020-02-27 04:39:41,960 EPOCH 4199
2020-02-27 04:39:43,917 Epoch 4199 Step:   466000 Batch Loss:     0.000004 Tokens per Sec: 10021677, Lr: 0.000240
2020-02-27 04:39:51,919 Epoch 4199: total training loss 0.00048
2020-02-27 04:39:51,920 EPOCH 4200
2020-02-27 04:40:01,730 Epoch 4200: total training loss 0.00049
2020-02-27 04:40:01,731 EPOCH 4201
2020-02-27 04:40:06,243 Epoch 4201 Step:   466250 Batch Loss:     0.000006 Tokens per Sec: 10510009, Lr: 0.000240
2020-02-27 04:40:11,567 Epoch 4201: total training loss 0.00049
2020-02-27 04:40:11,568 EPOCH 4202
2020-02-27 04:40:21,747 Epoch 4202: total training loss 0.00049
2020-02-27 04:40:21,748 EPOCH 4203
2020-02-27 04:40:28,825 Epoch 4203 Step:   466500 Batch Loss:     0.000005 Tokens per Sec: 10075243, Lr: 0.000240
2020-02-27 04:40:31,971 Epoch 4203: total training loss 0.00049
2020-02-27 04:40:31,972 EPOCH 4204
2020-02-27 04:40:42,269 Epoch 4204: total training loss 0.00049
2020-02-27 04:40:42,270 EPOCH 4205
2020-02-27 04:40:52,066 Epoch 4205 Step:   466750 Batch Loss:     0.000004 Tokens per Sec: 10030945, Lr: 0.000240
2020-02-27 04:40:52,457 Epoch 4205: total training loss 0.00049
2020-02-27 04:40:52,457 EPOCH 4206
2020-02-27 04:41:02,281 Epoch 4206: total training loss 0.00049
2020-02-27 04:41:02,283 EPOCH 4207
2020-02-27 04:41:12,240 Epoch 4207: total training loss 0.00049
2020-02-27 04:41:12,241 EPOCH 4208
2020-02-27 04:41:14,375 Epoch 4208 Step:   467000 Batch Loss:     0.000002 Tokens per Sec: 10351487, Lr: 0.000240
2020-02-27 04:41:22,445 Epoch 4208: total training loss 0.00049
2020-02-27 04:41:22,445 EPOCH 4209
2020-02-27 04:41:32,595 Epoch 4209: total training loss 0.00049
2020-02-27 04:41:32,596 EPOCH 4210
2020-02-27 04:41:37,321 Epoch 4210 Step:   467250 Batch Loss:     0.000005 Tokens per Sec: 10498452, Lr: 0.000240
2020-02-27 04:41:42,646 Epoch 4210: total training loss 0.00049
2020-02-27 04:41:42,646 EPOCH 4211
2020-02-27 04:41:52,528 Epoch 4211: total training loss 0.00049
2020-02-27 04:41:52,529 EPOCH 4212
2020-02-27 04:41:59,395 Epoch 4212 Step:   467500 Batch Loss:     0.000006 Tokens per Sec: 10197782, Lr: 0.000240
2020-02-27 04:42:02,457 Epoch 4212: total training loss 0.00049
2020-02-27 04:42:02,458 EPOCH 4213
2020-02-27 04:42:12,645 Epoch 4213: total training loss 0.00048
2020-02-27 04:42:12,646 EPOCH 4214
2020-02-27 04:42:22,751 Epoch 4214 Step:   467750 Batch Loss:     0.000005 Tokens per Sec:  9855983, Lr: 0.000240
2020-02-27 04:42:23,079 Epoch 4214: total training loss 0.00048
2020-02-27 04:42:23,079 EPOCH 4215
2020-02-27 04:42:33,453 Epoch 4215: total training loss 0.00048
2020-02-27 04:42:33,454 EPOCH 4216
2020-02-27 04:42:43,809 Epoch 4216: total training loss 0.00049
2020-02-27 04:42:43,810 EPOCH 4217
2020-02-27 04:42:46,295 Epoch 4217 Step:   468000 Batch Loss:     0.000005 Tokens per Sec: 10178969, Lr: 0.000240
2020-02-27 04:42:53,614 Epoch 4217: total training loss 0.00049
2020-02-27 04:42:53,615 EPOCH 4218
2020-02-27 04:43:03,496 Epoch 4218: total training loss 0.00049
2020-02-27 04:43:03,497 EPOCH 4219
2020-02-27 04:43:08,320 Epoch 4219 Step:   468250 Batch Loss:     0.000005 Tokens per Sec: 10462018, Lr: 0.000240
2020-02-27 04:43:13,540 Epoch 4219: total training loss 0.00049
2020-02-27 04:43:13,540 EPOCH 4220
2020-02-27 04:43:23,428 Epoch 4220: total training loss 0.00048
2020-02-27 04:43:23,429 EPOCH 4221
2020-02-27 04:43:30,527 Epoch 4221 Step:   468500 Batch Loss:     0.000005 Tokens per Sec: 10448965, Lr: 0.000240
2020-02-27 04:43:33,218 Epoch 4221: total training loss 0.00049
2020-02-27 04:43:33,218 EPOCH 4222
2020-02-27 04:43:43,075 Epoch 4222: total training loss 0.00048
2020-02-27 04:43:43,076 EPOCH 4223
2020-02-27 04:43:52,784 Epoch 4223 Step:   468750 Batch Loss:     0.000006 Tokens per Sec: 10275310, Lr: 0.000240
2020-02-27 04:43:53,059 Epoch 4223: total training loss 0.00049
2020-02-27 04:43:53,060 EPOCH 4224
2020-02-27 04:44:03,473 Epoch 4224: total training loss 0.00050
2020-02-27 04:44:03,475 EPOCH 4225
2020-02-27 04:44:14,153 Epoch 4225: total training loss 0.00049
2020-02-27 04:44:14,155 EPOCH 4226
2020-02-27 04:44:16,403 Epoch 4226 Step:   469000 Batch Loss:     0.000003 Tokens per Sec:  9422717, Lr: 0.000240
2020-02-27 04:44:24,860 Epoch 4226: total training loss 0.00049
2020-02-27 04:44:24,862 EPOCH 4227
2020-02-27 04:44:35,345 Epoch 4227: total training loss 0.00048
2020-02-27 04:44:35,346 EPOCH 4228
2020-02-27 04:44:40,204 Epoch 4228 Step:   469250 Batch Loss:     0.000004 Tokens per Sec:  9951066, Lr: 0.000240
2020-02-27 04:44:45,555 Epoch 4228: total training loss 0.00049
2020-02-27 04:44:45,556 EPOCH 4229
2020-02-27 04:44:55,347 Epoch 4229: total training loss 0.00048
2020-02-27 04:44:55,348 EPOCH 4230
2020-02-27 04:45:02,603 Epoch 4230 Step:   469500 Batch Loss:     0.000004 Tokens per Sec: 10339557, Lr: 0.000240
2020-02-27 04:45:05,274 Epoch 4230: total training loss 0.00049
2020-02-27 04:45:05,275 EPOCH 4231
2020-02-27 04:45:15,162 Epoch 4231: total training loss 0.00049
2020-02-27 04:45:15,163 EPOCH 4232
2020-02-27 04:45:24,963 Epoch 4232 Step:   469750 Batch Loss:     0.000007 Tokens per Sec: 10236748, Lr: 0.000240
2020-02-27 04:45:25,164 Epoch 4232: total training loss 0.00048
2020-02-27 04:45:25,164 EPOCH 4233
2020-02-27 04:45:35,092 Epoch 4233: total training loss 0.00048
2020-02-27 04:45:35,093 EPOCH 4234
2020-02-27 04:45:44,920 Epoch 4234: total training loss 0.00048
2020-02-27 04:45:44,921 EPOCH 4235
2020-02-27 04:45:47,203 Epoch 4235 Step:   470000 Batch Loss:     0.000005 Tokens per Sec:  9677171, Lr: 0.000240
2020-02-27 04:45:47,203 Model noise rate: 5
2020-02-27 04:46:52,292 Validation result at epoch 4235, step   470000: Val DTW Score:  10.79, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0437, GT DTW Score:      nan, duration: 65.0889s
2020-02-27 04:47:00,240 Epoch 4235: total training loss 0.00048
2020-02-27 04:47:00,241 EPOCH 4236
2020-02-27 04:47:10,230 Epoch 4236: total training loss 0.00049
2020-02-27 04:47:10,231 EPOCH 4237
2020-02-27 04:47:15,014 Epoch 4237 Step:   470250 Batch Loss:     0.000003 Tokens per Sec: 10281751, Lr: 0.000240
2020-02-27 04:47:20,136 Epoch 4237: total training loss 0.00049
2020-02-27 04:47:20,136 EPOCH 4238
2020-02-27 04:47:30,097 Epoch 4238: total training loss 0.00048
2020-02-27 04:47:30,099 EPOCH 4239
2020-02-27 04:47:37,579 Epoch 4239 Step:   470500 Batch Loss:     0.000003 Tokens per Sec: 10032793, Lr: 0.000240
2020-02-27 04:47:40,439 Epoch 4239: total training loss 0.00049
2020-02-27 04:47:40,439 EPOCH 4240
2020-02-27 04:47:50,621 Epoch 4240: total training loss 0.00049
2020-02-27 04:47:50,621 EPOCH 4241
2020-02-27 04:48:00,769 Epoch 4241 Step:   470750 Batch Loss:     0.000003 Tokens per Sec: 10034278, Lr: 0.000240
2020-02-27 04:48:00,878 Epoch 4241: total training loss 0.00048
2020-02-27 04:48:00,879 EPOCH 4242
2020-02-27 04:48:11,189 Epoch 4242: total training loss 0.00049
2020-02-27 04:48:11,190 EPOCH 4243
2020-02-27 04:48:21,120 Epoch 4243: total training loss 0.00049
2020-02-27 04:48:21,120 EPOCH 4244
2020-02-27 04:48:23,510 Epoch 4244 Step:   471000 Batch Loss:     0.000005 Tokens per Sec: 10435004, Lr: 0.000240
2020-02-27 04:48:30,951 Epoch 4244: total training loss 0.00048
2020-02-27 04:48:30,952 EPOCH 4245
2020-02-27 04:48:40,961 Epoch 4245: total training loss 0.00048
2020-02-27 04:48:40,961 EPOCH 4246
2020-02-27 04:48:46,047 Epoch 4246 Step:   471250 Batch Loss:     0.000005 Tokens per Sec: 10333018, Lr: 0.000240
2020-02-27 04:48:50,931 Epoch 4246: total training loss 0.00049
2020-02-27 04:48:50,931 EPOCH 4247
2020-02-27 04:49:00,837 Epoch 4247: total training loss 0.00049
2020-02-27 04:49:00,838 EPOCH 4248
2020-02-27 04:49:08,022 Epoch 4248 Step:   471500 Batch Loss:     0.000007 Tokens per Sec: 10494597, Lr: 0.000240
2020-02-27 04:49:10,651 Epoch 4248: total training loss 0.00049
2020-02-27 04:49:10,651 EPOCH 4249
2020-02-27 04:49:20,897 Epoch 4249: total training loss 0.00050
2020-02-27 04:49:20,898 EPOCH 4250
2020-02-27 04:49:31,330 Epoch 4250 Step:   471750 Batch Loss:     0.000006 Tokens per Sec:  9775861, Lr: 0.000240
2020-02-27 04:49:31,332 Epoch 4250: total training loss 0.00049
2020-02-27 04:49:31,332 EPOCH 4251
2020-02-27 04:49:41,968 Epoch 4251: total training loss 0.00048
2020-02-27 04:49:41,968 EPOCH 4252
2020-02-27 04:49:52,419 Epoch 4252: total training loss 0.00048
2020-02-27 04:49:52,420 EPOCH 4253
2020-02-27 04:49:55,114 Epoch 4253 Step:   472000 Batch Loss:     0.000004 Tokens per Sec: 10120645, Lr: 0.000240
2020-02-27 04:50:02,561 Epoch 4253: total training loss 0.00049
2020-02-27 04:50:02,562 EPOCH 4254
2020-02-27 04:50:12,448 Epoch 4254: total training loss 0.00049
2020-02-27 04:50:12,449 EPOCH 4255
2020-02-27 04:50:17,448 Epoch 4255 Step:   472250 Batch Loss:     0.000002 Tokens per Sec: 10264852, Lr: 0.000240
2020-02-27 04:50:22,464 Epoch 4255: total training loss 0.00048
2020-02-27 04:50:22,464 EPOCH 4256
2020-02-27 04:50:32,393 Epoch 4256: total training loss 0.00049
2020-02-27 04:50:32,394 EPOCH 4257
2020-02-27 04:50:39,973 Epoch 4257 Step:   472500 Batch Loss:     0.000004 Tokens per Sec: 10287487, Lr: 0.000240
2020-02-27 04:50:42,416 Epoch 4257: total training loss 0.00049
2020-02-27 04:50:42,416 EPOCH 4258
2020-02-27 04:50:52,251 Epoch 4258: total training loss 0.00049
2020-02-27 04:50:52,252 EPOCH 4259
2020-02-27 04:51:02,106 Epoch 4259: total training loss 0.00048
2020-02-27 04:51:02,107 EPOCH 4260
2020-02-27 04:51:02,241 Epoch 4260 Step:   472750 Batch Loss:     0.000006 Tokens per Sec:  7956333, Lr: 0.000240
2020-02-27 04:51:12,165 Epoch 4260: total training loss 0.00048
2020-02-27 04:51:12,166 EPOCH 4261
2020-02-27 04:51:22,643 Epoch 4261: total training loss 0.00048
2020-02-27 04:51:22,644 EPOCH 4262
2020-02-27 04:51:25,390 Epoch 4262 Step:   473000 Batch Loss:     0.000004 Tokens per Sec:  9860066, Lr: 0.000240
2020-02-27 04:51:32,827 Epoch 4262: total training loss 0.00048
2020-02-27 04:51:32,828 EPOCH 4263
2020-02-27 04:51:43,192 Epoch 4263: total training loss 0.00048
2020-02-27 04:51:43,193 EPOCH 4264
2020-02-27 04:51:48,537 Epoch 4264 Step:   473250 Batch Loss:     0.000006 Tokens per Sec:  9839867, Lr: 0.000240
2020-02-27 04:51:53,670 Epoch 4264: total training loss 0.00048
2020-02-27 04:51:53,670 EPOCH 4265
2020-02-27 04:52:03,781 Epoch 4265: total training loss 0.00049
2020-02-27 04:52:03,781 EPOCH 4266
2020-02-27 04:52:11,310 Epoch 4266 Step:   473500 Batch Loss:     0.000006 Tokens per Sec: 10509772, Lr: 0.000240
2020-02-27 04:52:13,670 Epoch 4266: total training loss 0.00049
2020-02-27 04:52:13,671 EPOCH 4267
2020-02-27 04:52:23,732 Epoch 4267: total training loss 0.00049
2020-02-27 04:52:23,733 EPOCH 4268
2020-02-27 04:52:33,505 Epoch 4268: total training loss 0.00049
2020-02-27 04:52:33,506 EPOCH 4269
2020-02-27 04:52:33,720 Epoch 4269 Step:   473750 Batch Loss:     0.000005 Tokens per Sec:  9334260, Lr: 0.000240
2020-02-27 04:52:43,562 Epoch 4269: total training loss 0.00048
2020-02-27 04:52:43,562 EPOCH 4270
2020-02-27 04:52:53,712 Epoch 4270: total training loss 0.00048
2020-02-27 04:52:53,712 EPOCH 4271
2020-02-27 04:52:56,538 Epoch 4271 Step:   474000 Batch Loss:     0.000005 Tokens per Sec:  9671057, Lr: 0.000240
2020-02-27 04:53:03,960 Epoch 4271: total training loss 0.00048
2020-02-27 04:53:03,961 EPOCH 4272
2020-02-27 04:53:13,938 Epoch 4272: total training loss 0.00048
2020-02-27 04:53:13,939 EPOCH 4273
2020-02-27 04:53:19,267 Epoch 4273 Step:   474250 Batch Loss:     0.000004 Tokens per Sec: 10384094, Lr: 0.000240
2020-02-27 04:53:23,900 Epoch 4273: total training loss 0.00048
2020-02-27 04:53:23,901 EPOCH 4274
2020-02-27 04:53:33,817 Epoch 4274: total training loss 0.00048
2020-02-27 04:53:33,818 EPOCH 4275
2020-02-27 04:53:41,568 Epoch 4275 Step:   474500 Batch Loss:     0.000004 Tokens per Sec: 10342531, Lr: 0.000240
2020-02-27 04:53:43,751 Epoch 4275: total training loss 0.00049
2020-02-27 04:53:43,751 EPOCH 4276
2020-02-27 04:53:53,609 Epoch 4276: total training loss 0.00048
2020-02-27 04:53:53,609 EPOCH 4277
2020-02-27 04:54:03,286 Epoch 4277: total training loss 0.00048
2020-02-27 04:54:03,287 EPOCH 4278
2020-02-27 04:54:03,577 Epoch 4278 Step:   474750 Batch Loss:     0.000004 Tokens per Sec: 10078789, Lr: 0.000240
2020-02-27 04:54:13,212 Epoch 4278: total training loss 0.00048
2020-02-27 04:54:13,213 EPOCH 4279
2020-02-27 04:54:23,208 Epoch 4279: total training loss 0.00049
2020-02-27 04:54:23,209 EPOCH 4280
2020-02-27 04:54:26,144 Epoch 4280 Step:   475000 Batch Loss:     0.000004 Tokens per Sec: 10334387, Lr: 0.000240
2020-02-27 04:54:33,310 Epoch 4280: total training loss 0.00051
2020-02-27 04:54:33,311 EPOCH 4281
2020-02-27 04:54:43,990 Epoch 4281: total training loss 0.00048
2020-02-27 04:54:43,991 EPOCH 4282
2020-02-27 04:54:49,639 Epoch 4282 Step:   475250 Batch Loss:     0.000004 Tokens per Sec:  9396689, Lr: 0.000240
2020-02-27 04:54:54,844 Epoch 4282: total training loss 0.00048
2020-02-27 04:54:54,845 EPOCH 4283
2020-02-27 04:55:05,194 Epoch 4283: total training loss 0.00049
2020-02-27 04:55:05,195 EPOCH 4284
2020-02-27 04:55:13,015 Epoch 4284 Step:   475500 Batch Loss:     0.000004 Tokens per Sec: 10085600, Lr: 0.000240
2020-02-27 04:55:15,293 Epoch 4284: total training loss 0.00049
2020-02-27 04:55:15,293 EPOCH 4285
2020-02-27 04:55:25,421 Epoch 4285: total training loss 0.00049
2020-02-27 04:55:25,421 EPOCH 4286
2020-02-27 04:55:35,440 Epoch 4286: total training loss 0.00050
2020-02-27 04:55:35,441 EPOCH 4287
2020-02-27 04:55:35,764 Epoch 4287 Step:   475750 Batch Loss:     0.000006 Tokens per Sec:  9610362, Lr: 0.000240
2020-02-27 04:55:45,327 Epoch 4287: total training loss 0.00048
2020-02-27 04:55:45,327 EPOCH 4288
2020-02-27 04:55:55,317 Epoch 4288: total training loss 0.00048
2020-02-27 04:55:55,317 EPOCH 4289
2020-02-27 04:55:58,262 Epoch 4289 Step:   476000 Batch Loss:     0.000005 Tokens per Sec: 10546596, Lr: 0.000240
2020-02-27 04:56:05,049 Epoch 4289: total training loss 0.00048
2020-02-27 04:56:05,050 EPOCH 4290
2020-02-27 04:56:14,828 Epoch 4290: total training loss 0.00048
2020-02-27 04:56:14,828 EPOCH 4291
2020-02-27 04:56:20,215 Epoch 4291 Step:   476250 Batch Loss:     0.000006 Tokens per Sec:  9903679, Lr: 0.000240
2020-02-27 04:56:25,158 Epoch 4291: total training loss 0.00049
2020-02-27 04:56:25,159 EPOCH 4292
2020-02-27 04:56:35,341 Epoch 4292: total training loss 0.00048
2020-02-27 04:56:35,341 EPOCH 4293
2020-02-27 04:56:43,232 Epoch 4293 Step:   476500 Batch Loss:     0.000004 Tokens per Sec: 10314780, Lr: 0.000240
2020-02-27 04:56:45,326 Epoch 4293: total training loss 0.00048
2020-02-27 04:56:45,326 EPOCH 4294
2020-02-27 04:56:55,661 Epoch 4294: total training loss 0.00048
2020-02-27 04:56:55,662 EPOCH 4295
2020-02-27 04:57:05,735 Epoch 4295: total training loss 0.00048
2020-02-27 04:57:05,736 EPOCH 4296
2020-02-27 04:57:06,251 Epoch 4296 Step:   476750 Batch Loss:     0.000005 Tokens per Sec: 10579408, Lr: 0.000240
2020-02-27 04:57:15,837 Epoch 4296: total training loss 0.00048
2020-02-27 04:57:15,838 EPOCH 4297
2020-02-27 04:57:25,746 Epoch 4297: total training loss 0.00048
2020-02-27 04:57:25,747 EPOCH 4298
2020-02-27 04:57:28,691 Epoch 4298 Step:   477000 Batch Loss:     0.000003 Tokens per Sec: 10125000, Lr: 0.000240
2020-02-27 04:57:35,667 Epoch 4298: total training loss 0.00049
2020-02-27 04:57:35,668 EPOCH 4299
2020-02-27 04:57:45,545 Epoch 4299: total training loss 0.00048
2020-02-27 04:57:45,546 EPOCH 4300
2020-02-27 04:57:50,885 Epoch 4300 Step:   477250 Batch Loss:     0.000004 Tokens per Sec: 10501984, Lr: 0.000240
2020-02-27 04:57:55,360 Epoch 4300: total training loss 0.00049
2020-02-27 04:57:55,360 EPOCH 4301
2020-02-27 04:58:05,033 Epoch 4301: total training loss 0.00048
2020-02-27 04:58:05,034 EPOCH 4302
2020-02-27 04:58:12,800 Epoch 4302 Step:   477500 Batch Loss:     0.000004 Tokens per Sec: 10517166, Lr: 0.000240
2020-02-27 04:58:14,807 Epoch 4302: total training loss 0.00048
2020-02-27 04:58:14,807 EPOCH 4303
2020-02-27 04:58:24,559 Epoch 4303: total training loss 0.00049
2020-02-27 04:58:24,560 EPOCH 4304
2020-02-27 04:58:34,333 Epoch 4304: total training loss 0.00048
2020-02-27 04:58:34,334 EPOCH 4305
2020-02-27 04:58:34,912 Epoch 4305 Step:   477750 Batch Loss:     0.000005 Tokens per Sec: 10402589, Lr: 0.000240
2020-02-27 04:58:44,148 Epoch 4305: total training loss 0.00048
2020-02-27 04:58:44,149 EPOCH 4306
2020-02-27 04:58:54,132 Epoch 4306: total training loss 0.00049
2020-02-27 04:58:54,132 EPOCH 4307
2020-02-27 04:58:57,403 Epoch 4307 Step:   478000 Batch Loss:     0.000004 Tokens per Sec:  9984112, Lr: 0.000240
2020-02-27 04:59:04,463 Epoch 4307: total training loss 0.00048
2020-02-27 04:59:04,464 EPOCH 4308
2020-02-27 04:59:14,659 Epoch 4308: total training loss 0.00048
2020-02-27 04:59:14,660 EPOCH 4309
2020-02-27 04:59:20,473 Epoch 4309 Step:   478250 Batch Loss:     0.000004 Tokens per Sec:  9895093, Lr: 0.000240
2020-02-27 04:59:25,080 Epoch 4309: total training loss 0.00049
2020-02-27 04:59:25,081 EPOCH 4310
2020-02-27 04:59:35,211 Epoch 4310: total training loss 0.00049
2020-02-27 04:59:35,213 EPOCH 4311
2020-02-27 04:59:43,280 Epoch 4311 Step:   478500 Batch Loss:     0.000005 Tokens per Sec: 10258940, Lr: 0.000240
2020-02-27 04:59:45,196 Epoch 4311: total training loss 0.00051
2020-02-27 04:59:45,196 EPOCH 4312
2020-02-27 04:59:55,147 Epoch 4312: total training loss 0.00049
2020-02-27 04:59:55,147 EPOCH 4313
2020-02-27 05:00:05,249 Epoch 4313: total training loss 0.00048
2020-02-27 05:00:05,250 EPOCH 4314
2020-02-27 05:00:05,855 Epoch 4314 Step:   478750 Batch Loss:     0.000005 Tokens per Sec:  9332019, Lr: 0.000240
2020-02-27 05:00:15,103 Epoch 4314: total training loss 0.00048
2020-02-27 05:00:15,104 EPOCH 4315
2020-02-27 05:00:25,239 Epoch 4315: total training loss 0.00048
2020-02-27 05:00:25,240 EPOCH 4316
2020-02-27 05:00:28,422 Epoch 4316 Step:   479000 Batch Loss:     0.000004 Tokens per Sec: 10030796, Lr: 0.000240
2020-02-27 05:00:35,470 Epoch 4316: total training loss 0.00048
2020-02-27 05:00:35,471 EPOCH 4317
2020-02-27 05:00:45,723 Epoch 4317: total training loss 0.00048
2020-02-27 05:00:45,723 EPOCH 4318
2020-02-27 05:00:51,287 Epoch 4318 Step:   479250 Batch Loss:     0.000005 Tokens per Sec: 10069346, Lr: 0.000240
2020-02-27 05:00:55,890 Epoch 4318: total training loss 0.00049
2020-02-27 05:00:55,891 EPOCH 4319
2020-02-27 05:01:05,826 Epoch 4319: total training loss 0.00049
2020-02-27 05:01:05,826 EPOCH 4320
2020-02-27 05:01:13,985 Epoch 4320 Step:   479500 Batch Loss:     0.000004 Tokens per Sec: 10392230, Lr: 0.000240
2020-02-27 05:01:15,758 Epoch 4320: total training loss 0.00049
2020-02-27 05:01:15,759 EPOCH 4321
2020-02-27 05:01:25,534 Epoch 4321: total training loss 0.00048
2020-02-27 05:01:25,534 EPOCH 4322
2020-02-27 05:01:35,453 Epoch 4322: total training loss 0.00048
2020-02-27 05:01:35,453 EPOCH 4323
2020-02-27 05:01:36,247 Epoch 4323 Step:   479750 Batch Loss:     0.000004 Tokens per Sec: 10485965, Lr: 0.000240
2020-02-27 05:01:45,164 Epoch 4323: total training loss 0.00048
2020-02-27 05:01:45,164 EPOCH 4324
2020-02-27 05:01:55,093 Epoch 4324: total training loss 0.00048
2020-02-27 05:01:55,093 EPOCH 4325
2020-02-27 05:01:58,358 Epoch 4325 Step:   480000 Batch Loss:     0.000002 Tokens per Sec: 10341039, Lr: 0.000240
2020-02-27 05:01:58,359 Model noise rate: 5
2020-02-27 05:02:59,715 Validation result at epoch 4325, step   480000: Val DTW Score:  10.86, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0439, GT DTW Score:      nan, duration: 61.3555s
2020-02-27 05:03:06,422 Epoch 4325: total training loss 0.00048
2020-02-27 05:03:06,423 EPOCH 4326
2020-02-27 05:03:16,449 Epoch 4326: total training loss 0.00048
2020-02-27 05:03:16,450 EPOCH 4327
2020-02-27 05:03:22,204 Epoch 4327 Step:   480250 Batch Loss:     0.000005 Tokens per Sec: 10482824, Lr: 0.000240
2020-02-27 05:03:26,256 Epoch 4327: total training loss 0.00048
2020-02-27 05:03:26,257 EPOCH 4328
2020-02-27 05:03:36,182 Epoch 4328: total training loss 0.00048
2020-02-27 05:03:36,183 EPOCH 4329
2020-02-27 05:03:44,221 Epoch 4329 Step:   480500 Batch Loss:     0.000004 Tokens per Sec: 10563167, Lr: 0.000240
2020-02-27 05:03:45,896 Epoch 4329: total training loss 0.00048
2020-02-27 05:03:45,896 EPOCH 4330
2020-02-27 05:03:55,817 Epoch 4330: total training loss 0.00048
2020-02-27 05:03:55,818 EPOCH 4331
2020-02-27 05:04:05,786 Epoch 4331: total training loss 0.00048
2020-02-27 05:04:05,787 EPOCH 4332
2020-02-27 05:04:06,599 Epoch 4332 Step:   480750 Batch Loss:     0.000003 Tokens per Sec:  9970250, Lr: 0.000240
2020-02-27 05:04:16,075 Epoch 4332: total training loss 0.00048
2020-02-27 05:04:16,076 EPOCH 4333
2020-02-27 05:04:26,216 Epoch 4333: total training loss 0.00048
2020-02-27 05:04:26,217 EPOCH 4334
2020-02-27 05:04:29,668 Epoch 4334 Step:   481000 Batch Loss:     0.000003 Tokens per Sec:  9933402, Lr: 0.000240
2020-02-27 05:04:36,402 Epoch 4334: total training loss 0.00049
2020-02-27 05:04:36,403 EPOCH 4335
2020-02-27 05:04:46,727 Epoch 4335: total training loss 0.00049
2020-02-27 05:04:46,728 EPOCH 4336
2020-02-27 05:04:52,715 Epoch 4336 Step:   481250 Batch Loss:     0.000005 Tokens per Sec: 10379004, Lr: 0.000240
2020-02-27 05:04:56,698 Epoch 4336: total training loss 0.00048
2020-02-27 05:04:56,699 EPOCH 4337
2020-02-27 05:05:06,658 Epoch 4337: total training loss 0.00048
2020-02-27 05:05:06,659 EPOCH 4338
2020-02-27 05:05:14,996 Epoch 4338 Step:   481500 Batch Loss:     0.000004 Tokens per Sec: 10349281, Lr: 0.000240
2020-02-27 05:05:16,613 Epoch 4338: total training loss 0.00048
2020-02-27 05:05:16,613 EPOCH 4339
2020-02-27 05:05:26,692 Epoch 4339: total training loss 0.00048
2020-02-27 05:05:26,692 EPOCH 4340
2020-02-27 05:05:36,643 Epoch 4340: total training loss 0.00048
2020-02-27 05:05:36,644 EPOCH 4341
2020-02-27 05:05:37,568 Epoch 4341 Step:   481750 Batch Loss:     0.000006 Tokens per Sec:  9576737, Lr: 0.000240
2020-02-27 05:05:46,834 Epoch 4341: total training loss 0.00048
2020-02-27 05:05:46,834 EPOCH 4342
2020-02-27 05:05:57,123 Epoch 4342: total training loss 0.00048
2020-02-27 05:05:57,124 EPOCH 4343
2020-02-27 05:06:00,568 Epoch 4343 Step:   482000 Batch Loss:     0.000003 Tokens per Sec: 10032864, Lr: 0.000240
2020-02-27 05:06:07,296 Epoch 4343: total training loss 0.00048
2020-02-27 05:06:07,296 EPOCH 4344
2020-02-27 05:06:17,514 Epoch 4344: total training loss 0.00048
2020-02-27 05:06:17,514 EPOCH 4345
2020-02-27 05:06:23,530 Epoch 4345 Step:   482250 Batch Loss:     0.000005 Tokens per Sec: 10460572, Lr: 0.000240
2020-02-27 05:06:27,408 Epoch 4345: total training loss 0.00048
2020-02-27 05:06:27,409 EPOCH 4346
2020-02-27 05:06:37,336 Epoch 4346: total training loss 0.00048
2020-02-27 05:06:37,337 EPOCH 4347
2020-02-27 05:06:45,775 Epoch 4347 Step:   482500 Batch Loss:     0.000003 Tokens per Sec: 10274178, Lr: 0.000240
2020-02-27 05:06:47,308 Epoch 4347: total training loss 0.00048
2020-02-27 05:06:47,308 EPOCH 4348
2020-02-27 05:06:57,077 Epoch 4348: total training loss 0.00048
2020-02-27 05:06:57,077 EPOCH 4349
2020-02-27 05:07:06,882 Epoch 4349: total training loss 0.00049
2020-02-27 05:07:06,883 EPOCH 4350
2020-02-27 05:07:07,938 Epoch 4350 Step:   482750 Batch Loss:     0.000006 Tokens per Sec: 10193449, Lr: 0.000240
2020-02-27 05:07:16,849 Epoch 4350: total training loss 0.00050
2020-02-27 05:07:16,850 EPOCH 4351
2020-02-27 05:07:27,034 Epoch 4351: total training loss 0.00050
2020-02-27 05:07:27,042 EPOCH 4352
2020-02-27 05:07:30,679 Epoch 4352 Step:   483000 Batch Loss:     0.000004 Tokens per Sec:  9887195, Lr: 0.000240
2020-02-27 05:07:37,310 Epoch 4352: total training loss 0.00050
2020-02-27 05:07:37,311 EPOCH 4353
2020-02-27 05:07:47,369 Epoch 4353: total training loss 0.00050
2020-02-27 05:07:47,369 EPOCH 4354
2020-02-27 05:07:53,317 Epoch 4354 Step:   483250 Batch Loss:     0.000004 Tokens per Sec: 10326760, Lr: 0.000240
2020-02-27 05:07:57,357 Epoch 4354: total training loss 0.00049
2020-02-27 05:07:57,358 EPOCH 4355
2020-02-27 05:08:07,411 Epoch 4355: total training loss 0.00048
2020-02-27 05:08:07,412 EPOCH 4356
2020-02-27 05:08:15,772 Epoch 4356 Step:   483500 Batch Loss:     0.000003 Tokens per Sec: 10445846, Lr: 0.000240
2020-02-27 05:08:17,165 Epoch 4356: total training loss 0.00049
2020-02-27 05:08:17,166 EPOCH 4357
2020-02-27 05:08:27,161 Epoch 4357: total training loss 0.00048
2020-02-27 05:08:27,162 EPOCH 4358
2020-02-27 05:08:37,215 Epoch 4358: total training loss 0.00048
2020-02-27 05:08:37,215 EPOCH 4359
2020-02-27 05:08:38,479 Epoch 4359 Step:   483750 Batch Loss:     0.000005 Tokens per Sec: 10199633, Lr: 0.000240
2020-02-27 05:08:47,159 Epoch 4359: total training loss 0.00048
2020-02-27 05:08:47,160 EPOCH 4360
2020-02-27 05:08:56,977 Epoch 4360: total training loss 0.00048
2020-02-27 05:08:56,978 EPOCH 4361
2020-02-27 05:09:00,637 Epoch 4361 Step:   484000 Batch Loss:     0.000003 Tokens per Sec: 10340941, Lr: 0.000240
2020-02-27 05:09:06,878 Epoch 4361: total training loss 0.00048
2020-02-27 05:09:06,879 EPOCH 4362
2020-02-27 05:09:16,922 Epoch 4362: total training loss 0.00048
2020-02-27 05:09:16,922 EPOCH 4363
2020-02-27 05:09:23,029 Epoch 4363 Step:   484250 Batch Loss:     0.000004 Tokens per Sec:  9804782, Lr: 0.000240
2020-02-27 05:09:27,226 Epoch 4363: total training loss 0.00048
2020-02-27 05:09:27,227 EPOCH 4364
2020-02-27 05:09:37,226 Epoch 4364: total training loss 0.00048
2020-02-27 05:09:37,227 EPOCH 4365
2020-02-27 05:09:46,268 Epoch 4365 Step:   484500 Batch Loss:     0.000004 Tokens per Sec: 10071928, Lr: 0.000240
2020-02-27 05:09:47,601 Epoch 4365: total training loss 0.00048
2020-02-27 05:09:47,602 EPOCH 4366
2020-02-27 05:09:57,683 Epoch 4366: total training loss 0.00048
2020-02-27 05:09:57,684 EPOCH 4367
2020-02-27 05:10:07,592 Epoch 4367: total training loss 0.00048
2020-02-27 05:10:07,593 EPOCH 4368
2020-02-27 05:10:08,836 Epoch 4368 Step:   484750 Batch Loss:     0.000005 Tokens per Sec: 10324344, Lr: 0.000240
2020-02-27 05:10:17,554 Epoch 4368: total training loss 0.00048
2020-02-27 05:10:17,554 EPOCH 4369
2020-02-27 05:10:27,559 Epoch 4369: total training loss 0.00048
2020-02-27 05:10:27,559 EPOCH 4370
2020-02-27 05:10:31,207 Epoch 4370 Step:   485000 Batch Loss:     0.000004 Tokens per Sec: 10371171, Lr: 0.000240
2020-02-27 05:10:37,412 Epoch 4370: total training loss 0.00048
2020-02-27 05:10:37,412 EPOCH 4371
2020-02-27 05:10:47,209 Epoch 4371: total training loss 0.00049
2020-02-27 05:10:47,210 EPOCH 4372
2020-02-27 05:10:53,455 Epoch 4372 Step:   485250 Batch Loss:     0.000004 Tokens per Sec: 10172043, Lr: 0.000240
2020-02-27 05:10:57,391 Epoch 4372: total training loss 0.00049
2020-02-27 05:10:57,391 EPOCH 4373
2020-02-27 05:11:08,049 Epoch 4373: total training loss 0.00048
2020-02-27 05:11:08,050 EPOCH 4374
2020-02-27 05:11:17,288 Epoch 4374 Step:   485500 Batch Loss:     0.000006 Tokens per Sec:  9848714, Lr: 0.000240
2020-02-27 05:11:18,515 Epoch 4374: total training loss 0.00048
2020-02-27 05:11:18,515 EPOCH 4375
2020-02-27 05:11:29,230 Epoch 4375: total training loss 0.00048
2020-02-27 05:11:29,230 EPOCH 4376
2020-02-27 05:11:39,363 Epoch 4376: total training loss 0.00048
2020-02-27 05:11:39,363 EPOCH 4377
2020-02-27 05:11:40,623 Epoch 4377 Step:   485750 Batch Loss:     0.000004 Tokens per Sec: 10034417, Lr: 0.000240
2020-02-27 05:11:49,171 Epoch 4377: total training loss 0.00049
2020-02-27 05:11:49,172 EPOCH 4378
2020-02-27 05:11:58,993 Epoch 4378: total training loss 0.00049
2020-02-27 05:11:58,994 EPOCH 4379
2020-02-27 05:12:02,824 Epoch 4379 Step:   486000 Batch Loss:     0.000006 Tokens per Sec: 10462427, Lr: 0.000240
2020-02-27 05:12:08,860 Epoch 4379: total training loss 0.00048
2020-02-27 05:12:08,860 EPOCH 4380
2020-02-27 05:12:18,799 Epoch 4380: total training loss 0.00048
2020-02-27 05:12:18,800 EPOCH 4381
2020-02-27 05:12:24,839 Epoch 4381 Step:   486250 Batch Loss:     0.000005 Tokens per Sec: 10732882, Lr: 0.000240
2020-02-27 05:12:28,370 Epoch 4381: total training loss 0.00048
2020-02-27 05:12:28,371 EPOCH 4382
2020-02-27 05:12:37,971 Epoch 4382: total training loss 0.00048
2020-02-27 05:12:37,971 EPOCH 4383
2020-02-27 05:12:46,620 Epoch 4383 Step:   486500 Batch Loss:     0.000003 Tokens per Sec: 10447608, Lr: 0.000240
2020-02-27 05:12:47,870 Epoch 4383: total training loss 0.00050
2020-02-27 05:12:47,870 EPOCH 4384
2020-02-27 05:12:57,977 Epoch 4384: total training loss 0.00049
2020-02-27 05:12:57,978 EPOCH 4385
2020-02-27 05:13:08,219 Epoch 4385: total training loss 0.00048
2020-02-27 05:13:08,220 EPOCH 4386
2020-02-27 05:13:09,426 Epoch 4386 Step:   486750 Batch Loss:     0.000003 Tokens per Sec: 10125095, Lr: 0.000240
2020-02-27 05:13:18,450 Epoch 4386: total training loss 0.00048
2020-02-27 05:13:18,451 EPOCH 4387
2020-02-27 05:13:28,937 Epoch 4387: total training loss 0.00048
2020-02-27 05:13:28,938 EPOCH 4388
2020-02-27 05:13:32,951 Epoch 4388 Step:   487000 Batch Loss:     0.000005 Tokens per Sec:  9924643, Lr: 0.000240
2020-02-27 05:13:39,047 Epoch 4388: total training loss 0.00048
2020-02-27 05:13:39,047 EPOCH 4389
2020-02-27 05:13:49,191 Epoch 4389: total training loss 0.00048
2020-02-27 05:13:49,192 EPOCH 4390
2020-02-27 05:13:55,394 Epoch 4390 Step:   487250 Batch Loss:     0.000005 Tokens per Sec: 10102291, Lr: 0.000240
2020-02-27 05:13:59,251 Epoch 4390: total training loss 0.00049
2020-02-27 05:13:59,252 EPOCH 4391
2020-02-27 05:14:08,995 Epoch 4391: total training loss 0.00048
2020-02-27 05:14:08,996 EPOCH 4392
2020-02-27 05:14:17,937 Epoch 4392 Step:   487500 Batch Loss:     0.000005 Tokens per Sec: 10183777, Lr: 0.000240
2020-02-27 05:14:19,034 Epoch 4392: total training loss 0.00048
2020-02-27 05:14:19,035 EPOCH 4393
2020-02-27 05:14:29,111 Epoch 4393: total training loss 0.00048
2020-02-27 05:14:29,112 EPOCH 4394
2020-02-27 05:14:39,202 Epoch 4394: total training loss 0.00048
2020-02-27 05:14:39,202 EPOCH 4395
2020-02-27 05:14:40,624 Epoch 4395 Step:   487750 Batch Loss:     0.000003 Tokens per Sec: 10163869, Lr: 0.000240
2020-02-27 05:14:49,389 Epoch 4395: total training loss 0.00048
2020-02-27 05:14:49,390 EPOCH 4396
2020-02-27 05:14:59,630 Epoch 4396: total training loss 0.00049
2020-02-27 05:14:59,631 EPOCH 4397
2020-02-27 05:15:03,744 Epoch 4397 Step:   488000 Batch Loss:     0.000005 Tokens per Sec: 10132745, Lr: 0.000240
2020-02-27 05:15:09,533 Epoch 4397: total training loss 0.00048
2020-02-27 05:15:09,533 EPOCH 4398
2020-02-27 05:15:19,427 Epoch 4398: total training loss 0.00048
2020-02-27 05:15:19,428 EPOCH 4399
2020-02-27 05:15:25,742 Epoch 4399 Step:   488250 Batch Loss:     0.000005 Tokens per Sec: 10258495, Lr: 0.000240
2020-02-27 05:15:29,406 Epoch 4399: total training loss 0.00048
2020-02-27 05:15:29,406 EPOCH 4400
2020-02-27 05:15:39,292 Epoch 4400: total training loss 0.00048
2020-02-27 05:15:39,293 EPOCH 4401
2020-02-27 05:15:48,175 Epoch 4401 Step:   488500 Batch Loss:     0.000005 Tokens per Sec: 10409968, Lr: 0.000240
2020-02-27 05:15:49,192 Epoch 4401: total training loss 0.00048
2020-02-27 05:15:49,192 EPOCH 4402
2020-02-27 05:15:58,984 Epoch 4402: total training loss 0.00048
2020-02-27 05:15:58,985 EPOCH 4403
2020-02-27 05:16:09,104 Epoch 4403: total training loss 0.00048
2020-02-27 05:16:09,105 EPOCH 4404
2020-02-27 05:16:10,709 Epoch 4404 Step:   488750 Batch Loss:     0.000003 Tokens per Sec:  9812624, Lr: 0.000240
2020-02-27 05:16:19,761 Epoch 4404: total training loss 0.00049
2020-02-27 05:16:19,762 EPOCH 4405
2020-02-27 05:16:30,298 Epoch 4405: total training loss 0.00048
2020-02-27 05:16:30,298 EPOCH 4406
2020-02-27 05:16:34,668 Epoch 4406 Step:   489000 Batch Loss:     0.000004 Tokens per Sec:  9669835, Lr: 0.000240
2020-02-27 05:16:40,792 Epoch 4406: total training loss 0.00048
2020-02-27 05:16:40,793 EPOCH 4407
2020-02-27 05:16:51,404 Epoch 4407: total training loss 0.00048
2020-02-27 05:16:51,405 EPOCH 4408
2020-02-27 05:16:57,887 Epoch 4408 Step:   489250 Batch Loss:     0.000003 Tokens per Sec: 10463680, Lr: 0.000240
2020-02-27 05:17:01,171 Epoch 4408: total training loss 0.00048
2020-02-27 05:17:01,171 EPOCH 4409
2020-02-27 05:17:11,059 Epoch 4409: total training loss 0.00048
2020-02-27 05:17:11,059 EPOCH 4410
2020-02-27 05:17:19,942 Epoch 4410 Step:   489500 Batch Loss:     0.000002 Tokens per Sec: 10382050, Lr: 0.000240
2020-02-27 05:17:20,869 Epoch 4410: total training loss 0.00049
2020-02-27 05:17:20,870 EPOCH 4411
2020-02-27 05:17:30,667 Epoch 4411: total training loss 0.00049
2020-02-27 05:17:30,667 EPOCH 4412
2020-02-27 05:17:40,403 Epoch 4412: total training loss 0.00051
2020-02-27 05:17:40,403 EPOCH 4413
2020-02-27 05:17:42,025 Epoch 4413 Step:   489750 Batch Loss:     0.000006 Tokens per Sec: 10343506, Lr: 0.000240
2020-02-27 05:17:50,059 Epoch 4413: total training loss 0.00050
2020-02-27 05:17:50,060 EPOCH 4414
2020-02-27 05:18:00,194 Epoch 4414: total training loss 0.00048
2020-02-27 05:18:00,195 EPOCH 4415
2020-02-27 05:18:04,324 Epoch 4415 Step:   490000 Batch Loss:     0.000002 Tokens per Sec:  9681577, Lr: 0.000240
2020-02-27 05:18:04,325 Model noise rate: 5
2020-02-27 05:19:07,088 Validation result at epoch 4415, step   490000: Val DTW Score:  10.80, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0437, GT DTW Score:      nan, duration: 62.7627s
2020-02-27 05:19:13,241 Epoch 4415: total training loss 0.00048
2020-02-27 05:19:13,241 EPOCH 4416
2020-02-27 05:19:23,285 Epoch 4416: total training loss 0.00048
2020-02-27 05:19:23,286 EPOCH 4417
2020-02-27 05:19:29,836 Epoch 4417 Step:   490250 Batch Loss:     0.000004 Tokens per Sec: 10734888, Lr: 0.000240
2020-02-27 05:19:32,875 Epoch 4417: total training loss 0.00048
2020-02-27 05:19:32,876 EPOCH 4418
2020-02-27 05:19:42,619 Epoch 4418: total training loss 0.00048
2020-02-27 05:19:42,620 EPOCH 4419
2020-02-27 05:19:51,546 Epoch 4419 Step:   490500 Batch Loss:     0.000003 Tokens per Sec: 10548460, Lr: 0.000240
2020-02-27 05:19:52,367 Epoch 4419: total training loss 0.00048
2020-02-27 05:19:52,367 EPOCH 4420
2020-02-27 05:20:02,269 Epoch 4420: total training loss 0.00048
2020-02-27 05:20:02,270 EPOCH 4421
2020-02-27 05:20:12,127 Epoch 4421: total training loss 0.00049
2020-02-27 05:20:12,127 EPOCH 4422
2020-02-27 05:20:13,848 Epoch 4422 Step:   490750 Batch Loss:     0.000005 Tokens per Sec: 10335787, Lr: 0.000240
2020-02-27 05:20:22,038 Epoch 4422: total training loss 0.00048
2020-02-27 05:20:22,039 EPOCH 4423
2020-02-27 05:20:32,316 Epoch 4423: total training loss 0.00049
2020-02-27 05:20:32,317 EPOCH 4424
2020-02-27 05:20:36,567 Epoch 4424 Step:   491000 Batch Loss:     0.000004 Tokens per Sec: 10006211, Lr: 0.000240
2020-02-27 05:20:42,476 Epoch 4424: total training loss 0.00048
2020-02-27 05:20:42,477 EPOCH 4425
2020-02-27 05:20:52,900 Epoch 4425: total training loss 0.00048
2020-02-27 05:20:52,900 EPOCH 4426
2020-02-27 05:20:59,946 Epoch 4426 Step:   491250 Batch Loss:     0.000004 Tokens per Sec:  9661735, Lr: 0.000240
2020-02-27 05:21:03,360 Epoch 4426: total training loss 0.00049
2020-02-27 05:21:03,360 EPOCH 4427
2020-02-27 05:21:13,369 Epoch 4427: total training loss 0.00048
2020-02-27 05:21:13,369 EPOCH 4428
2020-02-27 05:21:22,624 Epoch 4428 Step:   491500 Batch Loss:     0.000004 Tokens per Sec: 10232440, Lr: 0.000240
2020-02-27 05:21:23,333 Epoch 4428: total training loss 0.00048
2020-02-27 05:21:23,334 EPOCH 4429
2020-02-27 05:21:33,409 Epoch 4429: total training loss 0.00048
2020-02-27 05:21:33,410 EPOCH 4430
2020-02-27 05:21:43,555 Epoch 4430: total training loss 0.00048
2020-02-27 05:21:43,556 EPOCH 4431
2020-02-27 05:21:45,348 Epoch 4431 Step:   491750 Batch Loss:     0.000005 Tokens per Sec: 10541967, Lr: 0.000240
2020-02-27 05:21:53,655 Epoch 4431: total training loss 0.00047
2020-02-27 05:21:53,656 EPOCH 4432
2020-02-27 05:22:03,813 Epoch 4432: total training loss 0.00048
2020-02-27 05:22:03,813 EPOCH 4433
2020-02-27 05:22:08,372 Epoch 4433 Step:   492000 Batch Loss:     0.000003 Tokens per Sec:  9808621, Lr: 0.000240
2020-02-27 05:22:14,438 Epoch 4433: total training loss 0.00048
2020-02-27 05:22:14,438 EPOCH 4434
2020-02-27 05:22:24,801 Epoch 4434: total training loss 0.00048
2020-02-27 05:22:24,802 EPOCH 4435
2020-02-27 05:22:32,049 Epoch 4435 Step:   492250 Batch Loss:     0.000004 Tokens per Sec:  9792021, Lr: 0.000240
2020-02-27 05:22:35,217 Epoch 4435: total training loss 0.00048
2020-02-27 05:22:35,218 EPOCH 4436
2020-02-27 05:22:45,824 Epoch 4436: total training loss 0.00049
2020-02-27 05:22:45,825 EPOCH 4437
2020-02-27 05:22:55,274 Epoch 4437 Step:   492500 Batch Loss:     0.000004 Tokens per Sec: 10191636, Lr: 0.000240
2020-02-27 05:22:55,919 Epoch 4437: total training loss 0.00048
2020-02-27 05:22:55,919 EPOCH 4438
2020-02-27 05:23:05,917 Epoch 4438: total training loss 0.00048
2020-02-27 05:23:05,918 EPOCH 4439
2020-02-27 05:23:15,789 Epoch 4439: total training loss 0.00048
2020-02-27 05:23:15,790 EPOCH 4440
2020-02-27 05:23:17,699 Epoch 4440 Step:   492750 Batch Loss:     0.000004 Tokens per Sec: 10045711, Lr: 0.000240
2020-02-27 05:23:25,749 Epoch 4440: total training loss 0.00048
2020-02-27 05:23:25,750 EPOCH 4441
2020-02-27 05:23:35,675 Epoch 4441: total training loss 0.00048
2020-02-27 05:23:35,676 EPOCH 4442
2020-02-27 05:23:40,042 Epoch 4442 Step:   493000 Batch Loss:     0.000004 Tokens per Sec: 10434843, Lr: 0.000240
2020-02-27 05:23:45,377 Epoch 4442: total training loss 0.00048
2020-02-27 05:23:45,377 EPOCH 4443
2020-02-27 05:23:55,599 Epoch 4443: total training loss 0.00048
2020-02-27 05:23:55,600 EPOCH 4444
2020-02-27 05:24:02,637 Epoch 4444 Step:   493250 Batch Loss:     0.000004 Tokens per Sec: 10137835, Lr: 0.000240
2020-02-27 05:24:05,870 Epoch 4444: total training loss 0.00048
2020-02-27 05:24:05,871 EPOCH 4445
2020-02-27 05:24:16,381 Epoch 4445: total training loss 0.00049
2020-02-27 05:24:16,382 EPOCH 4446
2020-02-27 05:24:26,578 Epoch 4446 Step:   493500 Batch Loss:     0.000003 Tokens per Sec:  9535134, Lr: 0.000240
2020-02-27 05:24:27,168 Epoch 4446: total training loss 0.00048
2020-02-27 05:24:27,168 EPOCH 4447
2020-02-27 05:24:37,727 Epoch 4447: total training loss 0.00049
2020-02-27 05:24:37,728 EPOCH 4448
2020-02-27 05:24:47,808 Epoch 4448: total training loss 0.00048
2020-02-27 05:24:47,809 EPOCH 4449
2020-02-27 05:24:49,810 Epoch 4449 Step:   493750 Batch Loss:     0.000008 Tokens per Sec: 10353553, Lr: 0.000240
2020-02-27 05:24:57,820 Epoch 4449: total training loss 0.00048
2020-02-27 05:24:57,821 EPOCH 4450
2020-02-27 05:25:07,837 Epoch 4450: total training loss 0.00048
2020-02-27 05:25:07,837 EPOCH 4451
2020-02-27 05:25:12,627 Epoch 4451 Step:   494000 Batch Loss:     0.000004 Tokens per Sec: 10633360, Lr: 0.000240
2020-02-27 05:25:17,711 Epoch 4451: total training loss 0.00048
2020-02-27 05:25:17,711 EPOCH 4452
2020-02-27 05:25:27,531 Epoch 4452: total training loss 0.00048
2020-02-27 05:25:27,532 EPOCH 4453
2020-02-27 05:25:34,326 Epoch 4453 Step:   494250 Batch Loss:     0.000005 Tokens per Sec: 10543766, Lr: 0.000240
2020-02-27 05:25:37,430 Epoch 4453: total training loss 0.00048
2020-02-27 05:25:37,431 EPOCH 4454
2020-02-27 05:25:47,424 Epoch 4454: total training loss 0.00048
2020-02-27 05:25:47,425 EPOCH 4455
2020-02-27 05:25:57,038 Epoch 4455 Step:   494500 Batch Loss:     0.000003 Tokens per Sec: 10183572, Lr: 0.000240
2020-02-27 05:25:57,455 Epoch 4455: total training loss 0.00048
2020-02-27 05:25:57,455 EPOCH 4456
2020-02-27 05:26:07,590 Epoch 4456: total training loss 0.00049
2020-02-27 05:26:07,591 EPOCH 4457
2020-02-27 05:26:17,855 Epoch 4457: total training loss 0.00048
2020-02-27 05:26:17,855 EPOCH 4458
2020-02-27 05:26:20,056 Epoch 4458 Step:   494750 Batch Loss:     0.000005 Tokens per Sec: 10073537, Lr: 0.000240
2020-02-27 05:26:27,918 Epoch 4458: total training loss 0.00049
2020-02-27 05:26:27,918 EPOCH 4459
2020-02-27 05:26:37,697 Epoch 4459: total training loss 0.00049
2020-02-27 05:26:37,698 EPOCH 4460
2020-02-27 05:26:42,293 Epoch 4460 Step:   495000 Batch Loss:     0.000004 Tokens per Sec: 10330069, Lr: 0.000240
2020-02-27 05:26:47,628 Epoch 4460: total training loss 0.00048
2020-02-27 05:26:47,629 EPOCH 4461
2020-02-27 05:26:57,482 Epoch 4461: total training loss 0.00048
2020-02-27 05:26:57,483 EPOCH 4462
2020-02-27 05:27:04,499 Epoch 4462 Step:   495250 Batch Loss:     0.000006 Tokens per Sec: 10463336, Lr: 0.000240
2020-02-27 05:27:07,408 Epoch 4462: total training loss 0.00047
2020-02-27 05:27:07,409 EPOCH 4463
2020-02-27 05:27:17,570 Epoch 4463: total training loss 0.00049
2020-02-27 05:27:17,571 EPOCH 4464
2020-02-27 05:27:27,896 Epoch 4464 Step:   495500 Batch Loss:     0.000007 Tokens per Sec:  9533055, Lr: 0.000240
2020-02-27 05:27:28,258 Epoch 4464: total training loss 0.00049
2020-02-27 05:27:28,258 EPOCH 4465
2020-02-27 05:27:38,570 Epoch 4465: total training loss 0.00048
2020-02-27 05:27:38,571 EPOCH 4466
2020-02-27 05:27:49,025 Epoch 4466: total training loss 0.00048
2020-02-27 05:27:49,026 EPOCH 4467
2020-02-27 05:27:51,335 Epoch 4467 Step:   495750 Batch Loss:     0.000005 Tokens per Sec:  9681330, Lr: 0.000240
2020-02-27 05:27:59,533 Epoch 4467: total training loss 0.00048
2020-02-27 05:27:59,534 EPOCH 4468
2020-02-27 05:28:09,489 Epoch 4468: total training loss 0.00048
2020-02-27 05:28:09,489 EPOCH 4469
2020-02-27 05:28:14,335 Epoch 4469 Step:   496000 Batch Loss:     0.000003 Tokens per Sec: 10246978, Lr: 0.000240
2020-02-27 05:28:19,534 Epoch 4469: total training loss 0.00048
2020-02-27 05:28:19,534 EPOCH 4470
2020-02-27 05:28:29,340 Epoch 4470: total training loss 0.00048
2020-02-27 05:28:29,340 EPOCH 4471
2020-02-27 05:28:36,262 Epoch 4471 Step:   496250 Batch Loss:     0.000004 Tokens per Sec: 10608301, Lr: 0.000240
2020-02-27 05:28:39,014 Epoch 4471: total training loss 0.00051
2020-02-27 05:28:39,015 EPOCH 4472
2020-02-27 05:28:48,840 Epoch 4472: total training loss 0.00049
2020-02-27 05:28:48,841 EPOCH 4473
2020-02-27 05:28:58,510 Epoch 4473 Step:   496500 Batch Loss:     0.000003 Tokens per Sec: 10302721, Lr: 0.000240
2020-02-27 05:28:58,764 Epoch 4473: total training loss 0.00048
2020-02-27 05:28:58,765 EPOCH 4474
2020-02-27 05:29:08,793 Epoch 4474: total training loss 0.00048
2020-02-27 05:29:08,793 EPOCH 4475
2020-02-27 05:29:19,098 Epoch 4475: total training loss 0.00048
2020-02-27 05:29:19,099 EPOCH 4476
2020-02-27 05:29:21,502 Epoch 4476 Step:   496750 Batch Loss:     0.000004 Tokens per Sec: 10149751, Lr: 0.000240
2020-02-27 05:29:29,749 Epoch 4476: total training loss 0.00049
2020-02-27 05:29:29,750 EPOCH 4477
2020-02-27 05:29:40,489 Epoch 4477: total training loss 0.00049
2020-02-27 05:29:40,489 EPOCH 4478
2020-02-27 05:29:45,527 Epoch 4478 Step:   497000 Batch Loss:     0.000003 Tokens per Sec:  9136249, Lr: 0.000240
2020-02-27 05:29:51,017 Epoch 4478: total training loss 0.00048
2020-02-27 05:29:51,018 EPOCH 4479
2020-02-27 05:30:01,151 Epoch 4479: total training loss 0.00048
2020-02-27 05:30:01,151 EPOCH 4480
2020-02-27 05:30:08,510 Epoch 4480 Step:   497250 Batch Loss:     0.000004 Tokens per Sec: 10266431, Lr: 0.000240
2020-02-27 05:30:11,082 Epoch 4480: total training loss 0.00048
2020-02-27 05:30:11,082 EPOCH 4481
2020-02-27 05:30:20,942 Epoch 4481: total training loss 0.00048
2020-02-27 05:30:20,943 EPOCH 4482
2020-02-27 05:30:30,568 Epoch 4482 Step:   497500 Batch Loss:     0.000004 Tokens per Sec: 10377724, Lr: 0.000240
2020-02-27 05:30:30,823 Epoch 4482: total training loss 0.00048
2020-02-27 05:30:30,824 EPOCH 4483
2020-02-27 05:30:40,533 Epoch 4483: total training loss 0.00048
2020-02-27 05:30:40,533 EPOCH 4484
2020-02-27 05:30:50,439 Epoch 4484: total training loss 0.00049
2020-02-27 05:30:50,440 EPOCH 4485
2020-02-27 05:30:52,726 Epoch 4485 Step:   497750 Batch Loss:     0.000003 Tokens per Sec: 10455842, Lr: 0.000240
2020-02-27 05:31:00,575 Epoch 4485: total training loss 0.00048
2020-02-27 05:31:00,576 EPOCH 4486
2020-02-27 05:31:10,746 Epoch 4486: total training loss 0.00048
2020-02-27 05:31:10,747 EPOCH 4487
2020-02-27 05:31:15,842 Epoch 4487 Step:   498000 Batch Loss:     0.000003 Tokens per Sec: 10215726, Lr: 0.000240
2020-02-27 05:31:21,027 Epoch 4487: total training loss 0.00049
2020-02-27 05:31:21,027 EPOCH 4488
2020-02-27 05:31:31,149 Epoch 4488: total training loss 0.00049
2020-02-27 05:31:31,149 EPOCH 4489
2020-02-27 05:31:38,461 Epoch 4489 Step:   498250 Batch Loss:     0.000003 Tokens per Sec: 10419958, Lr: 0.000240
2020-02-27 05:31:40,927 Epoch 4489: total training loss 0.00049
2020-02-27 05:31:40,927 EPOCH 4490
2020-02-27 05:31:50,842 Epoch 4490: total training loss 0.00048
2020-02-27 05:31:50,843 EPOCH 4491
2020-02-27 05:32:00,643 Epoch 4491 Step:   498500 Batch Loss:     0.000004 Tokens per Sec: 10328694, Lr: 0.000240
2020-02-27 05:32:00,770 Epoch 4491: total training loss 0.00048
2020-02-27 05:32:00,770 EPOCH 4492
2020-02-27 05:32:10,849 Epoch 4492: total training loss 0.00047
2020-02-27 05:32:10,850 EPOCH 4493
2020-02-27 05:32:20,775 Epoch 4493: total training loss 0.00048
2020-02-27 05:32:20,775 EPOCH 4494
2020-02-27 05:32:23,302 Epoch 4494 Step:   498750 Batch Loss:     0.000004 Tokens per Sec: 10316927, Lr: 0.000240
2020-02-27 05:32:30,839 Epoch 4494: total training loss 0.00049
2020-02-27 05:32:30,840 EPOCH 4495
2020-02-27 05:32:40,704 Epoch 4495: total training loss 0.00049
2020-02-27 05:32:40,704 EPOCH 4496
2020-02-27 05:32:45,341 Epoch 4496 Step:   499000 Batch Loss:     0.000005 Tokens per Sec: 10306455, Lr: 0.000240
2020-02-27 05:32:50,730 Epoch 4496: total training loss 0.00049
2020-02-27 05:32:50,730 EPOCH 4497
2020-02-27 05:33:01,284 Epoch 4497: total training loss 0.00048
2020-02-27 05:33:01,285 EPOCH 4498
2020-02-27 05:33:09,056 Epoch 4498 Step:   499250 Batch Loss:     0.000007 Tokens per Sec: 10073483, Lr: 0.000240
2020-02-27 05:33:11,511 Epoch 4498: total training loss 0.00048
2020-02-27 05:33:11,511 EPOCH 4499
2020-02-27 05:33:21,602 Epoch 4499: total training loss 0.00048
2020-02-27 05:33:21,602 EPOCH 4500
2020-02-27 05:33:31,644 Epoch 4500 Step:   499500 Batch Loss:     0.000004 Tokens per Sec: 10318674, Lr: 0.000240
2020-02-27 05:33:31,645 Epoch 4500: total training loss 0.00047
2020-02-27 05:33:31,645 EPOCH 4501
2020-02-27 05:33:41,482 Epoch 4501: total training loss 0.00048
2020-02-27 05:33:41,482 EPOCH 4502
2020-02-27 05:33:51,301 Epoch 4502: total training loss 0.00048
2020-02-27 05:33:51,301 EPOCH 4503
2020-02-27 05:33:53,837 Epoch 4503 Step:   499750 Batch Loss:     0.000006 Tokens per Sec: 10331183, Lr: 0.000240
2020-02-27 05:34:01,250 Epoch 4503: total training loss 0.00048
2020-02-27 05:34:01,251 EPOCH 4504
2020-02-27 05:34:11,091 Epoch 4504: total training loss 0.00048
2020-02-27 05:34:11,092 EPOCH 4505
2020-02-27 05:34:16,173 Epoch 4505 Step:   500000 Batch Loss:     0.000006 Tokens per Sec: 10623213, Lr: 0.000240
2020-02-27 05:34:16,174 Model noise rate: 5
2020-02-27 05:35:19,367 Validation result at epoch 4505, step   500000: Val DTW Score:  10.80, Body Val DTW Score:    nan, Hand Val DTW Score:    nan, loss:   0.0439, GT DTW Score:      nan, duration: 63.1932s
2020-02-27 05:35:19,368 Training ended since minimum lr 0.000200 was reached.
2020-02-27 05:35:19,369 Best validation result at step   100000:  10.60 dtw.
2020-02-27 05:36:09,173 Helloooooooo! This is RegJoey-NMT.
2020-02-27 05:36:09,173 Helloooooooo! This is RegJoey-NMT.
2020-02-27 05:36:09,177 Total params: 15427584
2020-02-27 05:36:09,177 Total params: 15427584
2020-02-27 05:36:09,178 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-02-27 05:36:09,178 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-02-27 05:36:09,180 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-02-27 05:36:09,180 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-03 13:43:14,113 Helloooooooo! This is RegJoey-NMT.
2020-03-03 13:43:36,380 Total params: 15427584
2020-03-03 13:43:36,382 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-03 13:43:36,387 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-03 13:47:53,778 Helloooooooo! This is RegJoey-NMT.
2020-03-03 13:47:57,211 Total params: 15427584
2020-03-03 13:47:57,213 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-03 13:47:57,218 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-03 13:51:09,421 Helloooooooo! This is RegJoey-NMT.
2020-03-03 13:51:12,903 Total params: 15427584
2020-03-03 13:51:12,905 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-03 13:51:12,910 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-03 14:03:18,011 Helloooooooo! This is RegJoey-NMT.
2020-03-03 14:03:22,733 Total params: 15427584
2020-03-03 14:03:22,736 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-03 14:03:22,742 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-03 14:09:12,435 Helloooooooo! This is RegJoey-NMT.
2020-03-03 14:09:16,566 Total params: 15427584
2020-03-03 14:09:16,568 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-03 14:09:16,572 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-03 14:12:15,163 Helloooooooo! This is RegJoey-NMT.
2020-03-03 14:12:18,344 Total params: 15427584
2020-03-03 14:12:18,346 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-03 14:12:18,350 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-03 14:40:39,319 Helloooooooo! This is RegJoey-NMT.
2020-03-03 14:40:44,834 Total params: 15427584
2020-03-03 14:40:44,836 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-03 14:40:44,841 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-03 15:29:30,322 Helloooooooo! This is RegJoey-NMT.
2020-03-03 15:29:34,060 Total params: 15427584
2020-03-03 15:29:34,063 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-03 15:29:34,069 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-03 15:31:59,723 Helloooooooo! This is RegJoey-NMT.
2020-03-03 15:32:03,541 Total params: 15427584
2020-03-03 15:32:03,546 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-03 15:32:03,559 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-03 15:34:18,647 Helloooooooo! This is RegJoey-NMT.
2020-03-03 15:34:22,450 Total params: 15427584
2020-03-03 15:34:22,452 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-03 15:34:22,455 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-05 12:52:38,703 Helloooooooo! This is RegJoey-NMT.
2020-03-05 12:52:43,245 Total params: 15427584
2020-03-05 12:52:43,247 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-05 12:52:43,253 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-05 13:16:02,210 Helloooooooo! This is RegJoey-NMT.
2020-03-05 13:16:02,217 Total params: 15427584
2020-03-05 13:16:02,218 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-05 13:16:02,221 Continuing model from .Models/2_8_512_GN5/440000_every.ckpt
2020-03-05 17:55:50,786 Helloooooooo! This is RegJoey-NMT.
2020-03-05 17:55:57,766 Total params: 15427584
2020-03-05 17:55:57,767 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-05 17:55:57,772 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-05 17:58:08,451 Helloooooooo! This is RegJoey-NMT.
2020-03-05 17:58:12,579 Total params: 15427584
2020-03-05 17:58:12,581 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-05 17:58:12,586 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-05 18:21:07,205 Helloooooooo! This is RegJoey-NMT.
2020-03-05 18:21:07,212 Total params: 15427584
2020-03-05 18:21:07,213 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-05 18:21:07,217 Continuing model from .Models/2_8_512_GN5/440000_every.ckpt
2020-03-09 14:39:32,819 Helloooooooo! This is RegJoey-NMT.
2020-03-09 14:40:03,708 Total params: 15427584
2020-03-09 14:40:03,709 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-09 14:40:03,714 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-09 14:52:31,421 Helloooooooo! This is RegJoey-NMT.
2020-03-09 14:52:35,051 Total params: 15427584
2020-03-09 14:52:35,053 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-09 14:52:35,061 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-09 15:00:47,088 Helloooooooo! This is RegJoey-NMT.
2020-03-09 15:00:50,652 Total params: 15427584
2020-03-09 15:00:50,658 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-09 15:00:50,670 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-09 16:30:52,603 Helloooooooo! This is RegJoey-NMT.
2020-03-09 16:30:58,819 Total params: 15427584
2020-03-09 16:30:58,822 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-09 16:30:58,829 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-09 16:37:38,535 Helloooooooo! This is RegJoey-NMT.
2020-03-09 16:37:42,164 Total params: 15427584
2020-03-09 16:37:42,165 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-09 16:37:42,168 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-10 15:21:10,165 Helloooooooo! This is RegJoey-NMT.
2020-03-10 15:21:27,427 Total params: 15427584
2020-03-10 15:21:27,429 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-10 15:21:27,432 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-10 15:55:39,211 Helloooooooo! This is RegJoey-NMT.
2020-03-10 15:55:44,040 Total params: 15427584
2020-03-10 15:55:44,043 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-10 15:55:44,055 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-10 15:57:27,103 Helloooooooo! This is RegJoey-NMT.
2020-03-10 15:57:30,037 Total params: 15427584
2020-03-10 15:57:30,038 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-10 15:57:30,043 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-10 16:02:26,348 Helloooooooo! This is RegJoey-NMT.
2020-03-10 16:02:29,313 Total params: 15427584
2020-03-10 16:02:29,314 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-10 16:02:29,318 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-10 16:06:55,053 Helloooooooo! This is RegJoey-NMT.
2020-03-10 16:06:57,741 Total params: 15427584
2020-03-10 16:06:57,742 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-10 16:06:57,747 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-10 16:08:44,760 Helloooooooo! This is RegJoey-NMT.
2020-03-10 16:08:49,326 Total params: 15427584
2020-03-10 16:08:49,327 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-10 16:08:49,332 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-10 16:11:22,926 Helloooooooo! This is RegJoey-NMT.
2020-03-10 16:11:26,797 Total params: 15427584
2020-03-10 16:11:26,798 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-10 16:11:26,803 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-10 16:17:41,000 Helloooooooo! This is RegJoey-NMT.
2020-03-10 16:17:44,351 Total params: 15427584
2020-03-10 16:17:44,352 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-10 16:17:44,355 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-10 16:23:44,167 Helloooooooo! This is RegJoey-NMT.
2020-03-10 16:23:47,796 Total params: 15427584
2020-03-10 16:23:47,798 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-10 16:23:47,803 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-10 16:28:45,320 Helloooooooo! This is RegJoey-NMT.
2020-03-10 16:28:48,496 Total params: 15427584
2020-03-10 16:28:48,498 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-10 16:28:48,501 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-10 16:31:57,342 Helloooooooo! This is RegJoey-NMT.
2020-03-10 16:32:01,025 Total params: 15427584
2020-03-10 16:32:01,026 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-10 16:32:01,029 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-10 16:35:55,687 Helloooooooo! This is RegJoey-NMT.
2020-03-10 16:35:59,046 Total params: 15427584
2020-03-10 16:35:59,048 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-10 16:35:59,053 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-10 16:38:20,838 Helloooooooo! This is RegJoey-NMT.
2020-03-10 16:38:24,605 Total params: 15427584
2020-03-10 16:38:24,608 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-10 16:38:24,614 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-11 10:52:08,509 Helloooooooo! This is RegJoey-NMT.
2020-03-11 10:52:34,823 Total params: 15427584
2020-03-11 10:52:34,824 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-11 10:52:34,828 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-11 10:57:06,424 Helloooooooo! This is RegJoey-NMT.
2020-03-11 10:57:15,960 Total params: 15427584
2020-03-11 10:57:15,961 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-11 10:57:15,965 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-11 11:00:26,376 Helloooooooo! This is RegJoey-NMT.
2020-03-11 11:00:29,419 Total params: 15427584
2020-03-11 11:00:29,421 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-11 11:00:29,425 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-11 11:13:34,656 Helloooooooo! This is RegJoey-NMT.
2020-03-11 11:13:37,600 Total params: 15427584
2020-03-11 11:13:37,601 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-11 11:13:37,605 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-11 11:17:35,943 Helloooooooo! This is RegJoey-NMT.
2020-03-11 11:17:38,971 Total params: 15427584
2020-03-11 11:17:38,972 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-11 11:17:38,975 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-11 11:23:20,738 Helloooooooo! This is RegJoey-NMT.
2020-03-11 11:23:23,676 Total params: 15427584
2020-03-11 11:23:23,677 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-11 11:23:23,680 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-11 11:27:14,409 Helloooooooo! This is RegJoey-NMT.
2020-03-11 11:27:17,238 Total params: 15427584
2020-03-11 11:27:17,239 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-11 11:27:17,243 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-11 11:48:18,139 Helloooooooo! This is RegJoey-NMT.
2020-03-11 11:48:21,215 Total params: 15427584
2020-03-11 11:48:21,216 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-11 11:48:21,220 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-23 10:30:52,613 Helloooooooo! This is RegJoey-NMT.
2020-03-23 10:30:58,987 Total params: 15427584
2020-03-23 10:30:58,994 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-23 10:30:59,011 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-23 10:38:52,306 Helloooooooo! This is RegJoey-NMT.
2020-03-23 10:38:58,606 Total params: 15427584
2020-03-23 10:38:58,608 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-23 10:38:58,615 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2020-03-23 11:12:17,315 Helloooooooo! This is RegJoey-NMT.
2020-03-23 11:12:24,709 Total params: 15427584
2020-03-23 11:12:24,711 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.bias', 'trg_embed.weight']
2020-03-23 11:12:24,718 Continuing model from .Models/2_4_512_GN5/500000_every.ckpt
2021-03-12 15:56:22,536 Helloooooooo! This is SignProdJoey-NMT.
2021-03-12 15:56:22,547 Total params: 15427584
2021-03-12 15:56:22,557 Validation Frequency: 10000
2021-03-12 16:08:17,795 Helloooooooo! This is SignProdJoey-NMT.
2021-03-12 16:08:17,805 Total params: 15427584
2021-03-12 16:08:17,814 Validation Frequency: 10000
2021-03-12 16:08:17,817 Config: .Models/2_4_512_GN5/config.yaml
2021-03-12 16:08:17,817 Continuing model from .Models/2_4_512_GN5/100000_best.ckpt
2021-03-12 16:08:17,817 
 --- Testing the model ---- 
2021-03-12 16:08:17,817 
Dataset: TEST
2021-03-12 16:09:32,066 Helloooooooo! This is SignProdJoey-NMT.
2021-03-12 16:09:32,080 Total params: 15427584
2021-03-12 16:09:32,090 Validation Frequency: 10000
2021-03-12 16:09:32,093 Config: .Models/2_4_512_GN5/config.yaml
2021-03-12 16:09:32,093 Continuing model from .Models/2_4_512_GN5/100000_best.ckpt
2021-03-12 16:09:32,093 
 --- Testing the model ---- 
2021-03-12 16:09:32,094 
Dataset: TEST
2021-03-12 16:10:16,355 Producing videos for TEST
2021-03-12 16:11:35,543 Helloooooooo! This is SignProdJoey-NMT.
2021-03-12 16:11:35,552 Total params: 15427584
2021-03-12 16:11:35,560 Validation Frequency: 10000
2021-03-12 16:11:35,563 Config: .Models/2_4_512_GN5/config.yaml
2021-03-12 16:11:35,563 Continuing model from .Models/2_4_512_GN5/100000_best.ckpt
2021-03-12 16:11:35,563 
 --- Testing the model ---- 
2021-03-12 16:11:35,563 
Dataset: TEST
2021-03-12 16:12:20,605 Producing videos for TEST
